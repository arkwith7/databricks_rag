{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f61094",
   "metadata": {},
   "source": [
    "# 01. Databricks í™˜ê²½ ì„¤ì • ë° Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **Databricks Text-to-SQL RAG ì‹œìŠ¤í…œ**ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ë¡œ, ì‹¤ì œ Databricks Lakehouse í™˜ê²½ì—ì„œ Northwind ìƒ˜í”Œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "- **Databricks í™˜ê²½ ì„¤ì •** ë° Spark ì„¸ì…˜ ì´ˆê¸°í™”\n",
    "- **Northwind ìƒ˜í”Œ ë°ì´í„°ë² ì´ìŠ¤** ìŠ¤í‚¤ë§ˆ ì„¤ê³„ ë° êµ¬ì¶•\n",
    "- **Delta Lake í…Œì´ë¸”** ìƒì„± ë° ë°ì´í„° ë¡œë“œ\n",
    "- **ë°ì´í„° ê²€ì¦** ë° ê¸°ë³¸ SQL í…ŒìŠ¤íŠ¸\n",
    "- **LangChain Agent ì—°ë™** ì¤€ë¹„\n",
    "\n",
    "## ğŸ—ï¸ Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°\n",
    "```\n",
    "Northwind Database (8ê°œ í…Œì´ë¸”)\n",
    "â”œâ”€â”€ customers      # ê³ ê° ì •ë³´\n",
    "â”œâ”€â”€ suppliers      # ê³µê¸‰ì—…ì²´ ì •ë³´\n",
    "â”œâ”€â”€ categories     # ìƒí’ˆ ì¹´í…Œê³ ë¦¬\n",
    "â”œâ”€â”€ products       # ìƒí’ˆ ì •ë³´\n",
    "â”œâ”€â”€ employees      # ì§ì› ì •ë³´\n",
    "â”œâ”€â”€ shippers       # ìš´ì†¡ì—…ì²´ ì •ë³´\n",
    "â”œâ”€â”€ orders         # ì£¼ë¬¸ ì •ë³´\n",
    "â””â”€â”€ order_details  # ì£¼ë¬¸ ìƒì„¸ ì •ë³´\n",
    "```\n",
    "\n",
    "## ğŸ“‹ ì „ì œì¡°ê±´\n",
    "- Databricks Workspace ë° í´ëŸ¬ìŠ¤í„° ì¤€ë¹„\n",
    "- Delta Lake í…Œì´ë¸” ìƒì„± ê¶Œí•œ\n",
    "- Spark SQL ì‹¤í–‰ ê¶Œí•œ\n",
    "\n",
    "## ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "ì´ ë…¸íŠ¸ë¶ ì™„ë£Œ í›„ â†’ `02_langchain_agent_text_to_sql.ipynb`ì—ì„œ LangChain Agent êµ¬í˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027e64f",
   "metadata": {},
   "source": [
    "## ğŸ—„ï¸ Northwind ìƒ˜í”Œ ë°ì´í„°ë² ì´ìŠ¤ ì†Œê°œ\n",
    "\n",
    "NorthwindëŠ” **ê°€ìƒì˜ ë¬´ì—­íšŒì‚¬ ë°ì´í„°ë² ì´ìŠ¤**ë¡œ Text-to-SQL í•™ìŠµì— ì´ìƒì ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“Š Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°\n",
    "- **Customers** (ê³ ê°): íšŒì‚¬ëª…, ì—°ë½ì²˜, ì£¼ì†Œ ë“±\n",
    "- **Orders** (ì£¼ë¬¸): ì£¼ë¬¸ì¼ì, ê³ ê°ì •ë³´, ë°°ì†¡ì •ë³´ ë“±  \n",
    "- **Order_Details** (ì£¼ë¬¸ìƒì„¸): ìƒí’ˆë³„ ìˆ˜ëŸ‰, ê°€ê²©, í• ì¸ ë“±\n",
    "- **Products** (ìƒí’ˆ): ìƒí’ˆëª…, ì¹´í…Œê³ ë¦¬, ì¬ê³ , ê°€ê²© ë“±\n",
    "- **Categories** (ì¹´í…Œê³ ë¦¬): ìƒí’ˆ ë¶„ë¥˜ ì •ë³´\n",
    "- **Suppliers** (ê³µê¸‰ì—…ì²´): ê³µê¸‰ì—…ì²´ ì •ë³´ ë° ì—°ë½ì²˜\n",
    "- **Employees** (ì§ì›): ì§ì› ì •ë³´ ë° ê´€ë¦¬ì ê´€ê³„\n",
    "- **Shippers** (ë°°ì†¡ì—…ì²´): ë°°ì†¡ íšŒì‚¬ ì •ë³´\n",
    "\n",
    "### ğŸ¯ Text-to-SQLì— ì í•©í•œ ì´ìœ \n",
    "1. **ì‹¤ë¬´ ì¹œí™”ì **: ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ë°˜ì˜\n",
    "2. **ë‹¤ì–‘í•œ ê´€ê³„**: JOIN, ì§‘ê³„, í•„í„°ë§ ë“± ë‹¤ì–‘í•œ SQL íŒ¨í„´\n",
    "3. **ì§ê´€ì  ë°ì´í„°**: ë„ë©”ì¸ ì§€ì‹ì´ ì‰½ê²Œ ì´í•´ ê°€ëŠ¥\n",
    "4. **ì ë‹¹í•œ í¬ê¸°**: í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ì— ìµœì í™”ëœ ë°ì´í„° ê·œëª¨\n",
    "\n",
    "## 1. Databricks í™˜ê²½ ì´ˆê¸°í™” ë° Spark ì„¸ì…˜ ì„¤ì •\n",
    "1. **CSV íŒŒì¼ ì—…ë¡œë“œ** â†’ Delta Lake í…Œì´ë¸” ìƒì„±\n",
    "2. **PostgreSQL ì—°ê²°** â†’ Databricksë¡œ ë°ì´í„° ì´ê´€  \n",
    "3. **SQL ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰** â†’ ì§ì ‘ í…Œì´ë¸” ìƒì„±\n",
    "4. **ìƒ˜í”Œ ë°ì´í„° ìƒì„±** â†’ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a417291",
   "metadata": {},
   "source": [
    "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° Databricks ì„œë¹„ìŠ¤ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745aa3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Databricks Text-to-SQL RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
      "==================================================\n",
      "\n",
      "ğŸ“‹ í™˜ê²½ ì •ë³´:\n",
      "Python ë²„ì „: 3.11.6\n",
      "Databricks Runtime: N/A\n",
      "Spark ë²„ì „: 3.5.2\n",
      "Spark ë²„ì „: 3.5.2\n",
      "í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: default\n",
      "í˜„ì¬ ì‚¬ìš©ì: saintphs@gmail.com\n",
      "í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: default\n",
      "í˜„ì¬ ì‚¬ìš©ì: saintphs@gmail.com\n",
      "í˜„ì¬ ì¹´íƒˆë¡œê·¸: workspace\n",
      "í˜„ì¬ ìŠ¤í‚¤ë§ˆ: default\n",
      "í˜„ì¬ ì¹´íƒˆë¡œê·¸: workspace\n",
      "í˜„ì¬ ìŠ¤í‚¤ë§ˆ: default\n",
      "Adaptive Query Execution: âŒ ë¹„í™œì„±í™”ë¨\n",
      "\n",
      "âœ… Databricks í™˜ê²½ ë° Spark ì„¸ì…˜ ì¤€ë¹„ ì™„ë£Œ!\n",
      "ğŸ“… ì´ˆê¸°í™” ì‹œê°„: 2025-06-30 08:43:26\n",
      "\n",
      "ğŸ’¡ ì°¸ê³ : Spark Connect í™˜ê²½ì—ì„œ ì¼ë¶€ JVM ì¢…ì† ê¸°ëŠ¥ë“¤ì€ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "Adaptive Query Execution: âŒ ë¹„í™œì„±í™”ë¨\n",
      "\n",
      "âœ… Databricks í™˜ê²½ ë° Spark ì„¸ì…˜ ì¤€ë¹„ ì™„ë£Œ!\n",
      "ğŸ“… ì´ˆê¸°í™” ì‹œê°„: 2025-06-30 08:43:26\n",
      "\n",
      "ğŸ’¡ ì°¸ê³ : Spark Connect í™˜ê²½ì—ì„œ ì¼ë¶€ JVM ì¢…ì† ê¸°ëŠ¥ë“¤ì€ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Databricks í™˜ê²½ ë° Spark ì„¸ì…˜ ì´ˆê¸°í™”\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸš€ Databricks Text-to-SQL RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# í™˜ê²½ ì •ë³´ í™•ì¸\n",
    "print(\"\\nğŸ“‹ í™˜ê²½ ì •ë³´:\")\n",
    "print(f\"Python ë²„ì „: {sys.version.split()[0]}\")\n",
    "print(f\"Databricks Runtime: {os.environ.get('DATABRICKS_RUNTIME_VERSION', 'N/A')}\")\n",
    "\n",
    "# Spark ì„¸ì…˜ ì´ˆê¸°í™”\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Northwind-TextToSQL-RAG\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(f\"Spark ë²„ì „: {spark.version}\")\n",
    "\n",
    "# Spark Connect í™˜ê²½ì—ì„œ ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ ì •ë³´ ì¡°íšŒ\n",
    "try:\n",
    "    # Spark Connectì—ì„œëŠ” sparkContextì— ì§ì ‘ ì ‘ê·¼í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš©\n",
    "    app_info = spark.sql(\"SELECT current_database(), current_user()\").collect()[0]\n",
    "    print(f\"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {app_info[0] if app_info[0] else 'default'}\")\n",
    "    print(f\"í˜„ì¬ ì‚¬ìš©ì: {app_info[1] if app_info[1] else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì •ë³´ ì¡°íšŒ ì œí•œë¨ (Spark Connect í™˜ê²½)\")\n",
    "\n",
    "# í˜„ì¬ ì¹´íƒˆë¡œê·¸ ë° ìŠ¤í‚¤ë§ˆ í™•ì¸\n",
    "try:\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "    print(f\"í˜„ì¬ ì¹´íƒˆë¡œê·¸: {current_catalog}\")\n",
    "    print(f\"í˜„ì¬ ìŠ¤í‚¤ë§ˆ: {current_schema}\")\n",
    "except Exception as e:\n",
    "    print(f\"ì¹´íƒˆë¡œê·¸/ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Spark ì„¤ì • í™•ì¸ (Spark Connectì—ì„œ ì§€ì›ë˜ëŠ” ë°©ì‹)\n",
    "try:\n",
    "    spark_configs = spark.sql(\"SET\").collect()\n",
    "    adaptive_enabled = any(\"spark.sql.adaptive.enabled\" in row.key and \"true\" in str(row.value).lower() \n",
    "                          for row in spark_configs if hasattr(row, 'key'))\n",
    "    print(f\"Adaptive Query Execution: {'âœ… í™œì„±í™”ë¨' if adaptive_enabled else 'âŒ ë¹„í™œì„±í™”ë¨'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Spark ì„¤ì • í™•ì¸ ì œí•œë¨\")\n",
    "\n",
    "print(\"\\nâœ… Databricks í™˜ê²½ ë° Spark ì„¸ì…˜ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“… ì´ˆê¸°í™” ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nğŸ’¡ ì°¸ê³ : Spark Connect í™˜ê²½ì—ì„œ ì¼ë¶€ JVM ì¢…ì† ê¸°ëŠ¥ë“¤ì€ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83eac58",
   "metadata": {},
   "source": [
    "## 2. Northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa4e298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Databricks í™˜ê²½ ì •ë³´:\n",
      "   is_databricks: False\n",
      "   spark_version: 3.5.2\n",
      "   runtime_version: ë¡œì»¬\n",
      "   cluster_id: ë¡œì»¬\n",
      "âœ… Databricks í™˜ê²½ ì„¤ì • ì™„ë£Œ\n",
      "ğŸ“‚ northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±/ì„ íƒ ì™„ë£Œ\n",
      "\n",
      "ğŸš€ Spark ì„¸ì…˜ í™œì„±í™” ì„±ê³µ!\n",
      "   Spark Connect ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘\n",
      "ğŸ“‚ northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±/ì„ íƒ ì™„ë£Œ\n",
      "\n",
      "ğŸš€ Spark ì„¸ì…˜ í™œì„±í™” ì„±ê³µ!\n",
      "   Spark Connect ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘\n",
      "   í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: northwind\n",
      "   í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: northwind\n",
      "   ì„¸ì…˜ ì‹œì‘ ì‹œê°„: 2025-06-30 08:43:27.854717\n",
      "   ì„¸ì…˜ ì‹œì‘ ì‹œê°„: 2025-06-30 08:43:27.854717\n",
      "   í˜„ì¬ ì¹´íƒˆë¡œê·¸: workspace\n",
      "   í˜„ì¬ ìŠ¤í‚¤ë§ˆ: northwind\n",
      "\n",
      "âœ… Spark ì„¸ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ğŸ—ï¸ Northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹œì‘\n",
      "========================================\n",
      "   í˜„ì¬ ì¹´íƒˆë¡œê·¸: workspace\n",
      "   í˜„ì¬ ìŠ¤í‚¤ë§ˆ: northwind\n",
      "\n",
      "âœ… Spark ì„¸ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ğŸ—ï¸ Northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹œì‘\n",
      "========================================\n",
      "âœ… northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ\n",
      "âœ… northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ\n",
      "ğŸ“‚ í˜„ì¬ í™œì„± ë°ì´í„°ë² ì´ìŠ¤: northwind\n",
      "ğŸ“‚ í˜„ì¬ í™œì„± ë°ì´í„°ë² ì´ìŠ¤: northwind\n",
      "ğŸ“Š ê¸°ì¡´ í…Œì´ë¸”: 8ê°œ\n",
      "   ê¸°ì¡´ í…Œì´ë¸” ëª©ë¡:\n",
      "     ğŸ“‹ categories\n",
      "     ğŸ“‹ customers\n",
      "     ğŸ“‹ employees\n",
      "     ğŸ“‹ order_details\n",
      "     ğŸ“‹ orders\n",
      "     ... ê·¸ ì™¸ 3ê°œ\n",
      "\n",
      "ğŸ“‹ ì´ 8ê°œ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜ë¨:\n",
      "   ğŸ“‚ categories\n",
      "   ğŸ“‚ suppliers\n",
      "   ğŸ“‚ products\n",
      "   ğŸ“‚ customers\n",
      "   ğŸ“‚ employees\n",
      "   ğŸ“‚ shippers\n",
      "   ğŸ“‚ orders\n",
      "   ğŸ“‚ order_details\n",
      "\n",
      "âœ… Northwind ìŠ¤í‚¤ë§ˆ ì •ì˜ ì™„ë£Œ!\n",
      "ğŸ“Š ê¸°ì¡´ í…Œì´ë¸”: 8ê°œ\n",
      "   ê¸°ì¡´ í…Œì´ë¸” ëª©ë¡:\n",
      "     ğŸ“‹ categories\n",
      "     ğŸ“‹ customers\n",
      "     ğŸ“‹ employees\n",
      "     ğŸ“‹ order_details\n",
      "     ğŸ“‹ orders\n",
      "     ... ê·¸ ì™¸ 3ê°œ\n",
      "\n",
      "ğŸ“‹ ì´ 8ê°œ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜ë¨:\n",
      "   ğŸ“‚ categories\n",
      "   ğŸ“‚ suppliers\n",
      "   ğŸ“‚ products\n",
      "   ğŸ“‚ customers\n",
      "   ğŸ“‚ employees\n",
      "   ğŸ“‚ shippers\n",
      "   ğŸ“‚ orders\n",
      "   ğŸ“‚ order_details\n",
      "\n",
      "âœ… Northwind ìŠ¤í‚¤ë§ˆ ì •ì˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "# Databricks í™˜ê²½ í™•ì¸ ë° ê¸°ë³¸ ì„¤ì •\n",
    "def setup_databricks_environment():\n",
    "    \"\"\"Databricks í™˜ê²½ ì„¤ì • ë° í™•ì¸\"\"\"\n",
    "    try:\n",
    "        # Spark ì„¸ì…˜ í™•ì¸/ìƒì„±\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark is None:\n",
    "            spark = SparkSession.builder.appName(\"Northwind-TextToSQL\").getOrCreate()\n",
    "        \n",
    "        # í™˜ê²½ ì •ë³´ ìˆ˜ì§‘\n",
    "        is_databricks = \"DATABRICKS_RUNTIME_VERSION\" in os.environ\n",
    "        \n",
    "        env_info = {\n",
    "            \"is_databricks\": is_databricks,\n",
    "            \"spark_version\": spark.version,\n",
    "            \"runtime_version\": os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", \"ë¡œì»¬\"),\n",
    "            \"cluster_id\": os.environ.get(\"SPARK_LOCAL_HOSTNAME\", \"ë¡œì»¬\"),\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸ”§ Databricks í™˜ê²½ ì •ë³´:\")\n",
    "        for key, value in env_info.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        print(\"âœ… Databricks í™˜ê²½ ì„¤ì • ì™„ë£Œ\")\n",
    "        \n",
    "        # ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •\n",
    "        spark.sql(\"CREATE DATABASE IF NOT EXISTS northwind\")\n",
    "        spark.sql(\"USE northwind\")\n",
    "        \n",
    "        print(\"ğŸ“‚ northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±/ì„ íƒ ì™„ë£Œ\")\n",
    "        \n",
    "        return spark, env_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Databricks í™˜ê²½ ì„¤ì • ì‹¤íŒ¨: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# í™˜ê²½ ì„¤ì • ì‹¤í–‰\n",
    "spark, env_info = setup_databricks_environment()\n",
    "\n",
    "if spark:\n",
    "    print(\"\\nğŸš€ Spark ì„¸ì…˜ í™œì„±í™” ì„±ê³µ!\")\n",
    "    # Spark Connect í˜¸í™˜ - sparkContext ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
    "    print(\"   Spark Connect ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘\")\n",
    "\n",
    "    # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸\n",
    "    current_db = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "    print(f\"   í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {current_db}\")\n",
    "    \n",
    "    # Spark ì„¸ì…˜ ì •ë³´ (Spark Connect í˜¸í™˜)\n",
    "    app_info = spark.sql(\"SELECT 'session_active' as status, current_timestamp() as timestamp\").collect()[0]\n",
    "    print(f\"   ì„¸ì…˜ ì‹œì‘ ì‹œê°„: {app_info['timestamp']}\")\n",
    "    \n",
    "    # í˜„ì¬ ì¹´íƒˆë¡œê·¸ ë° ìŠ¤í‚¤ë§ˆ ì •ë³´\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "    print(f\"   í˜„ì¬ ì¹´íƒˆë¡œê·¸: {current_catalog}\")\n",
    "    print(f\"   í˜„ì¬ ìŠ¤í‚¤ë§ˆ: {current_schema}\")\n",
    "    \n",
    "    print(\"\\nâœ… Spark ì„¸ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Spark ì„¸ì…˜ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# Northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "\n",
    "print(\"ğŸ—ï¸ Northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹œì‘\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±\n",
    "try:\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS northwind\")\n",
    "    spark.sql(\"USE northwind\")\n",
    "    print(\"âœ… northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸\n",
    "    current_db = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "    print(f\"ğŸ“‚ í˜„ì¬ í™œì„± ë°ì´í„°ë² ì´ìŠ¤: {current_db}\")\n",
    "    \n",
    "    # ê¸°ì¡´ í…Œì´ë¸” í™•ì¸\n",
    "    existing_tables = [row.tableName for row in spark.sql(\"SHOW TABLES\").collect()]\n",
    "    print(f\"ğŸ“Š ê¸°ì¡´ í…Œì´ë¸”: {len(existing_tables)}ê°œ\")\n",
    "    if existing_tables:\n",
    "        print(\"   ê¸°ì¡´ í…Œì´ë¸” ëª©ë¡:\")\n",
    "        for table in existing_tables[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ\n",
    "            print(f\"     ğŸ“‹ {table}\")\n",
    "        if len(existing_tables) > 5:\n",
    "            print(f\"     ... ê·¸ ì™¸ {len(existing_tables) - 5}ê°œ\")\n",
    "    else:\n",
    "        print(\"   ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. í…Œì´ë¸”ì„ ìƒì„±í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# 2. Northwind í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "northwind_schemas = {\n",
    "    \"categories\": \"\"\"\n",
    "        categoryid INT,\n",
    "        categoryname STRING,\n",
    "        description STRING\n",
    "    \"\"\",\n",
    "    \n",
    "    \"suppliers\": \"\"\"\n",
    "        supplierid INT,\n",
    "        companyname STRING,\n",
    "        contactname STRING,\n",
    "        contacttitle STRING,\n",
    "        address STRING,\n",
    "        city STRING,\n",
    "        region STRING,\n",
    "        postalcode STRING,\n",
    "        country STRING,\n",
    "        phone STRING,\n",
    "        fax STRING,\n",
    "        homepage STRING\n",
    "    \"\"\",\n",
    "    \n",
    "    \"products\": \"\"\"\n",
    "        productid INT,\n",
    "        productname STRING,\n",
    "        supplierid INT,\n",
    "        categoryid INT,\n",
    "        quantityperunit STRING,\n",
    "        unitprice DECIMAL(10,2),\n",
    "        unitsinstock INT,\n",
    "        unitsonorder INT,\n",
    "        reorderlevel INT,\n",
    "        discontinued BOOLEAN\n",
    "    \"\"\",\n",
    "    \n",
    "    \"customers\": \"\"\"\n",
    "        customerid STRING,\n",
    "        companyname STRING,\n",
    "        contactname STRING,\n",
    "        contacttitle STRING,\n",
    "        address STRING,\n",
    "        city STRING,\n",
    "        region STRING,\n",
    "        postalcode STRING,\n",
    "        country STRING,\n",
    "        phone STRING,\n",
    "        fax STRING\n",
    "    \"\"\",\n",
    "    \n",
    "    \"employees\": \"\"\"\n",
    "        employeeid INT,\n",
    "        lastname STRING,\n",
    "        firstname STRING,\n",
    "        title STRING,\n",
    "        titleofcourtesy STRING,\n",
    "        birthdate DATE,\n",
    "        hiredate DATE,\n",
    "        address STRING,\n",
    "        city STRING,\n",
    "        region STRING,\n",
    "        postalcode STRING,\n",
    "        country STRING,\n",
    "        homephone STRING,\n",
    "        extension STRING,\n",
    "        notes STRING,\n",
    "        reportsto INT\n",
    "    \"\"\",\n",
    "    \n",
    "    \"shippers\": \"\"\"\n",
    "        shipperid INT,\n",
    "        companyname STRING,\n",
    "        phone STRING\n",
    "    \"\"\",\n",
    "    \n",
    "    \"orders\": \"\"\"\n",
    "        orderid INT,\n",
    "        customerid STRING,\n",
    "        employeeid INT,\n",
    "        orderdate DATE,\n",
    "        requireddate DATE,\n",
    "        shippeddate DATE,\n",
    "        shipvia INT,\n",
    "        freight DECIMAL(10,2),\n",
    "        shipname STRING,\n",
    "        shipaddress STRING,\n",
    "        shipcity STRING,\n",
    "        shipregion STRING,\n",
    "        shippostalcode STRING,\n",
    "        shipcountry STRING\n",
    "    \"\"\",\n",
    "    \n",
    "    \"order_details\": \"\"\"\n",
    "        orderid INT,\n",
    "        productid INT,\n",
    "        unitprice DECIMAL(10,2),\n",
    "        quantity INT,\n",
    "        discount DECIMAL(3,2)\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“‹ ì´ {len(northwind_schemas)}ê°œ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜ë¨:\")\n",
    "for table_name in northwind_schemas.keys():\n",
    "    print(f\"   ğŸ“‚ {table_name}\")\n",
    "\n",
    "print(\"\\nâœ… Northwind ìŠ¤í‚¤ë§ˆ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb36ee4",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•\n",
    "\n",
    "## 3. Delta Lake í…Œì´ë¸” ìƒì„± ë° ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d97ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ\n",
      "âœ… í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜ ì™„ë£Œ\n",
      "   ì •ì˜ëœ í…Œì´ë¸”: ['categories', 'suppliers', 'products', 'customers', 'employees', 'shippers', 'orders', 'order_details']\n",
      "âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ\n",
      "   categories: 8ê°œ ë ˆì½”ë“œ\n",
      "   suppliers: 5ê°œ ë ˆì½”ë“œ\n",
      "   products: 10ê°œ ë ˆì½”ë“œ\n",
      "   customers: 5ê°œ ë ˆì½”ë“œ\n",
      "   employees: 5ê°œ ë ˆì½”ë“œ\n",
      "   shippers: 3ê°œ ë ˆì½”ë“œ\n",
      "   orders: 100ê°œ ë ˆì½”ë“œ\n",
      "   order_details: 290ê°œ ë ˆì½”ë“œ\n",
      "\n",
      "ğŸ—ï¸ Northwind ë°ì´í„° ë¹Œë” ì¤€ë¹„ ì™„ë£Œ!\n",
      "ğŸ“¦ Delta Lake í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ë¡œë“œ\n",
      "=============================================\n",
      "\n",
      "ğŸ”„ ê¸°ì¡´ í…Œì´ë¸” ì •ë¦¬ ì¤‘...\n",
      "   ğŸ—‘ï¸ categories í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ categories í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ suppliers í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ suppliers í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ products í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ products í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ customers í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ customers í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ employees í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ employees í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ shippers í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ shippers í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ orders í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ orders í…Œì´ë¸” ì‚­ì œë¨\n",
      "   ğŸ—‘ï¸ order_details í…Œì´ë¸” ì‚­ì œë¨\n",
      "\n",
      "ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ì •ì˜ ì™„ë£Œ:\n",
      "   ğŸ“ˆ categories: 8ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ suppliers: 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ products: 10ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ customers: 10ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ employees: 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ shippers: 3ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ orders: 10ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ order_details: 17ê°œ ë ˆì½”ë“œ\n",
      "\n",
      "âœ… ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\n",
      "   ğŸ—‘ï¸ order_details í…Œì´ë¸” ì‚­ì œë¨\n",
      "\n",
      "ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ì •ì˜ ì™„ë£Œ:\n",
      "   ğŸ“ˆ categories: 8ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ suppliers: 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ products: 10ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ customers: 10ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ employees: 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ shippers: 3ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ orders: 10ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“ˆ order_details: 17ê°œ ë ˆì½”ë“œ\n",
      "\n",
      "âœ… ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, BooleanType, DateType\n",
    "\n",
    "class DatabaseSchemaAnalyzer:\n",
    "    \"\"\"ì‹¤ì œ Databricks í™˜ê²½ì—ì„œ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ë° ë²¡í„°í™”\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session, embedding_model=None):\n",
    "        self.spark = spark_session\n",
    "        self.embedding_model = embedding_model\n",
    "        self.schema_cache = {}\n",
    "        \n",
    "    def extract_table_schema(self, database_name: str, table_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"ì‹¤ì œ í…Œì´ë¸”ì—ì„œ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "        try:\n",
    "            full_table_name = f\"{database_name}.{table_name}\"\n",
    "            \n",
    "            # í…Œì´ë¸” ì„¤ëª… ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "            describe_df = self.spark.sql(f\"DESCRIBE TABLE EXTENDED {full_table_name}\")\n",
    "            columns_info = []\n",
    "            table_properties = {}\n",
    "            \n",
    "            # ì»¬ëŸ¼ ì •ë³´ ì²˜ë¦¬\n",
    "            for row in describe_df.collect():\n",
    "                col_name = row.col_name\n",
    "                data_type = row.data_type\n",
    "                comment = row.comment\n",
    "                \n",
    "                if col_name and not col_name.startswith('#') and col_name != '':\n",
    "                    if col_name == '# Detailed Table Information':\n",
    "                        break\n",
    "                    \n",
    "                    columns_info.append({\n",
    "                        'name': col_name,\n",
    "                        'type': data_type,\n",
    "                        'comment': comment or ''\n",
    "                    })\n",
    "            \n",
    "            # ìƒ˜í”Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì²˜ìŒ 3ê°œ í–‰)\n",
    "            sample_data = []\n",
    "            try:\n",
    "                sample_df = self.spark.sql(f\"SELECT * FROM {full_table_name} LIMIT 3\")\n",
    "                sample_data = [row.asDict() for row in sample_df.collect()]\n",
    "            except Exception:\n",
    "                sample_data = []\n",
    "            \n",
    "            # í…Œì´ë¸” í†µê³„ ì •ë³´\n",
    "            try:\n",
    "                count_result = self.spark.sql(f\"SELECT COUNT(*) as row_count FROM {full_table_name}\").collect()\n",
    "                row_count = count_result[0].row_count if count_result else 0\n",
    "            except Exception:\n",
    "                row_count = 0\n",
    "            \n",
    "            schema_info = {\n",
    "                'database_name': database_name,\n",
    "                'table_name': table_name,\n",
    "                'full_name': full_table_name,\n",
    "                'columns': columns_info,\n",
    "                'sample_data': sample_data,\n",
    "                'row_count': row_count,\n",
    "                'extracted_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # ìºì‹œì— ì €ì¥\n",
    "            self.schema_cache[full_table_name] = schema_info\n",
    "            \n",
    "            return schema_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í…Œì´ë¸” {database_name}.{table_name} ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def analyze_database_schema(self, database_name: str, limit_tables: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ ìŠ¤í‚¤ë§ˆ ë¶„ì„\"\"\"\n",
    "        if not self.spark:\n",
    "            print(\"âŒ Spark ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  í…Œì´ë¸” ê°€ì ¸ì˜¤ê¸°\n",
    "            tables_df = self.spark.sql(f\"SHOW TABLES IN {database_name}\")\n",
    "            tables = [(row.database, row.tableName) for row in tables_df.collect()]\n",
    "            \n",
    "            if limit_tables:\n",
    "                tables = tables[:limit_tables]\n",
    "            \n",
    "            print(f\"ğŸ“Š {database_name} ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì‹œì‘ ({len(tables)}ê°œ í…Œì´ë¸”)\")\n",
    "            \n",
    "            schemas = []\n",
    "            for db, table in tables:\n",
    "                print(f\"   ğŸ” {table} ë¶„ì„ ì¤‘...\")\n",
    "                schema = self.extract_table_schema(db, table)\n",
    "                if schema:\n",
    "                    schemas.append(schema)\n",
    "            \n",
    "            print(f\"âœ… ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì™„ë£Œ: {len(schemas)}ê°œ í…Œì´ë¸”\")\n",
    "            return schemas\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def create_schema_text_for_embedding(self, schema_info: Dict[str, Any]) -> str:\n",
    "        \"\"\"ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        table_name = schema_info['table_name']\n",
    "        columns = schema_info['columns']\n",
    "        \n",
    "        # í…Œì´ë¸” ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        text_parts = [\n",
    "            f\"í…Œì´ë¸”ëª…: {table_name}\",\n",
    "            f\"ë°ì´í„°ë² ì´ìŠ¤: {schema_info['database_name']}\",\n",
    "            f\"í–‰ ìˆ˜: {schema_info.get('row_count', 0):,}\"\n",
    "        ]\n",
    "        \n",
    "        # ì»¬ëŸ¼ ì •ë³´ ì¶”ê°€\n",
    "        text_parts.append(\"ì»¬ëŸ¼ ì •ë³´:\")\n",
    "        for col in columns:\n",
    "            col_desc = f\"- {col['name']} ({col['type']})\"\n",
    "            if col.get('comment'):\n",
    "                col_desc += f\": {col['comment']}\"\n",
    "            text_parts.append(col_desc)\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° í‚¤ ì¶”ê°€ (ê²€ìƒ‰ì— ë„ì›€)\n",
    "        if schema_info.get('sample_data'):\n",
    "            sample_keys = list(schema_info['sample_data'][0].keys()) if schema_info['sample_data'] else []\n",
    "            if sample_keys:\n",
    "                text_parts.append(f\"ì£¼ìš” í•„ë“œ: {', '.join(sample_keys[:5])}\")\n",
    "        \n",
    "        return \"\\n\".join(text_parts)\n",
    "    \n",
    "    def vectorize_schemas(self, schemas: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ë²¡í„°í™”\"\"\"\n",
    "        if not self.embedding_model:\n",
    "            print(\"âš ï¸ ì„ë² ë”© ëª¨ë¸ì´ ì—†ì–´ ë²¡í„°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            return schemas\n",
    "        \n",
    "        vectorized_schemas = []\n",
    "        \n",
    "        for schema in schemas:\n",
    "            try:\n",
    "                # í…ìŠ¤íŠ¸ ìƒì„±\n",
    "                schema_text = self.create_schema_text_for_embedding(schema)\n",
    "                \n",
    "                # ë²¡í„°í™”\n",
    "                embedding = self.embedding_model.embed_query(schema_text)\n",
    "                \n",
    "                # ë²¡í„° ì •ë³´ ì¶”ê°€\n",
    "                schema['schema_text'] = schema_text\n",
    "                schema['embedding'] = embedding\n",
    "                schema['embedding_dimension'] = len(embedding)\n",
    "                \n",
    "                vectorized_schemas.append(schema)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ {schema['table_name']} ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}\")\n",
    "                vectorized_schemas.append(schema)  # ë²¡í„° ì—†ì´ë¼ë„ ì¶”ê°€\n",
    "        \n",
    "        print(f\"âœ… ìŠ¤í‚¤ë§ˆ ë²¡í„°í™” ì™„ë£Œ: {len(vectorized_schemas)}ê°œ\")\n",
    "        return vectorized_schemas\n",
    "\n",
    "class NorthwindDataBuilder:\n",
    "    \"\"\"Northwind ìƒ˜í”Œ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        self.schemas = {}\n",
    "        self.sample_data = {}\n",
    "        \n",
    "    def define_schemas(self):\n",
    "        \"\"\"í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜\"\"\"\n",
    "        \n",
    "        # Categories (ì¹´í…Œê³ ë¦¬)\n",
    "        self.schemas['categories'] = StructType([\n",
    "            StructField(\"category_id\", IntegerType(), False),\n",
    "            StructField(\"category_name\", StringType(), False),\n",
    "            StructField(\"description\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Suppliers (ê³µê¸‰ì—…ì²´)\n",
    "        self.schemas['suppliers'] = StructType([\n",
    "            StructField(\"supplier_id\", IntegerType(), False),\n",
    "            StructField(\"company_name\", StringType(), False),\n",
    "            StructField(\"contact_name\", StringType(), True),\n",
    "            StructField(\"contact_title\", StringType(), True),\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"country\", StringType(), True),\n",
    "            StructField(\"phone\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Products (ìƒí’ˆ)\n",
    "        self.schemas['products'] = StructType([\n",
    "            StructField(\"product_id\", IntegerType(), False),\n",
    "            StructField(\"product_name\", StringType(), False),\n",
    "            StructField(\"supplier_id\", IntegerType(), True),\n",
    "            StructField(\"category_id\", IntegerType(), True),\n",
    "            StructField(\"unit_price\", DoubleType(), True),\n",
    "            StructField(\"units_in_stock\", IntegerType(), True),\n",
    "            StructField(\"units_on_order\", IntegerType(), True),\n",
    "            StructField(\"discontinued\", BooleanType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Customers (ê³ ê°)\n",
    "        self.schemas['customers'] = StructType([\n",
    "            StructField(\"customer_id\", StringType(), False),\n",
    "            StructField(\"company_name\", StringType(), False),\n",
    "            StructField(\"contact_name\", StringType(), True),\n",
    "            StructField(\"contact_title\", StringType(), True),\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"country\", StringType(), True),\n",
    "            StructField(\"phone\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Employees (ì§ì›)\n",
    "        self.schemas['employees'] = StructType([\n",
    "            StructField(\"employee_id\", IntegerType(), False),\n",
    "            StructField(\"first_name\", StringType(), False),\n",
    "            StructField(\"last_name\", StringType(), False),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"birth_date\", DateType(), True),\n",
    "            StructField(\"hire_date\", DateType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"country\", StringType(), True),\n",
    "            StructField(\"reports_to\", IntegerType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Shippers (ë°°ì†¡ì—…ì²´)\n",
    "        self.schemas['shippers'] = StructType([\n",
    "            StructField(\"shipper_id\", IntegerType(), False),\n",
    "            StructField(\"company_name\", StringType(), False),\n",
    "            StructField(\"phone\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Orders (ì£¼ë¬¸)\n",
    "        self.schemas['orders'] = StructType([\n",
    "            StructField(\"order_id\", IntegerType(), False),\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"employee_id\", IntegerType(), True),\n",
    "            StructField(\"order_date\", DateType(), True),\n",
    "            StructField(\"required_date\", DateType(), True),\n",
    "            StructField(\"shipped_date\", DateType(), True),\n",
    "            StructField(\"ship_via\", IntegerType(), True),\n",
    "            StructField(\"freight\", DoubleType(), True),\n",
    "            StructField(\"ship_name\", StringType(), True),\n",
    "            StructField(\"ship_address\", StringType(), True),\n",
    "            StructField(\"ship_city\", StringType(), True),\n",
    "            StructField(\"ship_country\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Order Details (ì£¼ë¬¸ ìƒì„¸)\n",
    "        self.schemas['order_details'] = StructType([\n",
    "            StructField(\"order_id\", IntegerType(), False),\n",
    "            StructField(\"product_id\", IntegerType(), False),\n",
    "            StructField(\"unit_price\", DoubleType(), False),\n",
    "            StructField(\"quantity\", IntegerType(), False),\n",
    "            StructField(\"discount\", DoubleType(), True)\n",
    "        ])\n",
    "        \n",
    "        print(\"âœ… í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜ ì™„ë£Œ\")\n",
    "        print(f\"   ì •ì˜ëœ í…Œì´ë¸”: {list(self.schemas.keys())}\")\n",
    "        \n",
    "    def generate_sample_data(self):\n",
    "        \"\"\"ìƒ˜í”Œ ë°ì´í„° ìƒì„±\"\"\"\n",
    "        \n",
    "        # Categories\n",
    "        self.sample_data['categories'] = [\n",
    "            (1, \"ìŒë£Œ\", \"ì²­ëŸ‰ìŒë£Œ, ì»¤í”¼, ì°¨, ë§¥ì£¼, ì—ì¼\"),\n",
    "            (2, \"ì¡°ë¯¸ë£Œ\", \"ë‹¬ì½¤í•˜ê³  ë§›ìˆëŠ” ì†ŒìŠ¤, ì–‘ë…, ì¡°ë¯¸ë£Œ, í–¥ì‹ ë£Œ\"),\n",
    "            (3, \"ê³¼ìë¥˜\", \"ë””ì €íŠ¸, ì‚¬íƒ•, ë‹¬ì½¤í•œ ë¹µ\"),\n",
    "            (4, \"ìœ ì œí’ˆ\", \"ì¹˜ì¦ˆ\"),\n",
    "            (5, \"ê³¡ë¬¼/ì‹œë¦¬ì–¼\", \"ë¹µ, í¬ë˜ì»¤, íŒŒìŠ¤íƒ€, ì‹œë¦¬ì–¼\"),\n",
    "            (6, \"ìœ¡ë¥˜/ê°€ê¸ˆë¥˜\", \"ì¡°ë¦¬ëœ ìœ¡ë¥˜\"),\n",
    "            (7, \"ë†ì‚°ë¬¼\", \"ê±´ì¡° ê³¼ì¼ê³¼ ì½© ë‘ë¶€\"),\n",
    "            (8, \"í•´ì‚°ë¬¼\", \"í•´ì´ˆì™€ ìƒì„ \")\n",
    "        ]\n",
    "        \n",
    "        # Suppliers\n",
    "        self.sample_data['suppliers'] = [\n",
    "            (1, \"ì‚¼ì„±ì‹í’ˆ\", \"ê¹€ì‚¼ì„±\", \"ì˜ì—… ë‹´ë‹¹ì\", \"ì„œìš¸ì‹œ ê°•ë‚¨êµ¬\", \"ì„œìš¸\", \"í•œêµ­\", \"02-123-4567\"),\n",
    "            (2, \"LGë¬´ì—­\", \"ì´ì—˜ì§€\", \"êµ¬ë§¤ ê´€ë¦¬ì\", \"ë¶€ì‚°ì‹œ í•´ìš´ëŒ€êµ¬\", \"ë¶€ì‚°\", \"í•œêµ­\", \"051-234-5678\"),\n",
    "            (3, \"í˜„ëŒ€ë¬¼ì‚°\", \"ë°•í˜„ëŒ€\", \"ë§ˆì¼€íŒ… ì´ì‚¬\", \"ì¸ì²œì‹œ ë‚¨ë™êµ¬\", \"ì¸ì²œ\", \"í•œêµ­\", \"032-345-6789\"),\n",
    "            (4, \"ë¡¯ë°ìƒì‚¬\", \"ìµœë¡¯ë°\", \"ì˜ì—… ì´ì‚¬\", \"ëŒ€êµ¬ì‹œ ì¤‘êµ¬\", \"ëŒ€êµ¬\", \"í•œêµ­\", \"053-456-7890\"),\n",
    "            (5, \"SKì¼€ë¯¸ì¹¼\", \"ì •ì—ìŠ¤ì¼€ì´\", \"í’ˆì§ˆ ê´€ë¦¬ì\", \"ê´‘ì£¼ì‹œ ì„œêµ¬\", \"ê´‘ì£¼\", \"í•œêµ­\", \"062-567-8901\")\n",
    "        ]\n",
    "        \n",
    "        # Products\n",
    "        self.sample_data['products'] = [\n",
    "            (1, \"ê¹€ì¹˜\", 1, 2, 15000.0, 100, 0, False),\n",
    "            (2, \"ë¹„ë¹”ë°¥\", 2, 5, 12000.0, 50, 10, False),\n",
    "            (3, \"ë¶ˆê³ ê¸°\", 3, 6, 25000.0, 30, 5, False),\n",
    "            (4, \"ëœì¥ì°Œê°œ\", 1, 2, 8000.0, 80, 20, False),\n",
    "            (5, \"ê°ˆë¹„íƒ•\", 3, 6, 18000.0, 25, 0, False),\n",
    "            (6, \"ëƒ‰ë©´\", 2, 5, 10000.0, 60, 15, False),\n",
    "            (7, \"ì‚¼ê²¹ì‚´\", 4, 6, 22000.0, 40, 8, False),\n",
    "            (8, \"í•´ë¬¼íŒŒì „\", 5, 8, 16000.0, 35, 12, False),\n",
    "            (9, \"ë–¡ë³¶ì´\", 1, 3, 7000.0, 90, 25, False),\n",
    "            (10, \"ìˆœë‘ë¶€ì°Œê°œ\", 2, 4, 9000.0, 70, 18, False)\n",
    "        ]\n",
    "        \n",
    "        # Customers  \n",
    "        self.sample_data['customers'] = [\n",
    "            (\"KOREA\", \"í•œêµ­ì‹ë‹¹\", \"ê¹€í•œêµ­\", \"ì‚¬ì¥\", \"ì„œìš¸ì‹œ ì¢…ë¡œêµ¬\", \"ì„œìš¸\", \"í•œêµ­\", \"02-111-2222\"),\n",
    "            (\"JAPAN\", \"ì¼ë³¸ë ˆìŠ¤í† ë‘\", \"ì‚¬í†  ë‹¤ì¹´ì‹œ\", \"ë§¤ë‹ˆì €\", \"ë„ì¿„ë„ ì‹œë¶€ì•¼êµ¬\", \"ë„ì¿„\", \"ì¼ë³¸\", \"+81-3-1234\"),\n",
    "            (\"CHINA\", \"ì¤‘êµ­ë°˜ì \", \"ì™•ì¤‘êµ­\", \"ì‚¬ì¥\", \"ë² ì´ì§•ì‹œ ì°¨ì˜¤ì–‘êµ¬\", \"ë² ì´ì§•\", \"ì¤‘êµ­\", \"+86-10-5678\"),\n",
    "            (\"USA\", \"ì½”ë¦¬ì•ˆ BBQ\", \"John Kim\", \"Owner\", \"LA California\", \"ë¡œìŠ¤ì•¤ì ¤ë ˆìŠ¤\", \"ë¯¸êµ­\", \"+1-213-9999\"),\n",
    "            (\"CANAD\", \"í•œì‹ë‹¹\", \"ì´ë¯¼ìˆ˜\", \"ì‚¬ì¥\", \"í† ë¡ í†  ì˜¨íƒ€ë¦¬ì˜¤\", \"í† ë¡ í† \", \"ìºë‚˜ë‹¤\", \"+1-416-8888\")\n",
    "        ]\n",
    "        \n",
    "        # Employees\n",
    "        today = date.today()\n",
    "        self.sample_data['employees'] = [\n",
    "            (1, \"ê¹€\", \"ì‚¬ì¥\", \"CEO\", date(1970, 1, 1), date(2020, 1, 1), \"ì„œìš¸\", \"í•œêµ­\", None),\n",
    "            (2, \"ì´\", \"ë¶€ì¥\", \"ì˜ì—…ë¶€ì¥\", date(1975, 6, 15), date(2020, 3, 1), \"ì„œìš¸\", \"í•œêµ­\", 1),\n",
    "            (3, \"ë°•\", \"ê³¼ì¥\", \"ë§ˆì¼€íŒ…ê³¼ì¥\", date(1980, 3, 20), date(2021, 1, 15), \"ë¶€ì‚°\", \"í•œêµ­\", 2),\n",
    "            (4, \"ìµœ\", \"ëŒ€ë¦¬\", \"ì˜ì—…ëŒ€ë¦¬\", date(1985, 9, 10), date(2022, 6, 1), \"ëŒ€êµ¬\", \"í•œêµ­\", 2),\n",
    "            (5, \"ì •\", \"ì‚¬ì›\", \"ì˜ì—…ì‚¬ì›\", date(1990, 12, 5), date(2023, 2, 1), \"ê´‘ì£¼\", \"í•œêµ­\", 4)\n",
    "        ]\n",
    "        \n",
    "        # Shippers\n",
    "        self.sample_data['shippers'] = [\n",
    "            (1, \"í•œì§„íƒë°°\", \"1588-0011\"),\n",
    "            (2, \"CJëŒ€í•œí†µìš´\", \"1588-1255\"),\n",
    "            (3, \"ë¡¯ë°íƒë°°\", \"1588-2121\")\n",
    "        ]\n",
    "        \n",
    "        # Orders (ìµœê·¼ 6ê°œì›” ë°ì´í„°)\n",
    "        base_date = today - timedelta(days=180)\n",
    "        self.sample_data['orders'] = []\n",
    "        for i in range(1, 101):  # 100ê°œ ì£¼ë¬¸\n",
    "            order_date = base_date + timedelta(days=np.random.randint(0, 180))\n",
    "            required_date = order_date + timedelta(days=np.random.randint(3, 14))\n",
    "            shipped_date = order_date + timedelta(days=np.random.randint(1, 10)) if np.random.random() > 0.1 else None\n",
    "            \n",
    "            customer_id = np.random.choice([\"KOREA\", \"JAPAN\", \"CHINA\", \"USA\", \"CANAD\"])\n",
    "            employee_id = np.random.randint(1, 6)\n",
    "            shipper_id = np.random.randint(1, 4)\n",
    "            \n",
    "            self.sample_data['orders'].append((\n",
    "                i, customer_id, employee_id, order_date, required_date, shipped_date,\n",
    "                shipper_id, np.round(np.random.uniform(5000, 50000), 2),\n",
    "                f\"ë°°ì†¡ì§€_{i}\", f\"ì£¼ì†Œ_{i}\", f\"ë„ì‹œ_{i}\", \"í•œêµ­\"\n",
    "            ))\n",
    "        \n",
    "        # Order Details\n",
    "        self.sample_data['order_details'] = []\n",
    "        for order_id in range(1, 101):\n",
    "            # ê° ì£¼ë¬¸ë‹¹ 1-5ê°œ ìƒí’ˆ\n",
    "            num_products = np.random.randint(1, 6)\n",
    "            products = np.random.choice(range(1, 11), num_products, replace=False)\n",
    "            \n",
    "            for product_id in products:\n",
    "                # ìƒí’ˆ ê°€ê²© ê°€ì ¸ì˜¤ê¸°\n",
    "                product_price = next(p[4] for p in self.sample_data['products'] if p[0] == product_id)\n",
    "                quantity = np.random.randint(1, 10)\n",
    "                discount = np.round(np.random.choice([0.0, 0.05, 0.1, 0.15], p=[0.7, 0.15, 0.1, 0.05]), 2)\n",
    "                \n",
    "                self.sample_data['order_details'].append((\n",
    "                    order_id, int(product_id), product_price, quantity, discount\n",
    "                ))\n",
    "        \n",
    "        print(\"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ\")\n",
    "        for table, data in self.sample_data.items():\n",
    "            print(f\"   {table}: {len(data)}ê°œ ë ˆì½”ë“œ\")\n",
    "\n",
    "# ìŠ¤í‚¤ë§ˆ ë¶„ì„ê¸° ì´ˆê¸°í™” (embedding_modelì€ ì„ íƒì‚¬í•­)\n",
    "if spark:\n",
    "    # ì„ë² ë”© ëª¨ë¸ ì—†ì´ ì´ˆê¸°í™” (ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ìƒì„±ìš©)\n",
    "    schema_analyzer = DatabaseSchemaAnalyzer(spark, embedding_model=None)\n",
    "    print(\"âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "    # Northwind ë°ì´í„° ë¹Œë” ì´ˆê¸°í™” ë° ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    northwind_builder = NorthwindDataBuilder(spark)\n",
    "    northwind_builder.define_schemas()\n",
    "    northwind_builder.generate_sample_data()\n",
    "    print(\"\\nğŸ—ï¸ Northwind ë°ì´í„° ë¹Œë” ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "else:\n",
    "    schema_analyzer = None\n",
    "    print(\"âš ï¸ Spark ì„¸ì…˜ì´ ì—†ì–´ ìŠ¤í‚¤ë§ˆ ë¶„ì„ê¸°ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# Delta Lake í…Œì´ë¸” ìƒì„± ë° ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "print(\"ğŸ“¦ Delta Lake í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ë¡œë“œ\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ (ì´ˆê¸°í™”)\n",
    "print(\"\\nğŸ”„ ê¸°ì¡´ í…Œì´ë¸” ì •ë¦¬ ì¤‘...\")\n",
    "for table_name in northwind_builder.schemas.keys():\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS northwind.{table_name}\")\n",
    "        print(f\"   ğŸ—‘ï¸ {table_name} í…Œì´ë¸” ì‚­ì œë¨\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ {table_name} ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 2. ìƒ˜í”Œ ë°ì´í„° ì •ì˜\n",
    "sample_data = {\n",
    "    \"categories\": [\n",
    "        (1, \"Beverages\", \"Soft drinks, coffees, teas, beers, and ales\"),\n",
    "        (2, \"Condiments\", \"Sweet and savory sauces, relishes, spreads, and seasonings\"),\n",
    "        (3, \"Dairy Products\", \"Cheeses\"),\n",
    "        (4, \"Grains/Cereals\", \"Breads, crackers, pasta, and cereal\"),\n",
    "        (5, \"Meat/Poultry\", \"Prepared meats\"),\n",
    "        (6, \"Produce\", \"Dried fruit and bean curd\"),\n",
    "        (7, \"Seafood\", \"Seaweed and fish\"),\n",
    "        (8, \"Confections\", \"Desserts, candies, and sweet breads\")\n",
    "    ],\n",
    "    \n",
    "    \"suppliers\": [\n",
    "        (1, \"Exotic Liquids\", \"Charlotte Cooper\", \"Purchasing Manager\", \"49 Gilbert St.\", \"London\", None, \"EC1 4SD\", \"UK\", \"(171) 555-2222\", None, None),\n",
    "        (2, \"New Orleans Cajun Delights\", \"Shelley Burke\", \"Order Administrator\", \"P.O. Box 78934\", \"New Orleans\", \"LA\", \"70117\", \"USA\", \"(100) 555-4822\", None, \"#CAJUN.HTM#\"),\n",
    "        (3, \"Grandma Kelly's Homestead\", \"Regina Murphy\", \"Sales Representative\", \"707 Oxford Rd.\", \"Ann Arbor\", \"MI\", \"48104\", \"USA\", \"(313) 555-5735\", \"(313) 555-3349\", None),\n",
    "        (4, \"Tokyo Traders\", \"Yoshi Nagase\", \"Marketing Manager\", \"9-8 Sekimai Musashino-shi\", \"Tokyo\", None, \"100\", \"Japan\", \"(03) 3555-5011\", None, None),\n",
    "        (5, \"Cooperativa de Quesos 'Las Cabras'\", \"Antonio del Valle Saavedra\", \"Export Administrator\", \"Calle del Rosal 4\", \"Oviedo\", \"Asturias\", \"33007\", \"Spain\", \"(98) 598 76 98\", None, None)\n",
    "    ],\n",
    "    \n",
    "    \"products\": [\n",
    "        (1, \"Chai\", 1, 1, \"10 boxes x 20 bags\", 18.00, 39, 0, 10, False),\n",
    "        (2, \"Chang\", 1, 1, \"24 - 12 oz bottles\", 19.00, 17, 40, 25, False),\n",
    "        (3, \"Aniseed Syrup\", 1, 2, \"12 - 550 ml bottles\", 10.00, 13, 70, 25, False),\n",
    "        (4, \"Chef Anton's Cajun Seasoning\", 2, 2, \"48 - 6 oz jars\", 22.00, 53, 0, 0, False),\n",
    "        (5, \"Chef Anton's Gumbo Mix\", 2, 2, \"36 boxes\", 21.35, 0, 0, 0, True),\n",
    "        (6, \"Grandma's Boysenberry Spread\", 3, 2, \"12 - 8 oz jars\", 25.00, 120, 0, 25, False),\n",
    "        (7, \"Uncle Bob's Organic Dried Pears\", 3, 7, \"12 - 1 lb pkgs.\", 30.00, 15, 0, 10, False),\n",
    "        (8, \"Northwoods Cranberry Sauce\", 3, 2, \"12 - 12 oz jars\", 40.00, 6, 0, 0, False),\n",
    "        (9, \"Mishi Kobe Niku\", 4, 6, \"18 - 500 g pkgs.\", 97.00, 29, 0, 0, True),\n",
    "        (10, \"Ikura\", 4, 8, \"12 - 200 ml jars\", 31.00, 31, 0, 0, False)\n",
    "    ],\n",
    "    \n",
    "    \"customers\": [\n",
    "        (\"ALFKI\", \"Alfreds Futterkiste\", \"Maria Anders\", \"Sales Representative\", \"Obere Str. 57\", \"Berlin\", None, \"12209\", \"Germany\", \"030-0074321\", \"030-0076545\"),\n",
    "        (\"ANATR\", \"Ana Trujillo Emparedados y helados\", \"Ana Trujillo\", \"Owner\", \"Avda. de la ConstituciÃ³n 2222\", \"MÃ©xico D.F.\", None, \"05021\", \"Mexico\", \"(5) 555-4729\", \"(5) 555-3745\"),\n",
    "        (\"ANTON\", \"Antonio Moreno TaquerÃ­a\", \"Antonio Moreno\", \"Owner\", \"Mataderos 2312\", \"MÃ©xico D.F.\", None, \"05023\", \"Mexico\", \"(5) 555-3932\", None),\n",
    "        (\"AROUT\", \"Around the Horn\", \"Thomas Hardy\", \"Sales Representative\", \"120 Hanover Sq.\", \"London\", None, \"WA1 1DP\", \"UK\", \"(171) 555-7788\", \"(171) 555-6750\"),\n",
    "        (\"BERGS\", \"Berglunds snabbkÃ¶p\", \"Christina Berglund\", \"Order Administrator\", \"BerguvsvÃ¤gen 8\", \"LuleÃ¥\", None, \"S-958 22\", \"Sweden\", \"0921-12 34 65\", \"0921-12 34 67\"),\n",
    "        (\"BLAUS\", \"Blauer See Delikatessen\", \"Hanna Moos\", \"Sales Representative\", \"Forsterstr. 57\", \"Mannheim\", None, \"68306\", \"Germany\", \"0621-08460\", \"0621-08924\"),\n",
    "        (\"BLONP\", \"Blondesddsl pÃ¨re et fils\", \"FrÃ©dÃ©rique Citeaux\", \"Marketing Manager\", \"24, place KlÃ©ber\", \"Strasbourg\", None, \"67000\", \"France\", \"88.60.15.31\", \"88.60.15.32\"),\n",
    "        (\"BOLID\", \"BÃ³lido Comidas preparadas\", \"MartÃ­n Sommer\", \"Owner\", \"C/ Araquil, 67\", \"Madrid\", None, \"28023\", \"Spain\", \"(91) 555 22 82\", \"(91) 555 91 99\"),\n",
    "        (\"BONAP\", \"Bon app'\", \"Laurence Lebihan\", \"Owner\", \"12, rue des Bouchers\", \"Marseille\", None, \"13008\", \"France\", \"91.24.45.40\", \"91.24.45.41\"),\n",
    "        (\"BOTTM\", \"Bottom-Dollar Markets\", \"Elizabeth Lincoln\", \"Accounting Manager\", \"23 Tsawassen Blvd.\", \"Tsawassen\", \"BC\", \"T2F 8M4\", \"Canada\", \"(604) 555-4729\", \"(604) 555-3745\")\n",
    "    ],\n",
    "    \n",
    "    \"employees\": [\n",
    "        (1, \"Davolio\", \"Nancy\", \"Sales Representative\", \"Ms.\", date(1948, 12, 8), date(1992, 5, 1), \"507 - 20th Ave. E. Apt. 2A\", \"Seattle\", \"WA\", \"98122\", \"USA\", \"(206) 555-9857\", \"5467\", \"Education includes a BA in psychology from Colorado State University in 1970.\", None),\n",
    "        (2, \"Fuller\", \"Andrew\", \"Vice President, Sales\", \"Dr.\", date(1952, 2, 19), date(1992, 8, 14), \"908 W. Capital Way\", \"Tacoma\", \"WA\", \"98401\", \"USA\", \"(206) 555-9482\", \"3457\", \"Andrew received his BTS commercial in 1974 and a Ph.D. in international marketing from the University of Dallas in 1981.\", None),\n",
    "        (3, \"Leverling\", \"Janet\", \"Sales Representative\", \"Ms.\", date(1963, 8, 30), date(1992, 4, 1), \"722 Moss Bay Blvd.\", \"Kirkland\", \"WA\", \"98033\", \"USA\", \"(206) 555-3412\", \"3355\", \"Janet has a BS degree in chemistry from Boston College (1984).\", 2),\n",
    "        (4, \"Peacock\", \"Margaret\", \"Sales Representative\", \"Mrs.\", date(1937, 9, 19), date(1993, 5, 3), \"4110 Old Redmond Rd.\", \"Redmond\", \"WA\", \"98052\", \"USA\", \"(206) 555-8122\", \"5176\", \"Margaret holds a BA in English literature from Concordia College (1958) and an MA from the American Institute of Culinary Arts (1966).\", 2),\n",
    "        (5, \"Buchanan\", \"Steven\", \"Sales Manager\", \"Mr.\", date(1955, 3, 4), date(1993, 10, 17), \"14 Garrett Hill\", \"London\", None, \"SW1 8JR\", \"UK\", \"(71) 555-4848\", \"3453\", \"Steven Buchanan graduated from St. Andrews University, Scotland, with a BSC degree in 1976.\", 2)\n",
    "    ],\n",
    "    \n",
    "    \"shippers\": [\n",
    "        (1, \"Speedy Express\", \"(503) 555-9831\"),\n",
    "        (2, \"United Package\", \"(503) 555-3199\"),\n",
    "        (3, \"Federal Shipping\", \"(503) 555-9931\")\n",
    "    ],\n",
    "    \n",
    "    \"orders\": [\n",
    "        (10248, \"VINET\", 5, date(1996, 7, 4), date(1996, 8, 1), date(1996, 7, 16), 3, 32.38, \"Vins et alcools Chevalier\", \"59 rue de l'Abbaye\", \"Reims\", None, \"51100\", \"France\"),\n",
    "        (10249, \"TOMSP\", 6, date(1996, 7, 5), date(1996, 8, 16), date(1996, 7, 10), 1, 11.61, \"Toms SpezialitÃ¤ten\", \"Luisenstr. 48\", \"MÃ¼nster\", None, \"44087\", \"Germany\"),\n",
    "        (10250, \"HANAR\", 4, date(1996, 7, 8), date(1996, 8, 5), date(1996, 7, 12), 2, 65.83, \"Hanari Carnes\", \"Rua do PaÃ§o, 67\", \"Rio de Janeiro\", \"RJ\", \"05454-876\", \"Brazil\"),\n",
    "        (10251, \"VICTE\", 3, date(1996, 7, 8), date(1996, 8, 5), date(1996, 7, 15), 1, 41.34, \"Victuailles en stock\", \"2, rue du Commerce\", \"Lyon\", None, \"69004\", \"France\"),\n",
    "        (10252, \"SUPRD\", 4, date(1996, 7, 9), date(1996, 8, 6), date(1996, 7, 11), 2, 51.30, \"SuprÃªmes dÃ©lices\", \"Boulevard Tirou, 255\", \"Charleroi\", None, \"B-6000\", \"Belgium\"),\n",
    "        (10253, \"HANAR\", 3, date(1996, 7, 10), date(1996, 7, 24), date(1996, 7, 16), 2, 58.17, \"Hanari Carnes\", \"Rua do PaÃ§o, 67\", \"Rio de Janeiro\", \"RJ\", \"05454-876\", \"Brazil\"),\n",
    "        (10254, \"CHOPS\", 5, date(1996, 7, 11), date(1996, 8, 8), date(1996, 7, 23), 2, 22.98, \"Chop-suey Chinese\", \"Hauptstr. 31\", \"Bern\", None, \"3012\", \"Switzerland\"),\n",
    "        (10255, \"RICSU\", 9, date(1996, 7, 12), date(1996, 8, 9), date(1996, 7, 15), 3, 148.33, \"Richter Supermarkt\", \"Starenweg 5\", \"GenÃ¨ve\", None, \"1204\", \"Switzerland\"),\n",
    "        (10256, \"WELLI\", 3, date(1996, 7, 15), date(1996, 8, 12), date(1996, 7, 17), 2, 13.97, \"Wellington Importadora\", \"Rua do Mercado, 12\", \"Resende\", \"SP\", \"08737-363\", \"Brazil\"),\n",
    "        (10257, \"HILAA\", 4, date(1996, 7, 16), date(1996, 8, 13), date(1996, 7, 22), 3, 81.91, \"HILARION-Abastos\", \"Carrera 22 con Ave. Carlos Soublette #8-35\", \"San CristÃ³bal\", \"TÃ¡chira\", \"5022\", \"Venezuela\")\n",
    "    ],\n",
    "    \n",
    "    \"order_details\": [\n",
    "        (10248, 11, 14.00, 12, 0.00),\n",
    "        (10248, 42, 9.80, 10, 0.00),\n",
    "        (10248, 72, 34.80, 5, 0.00),\n",
    "        (10249, 14, 18.60, 9, 0.00),\n",
    "        (10249, 51, 42.40, 40, 0.00),\n",
    "        (10250, 41, 7.70, 10, 0.00),\n",
    "        (10250, 51, 42.40, 35, 0.15),\n",
    "        (10250, 65, 16.80, 15, 0.15),\n",
    "        (10251, 22, 16.80, 6, 0.05),\n",
    "        (10251, 57, 15.60, 15, 0.05),\n",
    "        (10251, 65, 16.80, 20, 0.00),\n",
    "        (10252, 20, 64.80, 40, 0.05),\n",
    "        (10252, 33, 2.00, 25, 0.05),\n",
    "        (10252, 60, 27.20, 40, 0.00),\n",
    "        (10253, 31, 10.00, 20, 0.00),\n",
    "        (10253, 39, 14.40, 42, 0.00),\n",
    "        (10253, 49, 16.00, 40, 0.00)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“Š ìƒ˜í”Œ ë°ì´í„° ì •ì˜ ì™„ë£Œ:\")\n",
    "for table_name, data in sample_data.items():\n",
    "    print(f\"   ğŸ“ˆ {table_name}: {len(data)}ê°œ ë ˆì½”ë“œ\")\n",
    "\n",
    "print(\"\\nâœ… ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d014cd3",
   "metadata": {},
   "source": [
    "## 4. í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ì‚½ì…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d651501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰ ë° SQL ìƒì„±ê¸° ì •ì˜ ì™„ë£Œ\n",
      "ğŸš€ Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘...\n",
      "ğŸ”§ Delta Lake í…Œì´ë¸” ìƒì„± ì‹œì‘...\n",
      "\n",
      "ğŸ“‹ categories í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… categories: 8ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ suppliers í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… categories: 8ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ suppliers í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… suppliers: 5ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ products í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… suppliers: 5ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ products í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… products: 10ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ customers í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… products: 10ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ customers í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… customers: 5ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ employees í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… customers: 5ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ employees í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… employees: 5ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ shippers í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… employees: 5ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ shippers í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… shippers: 3ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ orders í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… shippers: 3ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ orders í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… orders: 100ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ order_details í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… orders: 100ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“‹ order_details í…Œì´ë¸” ìƒì„± ì¤‘...\n",
      "   âœ… order_details: 290ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“ˆ í…Œì´ë¸” ìƒì„± ê²°ê³¼:\n",
      "   âœ… ì„±ê³µ: 8ê°œ í…Œì´ë¸”\n",
      "   âŒ ì‹¤íŒ¨: 0ê°œ í…Œì´ë¸”\n",
      "\n",
      "âœ… ìƒì„±ëœ í…Œì´ë¸”:\n",
      "   ğŸ“‚ northwind.categories\n",
      "   ğŸ“‚ northwind.suppliers\n",
      "   ğŸ“‚ northwind.products\n",
      "   ğŸ“‚ northwind.customers\n",
      "   ğŸ“‚ northwind.employees\n",
      "   ğŸ“‚ northwind.shippers\n",
      "   ğŸ“‚ northwind.orders\n",
      "   ğŸ“‚ northwind.order_details\n",
      "\n",
      "ğŸ‰ ì´ 8ê°œ Northwind í…Œì´ë¸”ì´ Delta Lakeì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨\n",
      "   âœ… order_details: 290ê°œ ë ˆì½”ë“œ ë¡œë“œ\n",
      "\n",
      "ğŸ“ˆ í…Œì´ë¸” ìƒì„± ê²°ê³¼:\n",
      "   âœ… ì„±ê³µ: 8ê°œ í…Œì´ë¸”\n",
      "   âŒ ì‹¤íŒ¨: 0ê°œ í…Œì´ë¸”\n",
      "\n",
      "âœ… ìƒì„±ëœ í…Œì´ë¸”:\n",
      "   ğŸ“‚ northwind.categories\n",
      "   ğŸ“‚ northwind.suppliers\n",
      "   ğŸ“‚ northwind.products\n",
      "   ğŸ“‚ northwind.customers\n",
      "   ğŸ“‚ northwind.employees\n",
      "   ğŸ“‚ northwind.shippers\n",
      "   ğŸ“‚ northwind.orders\n",
      "   ğŸ“‚ northwind.order_details\n",
      "\n",
      "ğŸ‰ ì´ 8ê°œ Northwind í…Œì´ë¸”ì´ Delta Lakeì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class RAGSchemaRetriever:\n",
    "    \"\"\"RAG ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰ ë° ë§¤ì¹­\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorized_schemas: List[Dict[str, Any]], embedding_model=None):\n",
    "        self.vectorized_schemas = vectorized_schemas\n",
    "        self.embedding_model = embedding_model\n",
    "        self.schema_embeddings = None\n",
    "        self.schema_texts = None\n",
    "        \n",
    "        # ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±\n",
    "        self._build_embedding_matrix()\n",
    "    \n",
    "    def _build_embedding_matrix(self):\n",
    "        \"\"\"ìŠ¤í‚¤ë§ˆ ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±\"\"\"\n",
    "        embeddings = []\n",
    "        texts = []\n",
    "        \n",
    "        for schema in self.vectorized_schemas:\n",
    "            if 'embedding' in schema:\n",
    "                embeddings.append(schema['embedding'])\n",
    "                texts.append(schema.get('schema_text', ''))\n",
    "            \n",
    "        if embeddings:\n",
    "            self.schema_embeddings = np.array(embeddings)\n",
    "            self.schema_texts = texts\n",
    "            print(f\"âœ… ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„± ì™„ë£Œ: {self.schema_embeddings.shape}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    def search_relevant_schemas(self, question: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰\"\"\"\n",
    "        if not self.embedding_model or self.schema_embeddings is None:\n",
    "            print(\"âš ï¸ ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "            return self._keyword_based_search(question, top_k)\n",
    "        \n",
    "        try:\n",
    "            # ì§ˆë¬¸ ì„ë² ë”©\n",
    "            question_embedding = self.embedding_model.embed_query(question)\n",
    "            question_vector = np.array(question_embedding).reshape(1, -1)\n",
    "            \n",
    "            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "            similarities = cosine_similarity(question_vector, self.schema_embeddings)[0]\n",
    "            \n",
    "            # ìƒìœ„ kê°œ ì¸ë±ìŠ¤\n",
    "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "            \n",
    "            # ê²°ê³¼ êµ¬ì„±\n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                schema = self.vectorized_schemas[idx].copy()\n",
    "                schema['similarity_score'] = float(similarities[idx])\n",
    "                results.append(schema)\n",
    "            \n",
    "            print(f\"ğŸ” ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ìŠ¤í‚¤ë§ˆ ë§¤ì¹­\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}\")\n",
    "            return self._keyword_based_search(question, top_k)\n",
    "    \n",
    "    def _keyword_based_search(self, question: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"í‚¤ì›Œë“œ ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰ (ë°±ì—… ë°©ë²•)\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # í‚¤ì›Œë“œ ë§¤ì¹­ ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "        scored_schemas = []\n",
    "        \n",
    "        for schema in self.vectorized_schemas:\n",
    "            score = 0\n",
    "            \n",
    "            # í…Œì´ë¸”ëª… ë§¤ì¹­\n",
    "            if question_lower in schema['table_name'].lower():\n",
    "                score += 10\n",
    "            \n",
    "            # ì»¬ëŸ¼ëª… ë§¤ì¹­\n",
    "            for col in schema['columns']:\n",
    "                if question_lower in col['name'].lower():\n",
    "                    score += 5\n",
    "                if col.get('comment') and question_lower in col['comment'].lower():\n",
    "                    score += 3\n",
    "            \n",
    "            # ìƒ˜í”Œ ë°ì´í„° ë§¤ì¹­\n",
    "            sample_text = str(schema.get('sample_data', '')).lower()\n",
    "            if question_lower in sample_text:\n",
    "                score += 2\n",
    "            \n",
    "            if score > 0:\n",
    "                schema_copy = schema.copy()\n",
    "                schema_copy['similarity_score'] = score\n",
    "                scored_schemas.append(schema_copy)\n",
    "        \n",
    "        # ìŠ¤ì½”ì–´ë¡œ ì •ë ¬\n",
    "        scored_schemas.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        \n",
    "        print(f\"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(scored_schemas[:top_k])}ê°œ ìŠ¤í‚¤ë§ˆ ë§¤ì¹­\")\n",
    "        return scored_schemas[:top_k]\n",
    "\n",
    "class TextToSQLGenerator:\n",
    "    \"\"\"Text-to-SQL ìƒì„±ê¸°\"\"\"\n",
    "    \n",
    "    def __init__(self, foundation_model, schema_retriever: RAGSchemaRetriever):\n",
    "        self.foundation_model = foundation_model\n",
    "        self.schema_retriever = schema_retriever\n",
    "        \n",
    "    def generate_sql(self, question: str, top_k_schemas: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"ìì—°ì–´ ì§ˆë¬¸ì„ SQLë¡œ ë³€í™˜\"\"\"\n",
    "        try:\n",
    "            # 1. ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰\n",
    "            relevant_schemas = self.schema_retriever.search_relevant_schemas(question, top_k_schemas)\n",
    "            \n",
    "            if not relevant_schemas:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"ê´€ë ¨ëœ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\",\n",
    "                    \"sql_query\": None\n",
    "                }\n",
    "            \n",
    "            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "            prompt = self._create_prompt(question, relevant_schemas)\n",
    "            \n",
    "            # 3. LLMìœ¼ë¡œ SQL ìƒì„±\n",
    "            if self.foundation_model:\n",
    "                response = self.foundation_model.predict(prompt)\n",
    "                sql_query = self._extract_sql_from_response(response)\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\",\n",
    "                    \"sql_query\": None\n",
    "                }\n",
    "            \n",
    "            # 4. ê²°ê³¼ êµ¬ì„±\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"sql_query\": sql_query,\n",
    "                \"relevant_schemas\": relevant_schemas,\n",
    "                \"raw_response\": response,\n",
    "                \"prompt_used\": prompt\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"SQL ìƒì„± ì‹¤íŒ¨: {str(e)}\",\n",
    "                \"sql_query\": None\n",
    "            }\n",
    "    \n",
    "    def _create_prompt(self, question: str, schemas: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"RAG ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
    "        schema_context = \"\"\n",
    "        \n",
    "        for i, schema in enumerate(schemas, 1):\n",
    "            schema_context += f\"\\n=== í…Œì´ë¸” {i}: {schema['full_name']} ===\\n\"\n",
    "            schema_context += f\"í–‰ ìˆ˜: {schema.get('row_count', 0):,}\\n\"\n",
    "            schema_context += \"ì»¬ëŸ¼:\\n\"\n",
    "            \n",
    "            for col in schema['columns']:\n",
    "                comment = f\" -- {col['comment']}\" if col.get('comment') else \"\"\n",
    "                schema_context += f\"  - {col['name']} ({col['type']}){comment}\\n\"\n",
    "            \n",
    "            # ìƒ˜í”Œ ë°ì´í„° ì¼ë¶€ í¬í•¨\n",
    "            if schema.get('sample_data'):\n",
    "                schema_context += \"ìƒ˜í”Œ ë°ì´í„° (ì¼ë¶€):\\n\"\n",
    "                for sample in schema['sample_data'][:2]:\n",
    "                    # ì¤‘ìš”í•œ í•„ë“œë§Œ í‘œì‹œ\n",
    "                    key_fields = list(sample.keys())[:3]\n",
    "                    sample_str = {k: sample[k] for k in key_fields}\n",
    "                    schema_context += f\"  {sample_str}\\n\"\n",
    "            \n",
    "            if 'similarity_score' in schema:\n",
    "                schema_context += f\"ë§¤ì¹­ ì ìˆ˜: {schema['similarity_score']:.3f}\\n\"\n",
    "        \n",
    "        prompt = f\"\"\"ë‹¹ì‹ ì€ Databricks í™˜ê²½ì˜ SQL ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ì–´ ì§ˆë¬¸ì„ ì •í™•í•œ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.\n",
    "\n",
    "=== ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ ===\n",
    "{schema_context}\n",
    "\n",
    "=== ë³€í™˜ ê·œì¹™ ===\n",
    "1. ìœ„ì— ì œê³µëœ í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ëª…ë§Œ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "2. Databricks SQL ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "3. ì ì ˆí•œ JOIN, WHERE, GROUP BY, ORDER BYë¥¼ í™œìš©í•˜ì„¸ìš”\n",
    "4. ê²°ê³¼ëŠ” ì‹¤í–‰ ê°€ëŠ¥í•œ SQL ì¿¼ë¦¬ë§Œ ë°˜í™˜í•˜ì„¸ìš”\n",
    "5. ì¿¼ë¦¬ë¥¼ ```sqlê³¼ ``` ì‚¬ì´ì— ì‘ì„±í•˜ì„¸ìš”\n",
    "\n",
    "=== ì˜ˆì‹œ ===\n",
    "ì§ˆë¬¸: \"ê³ ê°ë³„ ì£¼ë¬¸ ê±´ìˆ˜ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”\"\n",
    "```sql\n",
    "SELECT customer_id, COUNT(*) as order_count\n",
    "FROM orders\n",
    "GROUP BY customer_id\n",
    "ORDER BY order_count DESC;\n",
    "```\n",
    "\n",
    "=== ì§ˆë¬¸ ===\n",
    "{question}\n",
    "\n",
    "=== ë‹µë³€ ===\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _extract_sql_from_response(self, response: str) -> str:\n",
    "        \"\"\"LLM ì‘ë‹µì—ì„œ SQL ì¿¼ë¦¬ ì¶”ì¶œ\"\"\"\n",
    "        # ```sqlê³¼ ``` ì‚¬ì´ì˜ ë‚´ìš© ì¶”ì¶œ\n",
    "        sql_pattern = r'```sql\\s*(.*?)\\s*```'\n",
    "        matches = re.findall(sql_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if matches:\n",
    "            return matches[0].strip()\n",
    "        \n",
    "        # SQL í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì°¾ê¸°\n",
    "        lines = response.strip().split('\\n')\n",
    "        sql_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'WITH', 'CREATE']\n",
    "        \n",
    "        for line in lines:\n",
    "            line_upper = line.strip().upper()\n",
    "            if any(line_upper.startswith(keyword) for keyword in sql_keywords):\n",
    "                return line.strip()\n",
    "        \n",
    "        return response.strip()\n",
    "\n",
    "print(\"âœ… RAG ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰ ë° SQL ìƒì„±ê¸° ì •ì˜ ì™„ë£Œ\")\n",
    "\n",
    "def create_delta_tables():\n",
    "    \"\"\"Delta Lake í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "    \n",
    "    if not spark or not northwind_builder:\n",
    "        print(\"âŒ Spark ì„¸ì…˜ ë˜ëŠ” ë°ì´í„° ë¹Œë”ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        return False\n",
    "    \n",
    "    created_tables = []\n",
    "    failed_tables = []\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ”§ Delta Lake í…Œì´ë¸” ìƒì„± ì‹œì‘...\")\n",
    "        \n",
    "        # ê° í…Œì´ë¸”ë³„ë¡œ ìƒì„±\n",
    "        for table_name, schema in northwind_builder.schemas.items():\n",
    "            try:\n",
    "                print(f\"\\nğŸ“‹ {table_name} í…Œì´ë¸” ìƒì„± ì¤‘...\")\n",
    "                \n",
    "                # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "                data = northwind_builder.sample_data[table_name]\n",
    "                df = spark.createDataFrame(data, schema)\n",
    "                \n",
    "                # Delta í…Œì´ë¸”ë¡œ ì €ì¥ (ë®ì–´ì“°ê¸°)\n",
    "                df.write \\\n",
    "                  .format(\"delta\") \\\n",
    "                  .mode(\"overwrite\") \\\n",
    "                  .option(\"mergeSchema\", \"true\") \\\n",
    "                  .saveAsTable(f\"northwind.{table_name}\")\n",
    "                \n",
    "                # ë°ì´í„° í™•ì¸\n",
    "                count = spark.table(f\"northwind.{table_name}\").count()\n",
    "                print(f\"   âœ… {table_name}: {count}ê°œ ë ˆì½”ë“œ ë¡œë“œ\")\n",
    "                \n",
    "                created_tables.append(table_name)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ {table_name} í…Œì´ë¸” ìƒì„±/ì‚½ì… ì‹¤íŒ¨: {e}\")\n",
    "                failed_tables.append((table_name, str(e)))\n",
    "\n",
    "        print(f\"\\nğŸ“ˆ í…Œì´ë¸” ìƒì„± ê²°ê³¼:\")\n",
    "        print(f\"   âœ… ì„±ê³µ: {len(created_tables)}ê°œ í…Œì´ë¸”\")\n",
    "        print(f\"   âŒ ì‹¤íŒ¨: {len(failed_tables)}ê°œ í…Œì´ë¸”\")\n",
    "\n",
    "        if created_tables:\n",
    "            print(f\"\\nâœ… ìƒì„±ëœ í…Œì´ë¸”:\")\n",
    "            for table in created_tables:\n",
    "                print(f\"   ğŸ“‚ northwind.{table}\")\n",
    "\n",
    "        if failed_tables:\n",
    "            print(f\"\\nâŒ ì‹¤íŒ¨í•œ í…Œì´ë¸”:\")\n",
    "            for table, error in failed_tables:\n",
    "                print(f\"   ğŸ“‚ {table}: {error}\")\n",
    "\n",
    "        print(f\"\\nğŸ‰ ì´ {len(created_tables)}ê°œ Northwind í…Œì´ë¸”ì´ Delta Lakeì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def verify_northwind_database():\n",
    "    \"\"\"Northwind ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦\"\"\"\n",
    "    \n",
    "    if not spark:\n",
    "        print(\"âŒ Spark ì„¸ì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ” Northwind ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì¤‘...\")\n",
    "        \n",
    "        # í…Œì´ë¸” ëª©ë¡ í™•ì¸\n",
    "        tables = spark.sql(\"SHOW TABLES IN northwind\").collect()\n",
    "        table_names = [row.tableName for row in tables]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ìƒì„±ëœ í…Œì´ë¸” ({len(table_names)}ê°œ):\")\n",
    "        \n",
    "        verification_results = {}\n",
    "        \n",
    "        for table_name in sorted(table_names):\n",
    "            # í…Œì´ë¸” ì •ë³´ ì¡°íšŒ\n",
    "            df = spark.table(f\"northwind.{table_name}\")\n",
    "            count = df.count()\n",
    "            columns = len(df.columns)\n",
    "            \n",
    "            print(f\"   ğŸ“‹ {table_name}:\")\n",
    "            print(f\"      - ë ˆì½”ë“œ ìˆ˜: {count:,}\")\n",
    "            print(f\"      - ì»¬ëŸ¼ ìˆ˜: {columns}\")\n",
    "            \n",
    "            # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "            sample = df.limit(3).toPandas()\n",
    "            print(f\"      - ìƒ˜í”Œ: {sample.iloc[0].to_dict() if len(sample) > 0 else 'No data'}\")\n",
    "            \n",
    "            verification_results[table_name] = {\n",
    "                \"record_count\": count,\n",
    "                \"column_count\": columns,\n",
    "                \"status\": \"OK\" if count > 0 else \"EMPTY\"\n",
    "            }\n",
    "        \n",
    "        # ê´€ê³„ ê²€ì¦ (ê¸°ë³¸ì ì¸ JOIN í…ŒìŠ¤íŠ¸)\n",
    "        print(f\"\\nğŸ”— ê´€ê³„ ê²€ì¦:\")\n",
    "        \n",
    "        # ì£¼ë¬¸-ê³ ê° ê´€ê³„\n",
    "        join_test = spark.sql(\"\"\"\n",
    "            SELECT COUNT(*) as order_count \n",
    "            FROM northwind.orders o \n",
    "            INNER JOIN northwind.customers c ON o.customer_id = c.customer_id\n",
    "        \"\"\").collect()[0].order_count\n",
    "        \n",
    "        print(f\"   âœ… ì£¼ë¬¸-ê³ ê° JOIN: {join_test}ê°œ ë§¤ì¹­\")\n",
    "        \n",
    "        # ì£¼ë¬¸ìƒì„¸-ìƒí’ˆ ê´€ê³„  \n",
    "        join_test2 = spark.sql(\"\"\"\n",
    "            SELECT COUNT(*) as detail_count\n",
    "            FROM northwind.order_details od\n",
    "            INNER JOIN northwind.products p ON od.product_id = p.product_id\n",
    "        \"\"\").collect()[0].detail_count\n",
    "        \n",
    "        print(f\"   âœ… ì£¼ë¬¸ìƒì„¸-ìƒí’ˆ JOIN: {join_test2}ê°œ ë§¤ì¹­\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ê²€ì¦ ì™„ë£Œ! Text-to-SQL í…ŒìŠ¤íŠ¸ ì¤€ë¹„ë¨\")\n",
    "        return verification_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# í…Œì´ë¸” ìƒì„± ì‹¤í–‰\n",
    "if 'northwind_builder' in locals():\n",
    "    print(\"ğŸš€ Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘...\")\n",
    "    \n",
    "    success = create_delta_tables()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        verification_results = verify_northwind_database()\n",
    "        \n",
    "        if verification_results:\n",
    "            print(\"\\nâœ… Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì„±ê³µ!\")\n",
    "            print(\"ğŸ”— ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ LangChain Agentë¥¼ í™œìš©í•œ Text-to-SQLì„ êµ¬í˜„í•©ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì‹¤íŒ¨\")\n",
    "    else:\n",
    "        print(\"âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨\")\n",
    "else:\n",
    "    print(\"âŒ ë°ì´í„° ë¹Œë”ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbfa738",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„° ê²€ì¦ ë° ê¸°ë³¸ SQL í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2b8bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê¸°ë³¸ Text-to-SQL í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ì™„ë£Œ\n",
      "âœ… SQL ì‹¤í–‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ\n",
      "ğŸ§ª Northwind ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n",
      "\n",
      "ğŸ” í…ŒìŠ¤íŠ¸ 1: ì „ì²´ ê³ ê° ìˆ˜\n",
      "ğŸ“ ì„¤ëª…: ë“±ë¡ëœ ê³ ê°ì˜ ì´ ê°œìˆ˜\n",
      "âœ… ì‹¤í–‰ ì„±ê³µ (1ê°œ í–‰)\n",
      "ğŸ“Š ê²°ê³¼:\n",
      "   1. customer_count: 5\n",
      "--------------------------------------------------\n",
      "ğŸ” í…ŒìŠ¤íŠ¸ 2: ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜\n",
      "ğŸ“ ì„¤ëª…: ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ê°œìˆ˜\n",
      "âœ… ì‹¤í–‰ ì„±ê³µ (1ê°œ í–‰)\n",
      "ğŸ“Š ê²°ê³¼:\n",
      "   1. customer_count: 5\n",
      "--------------------------------------------------\n",
      "ğŸ” í…ŒìŠ¤íŠ¸ 2: ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜\n",
      "ğŸ“ ì„¤ëª…: ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ê°œìˆ˜\n",
      "âœ… ì‹¤í–‰ ì„±ê³µ (8ê°œ í–‰)\n",
      "ğŸ“Š ê²°ê³¼:\n",
      "   1. category_name: ìœ¡ë¥˜/ê°€ê¸ˆë¥˜, product_count: 3\n",
      "   2. category_name: ê³¡ë¬¼/ì‹œë¦¬ì–¼, product_count: 2\n",
      "   3. category_name: ì¡°ë¯¸ë£Œ, product_count: 2\n",
      "   4. category_name: ê³¼ìë¥˜, product_count: 1\n",
      "   5. category_name: í•´ì‚°ë¬¼, product_count: 1\n",
      "   ... ê·¸ ì™¸ 3ê°œ í–‰\n",
      "--------------------------------------------------\n",
      "ğŸ” í…ŒìŠ¤íŠ¸ 3: ì›”ë³„ ì£¼ë¬¸ í˜„í™©\n",
      "ğŸ“ ì„¤ëª…: ìµœê·¼ ì›”ë³„ ì£¼ë¬¸ ê±´ìˆ˜ ë° í‰ê·  ë°°ì†¡ë¹„\n",
      "âœ… ì‹¤í–‰ ì„±ê³µ (8ê°œ í–‰)\n",
      "ğŸ“Š ê²°ê³¼:\n",
      "   1. category_name: ìœ¡ë¥˜/ê°€ê¸ˆë¥˜, product_count: 3\n",
      "   2. category_name: ê³¡ë¬¼/ì‹œë¦¬ì–¼, product_count: 2\n",
      "   3. category_name: ì¡°ë¯¸ë£Œ, product_count: 2\n",
      "   4. category_name: ê³¼ìë¥˜, product_count: 1\n",
      "   5. category_name: í•´ì‚°ë¬¼, product_count: 1\n",
      "   ... ê·¸ ì™¸ 3ê°œ í–‰\n",
      "--------------------------------------------------\n",
      "ğŸ” í…ŒìŠ¤íŠ¸ 3: ì›”ë³„ ì£¼ë¬¸ í˜„í™©\n",
      "ğŸ“ ì„¤ëª…: ìµœê·¼ ì›”ë³„ ì£¼ë¬¸ ê±´ìˆ˜ ë° í‰ê·  ë°°ì†¡ë¹„\n",
      "âœ… ì‹¤í–‰ ì„±ê³µ (6ê°œ í–‰)\n",
      "ğŸ“Š ê²°ê³¼:\n",
      "   1. year: 2025.0, month: 6.0, order_count: 15.0, avg_freight: 27776.4\n",
      "   2. year: 2025.0, month: 5.0, order_count: 12.0, avg_freight: 31256.24\n",
      "   3. year: 2025.0, month: 4.0, order_count: 22.0, avg_freight: 26207.64\n",
      "   4. year: 2025.0, month: 3.0, order_count: 16.0, avg_freight: 28940.67\n",
      "   5. year: 2025.0, month: 2.0, order_count: 16.0, avg_freight: 26968.31\n",
      "   ... ê·¸ ì™¸ 1ê°œ í–‰\n",
      "--------------------------------------------------\n",
      "ğŸ” í…ŒìŠ¤íŠ¸ 4: ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìƒí’ˆ TOP 5\n",
      "ğŸ“ ì„¤ëª…: íŒë§¤ëŸ‰ ê¸°ì¤€ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìƒí’ˆ\n",
      "âœ… ì‹¤í–‰ ì„±ê³µ (6ê°œ í–‰)\n",
      "ğŸ“Š ê²°ê³¼:\n",
      "   1. year: 2025.0, month: 6.0, order_count: 15.0, avg_freight: 27776.4\n",
      "   2. year: 2025.0, month: 5.0, order_count: 12.0, avg_freight: 31256.24\n",
      "   3. year: 2025.0, month: 4.0, order_count: 22.0, avg_freight: 26207.64\n",
      "   4. year: 2025.0, month: 3.0, order_count: 16.0, avg_freight: 28940.67\n",
      "   5. year: 2025.0, month: 2.0, order_count: 16.0, avg_freight: 26968.31\n",
      "   ... ê·¸ ì™¸ 1ê°œ í–‰\n",
      "--------------------------------------------------\n",
      "ğŸ” í…ŒìŠ¤íŠ¸ 4: ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìƒí’ˆ TOP 5\n",
      "ğŸ“ ì„¤ëª…: íŒë§¤ëŸ‰ ê¸°ì¤€ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìƒí’ˆ\n",
      "âœ… ì‹¤í–‰ ì„±ê³µ (5ê°œ í–‰)\n",
      "ğŸ“Š ê²°ê³¼:\n",
      "   1. product_name: í•´ë¬¼íŒŒì „, total_quantity: 184, total_revenue: 2880000.0\n",
      "   2. product_name: ìˆœë‘ë¶€ì°Œê°œ, total_quantity: 180, total_revenue: 1591200.0\n",
      "   3. product_name: ê¹€ì¹˜, total_quantity: 156, total_revenue: 2264250.0\n",
      "   4. product_name: ì‚¼ê²¹ì‚´, total_quantity: 150, total_revenue: 3202100.0\n",
      "   5. product_name: ë¹„ë¹”ë°¥, total_quantity: 148, total_revenue: 1724400.0\n",
      "--------------------------------------------------\n",
      "ğŸ” í…ŒìŠ¤íŠ¸ 5: ê³ ê°ë³„ ì£¼ë¬¸ í†µê³„\n",
      "ğŸ“ ì„¤ëª…: êµ¬ë§¤ ê¸ˆì•¡ ê¸°ì¤€ ìƒìœ„ ê³ ê°\n",
      "âœ… ì‹¤í–‰ ì„±ê³µ (5ê°œ í–‰)\n",
      "ğŸ“Š ê²°ê³¼:\n",
      "   1. product_name: í•´ë¬¼íŒŒì „, total_quantity: 184, total_revenue: 2880000.0\n",
      "   2. product_name: ìˆœë‘ë¶€ì°Œê°œ, total_quantity: 180, total_revenue: 1591200.0\n",
      "   3. product_name: ê¹€ì¹˜, total_quantity: 156, total_revenue: 2264250.0\n",
      "   4. product_name: ì‚¼ê²¹ì‚´, total_quantity: 150, total_revenue: 3202100.0\n",
      "   5. product_name: ë¹„ë¹”ë°¥, total_quantity: 148, total_revenue: 1724400.0\n",
      "--------------------------------------------------\n",
      "ğŸ” í…ŒìŠ¤íŠ¸ 5: ê³ ê°ë³„ ì£¼ë¬¸ í†µê³„\n",
      "ğŸ“ ì„¤ëª…: êµ¬ë§¤ ê¸ˆì•¡ ê¸°ì¤€ ìƒìœ„ ê³ ê°\n",
      "âœ… ì‹¤í–‰ ì„±ê³µ (5ê°œ í–‰)\n",
      "ğŸ“Š ê²°ê³¼:\n",
      "   1. company_name: í•œêµ­ì‹ë‹¹, order_count: 70, total_spent: 5197600.0\n",
      "   2. company_name: ì½”ë¦¬ì•ˆ BBQ, order_count: 60, total_spent: 4863000.0\n",
      "   3. company_name: ì¤‘êµ­ë°˜ì , order_count: 66, total_spent: 4038200.0\n",
      "   4. company_name: í•œì‹ë‹¹, order_count: 51, total_spent: 3824100.0\n",
      "   5. company_name: ì¼ë³¸ë ˆìŠ¤í† ë‘, order_count: 43, total_spent: 2630500.0\n",
      "--------------------------------------------------\n",
      "ğŸ‰ ê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ LangChain Agentìš© ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì¤‘...\n",
      "âœ… ì‹¤í–‰ ì„±ê³µ (5ê°œ í–‰)\n",
      "ğŸ“Š ê²°ê³¼:\n",
      "   1. company_name: í•œêµ­ì‹ë‹¹, order_count: 70, total_spent: 5197600.0\n",
      "   2. company_name: ì½”ë¦¬ì•ˆ BBQ, order_count: 60, total_spent: 4863000.0\n",
      "   3. company_name: ì¤‘êµ­ë°˜ì , order_count: 66, total_spent: 4038200.0\n",
      "   4. company_name: í•œì‹ë‹¹, order_count: 51, total_spent: 3824100.0\n",
      "   5. company_name: ì¼ë³¸ë ˆìŠ¤í† ë‘, order_count: 43, total_spent: 2630500.0\n",
      "--------------------------------------------------\n",
      "ğŸ‰ ê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ LangChain Agentìš© ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì¤‘...\n",
      "âœ… ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì™„ë£Œ: 8ê°œ í…Œì´ë¸”\n",
      "\n",
      "ğŸ“‹ LangChain Agentìš© ìŠ¤í‚¤ë§ˆ ìš”ì•½:\n",
      "   ë°ì´í„°ë² ì´ìŠ¤: northwind\n",
      "   í…Œì´ë¸” ìˆ˜: 8\n",
      "   ê´€ê³„ ìˆ˜: 8\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: LangChain Agent êµ¬í˜„\n",
      "   ë…¸íŠ¸ë¶: 02_langchain_agent_text_to_sql.ipynb\n",
      "\n",
      "âœ… 01ë‹¨ê³„ ì™„ë£Œ: Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° ê²€ì¦\n",
      "ğŸ” Northwind ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ë° í…ŒìŠ¤íŠ¸\n",
      "=============================================\n",
      "\n",
      "ğŸ“‹ ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©:\n",
      "âœ… ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì™„ë£Œ: 8ê°œ í…Œì´ë¸”\n",
      "\n",
      "ğŸ“‹ LangChain Agentìš© ìŠ¤í‚¤ë§ˆ ìš”ì•½:\n",
      "   ë°ì´í„°ë² ì´ìŠ¤: northwind\n",
      "   í…Œì´ë¸” ìˆ˜: 8\n",
      "   ê´€ê³„ ìˆ˜: 8\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: LangChain Agent êµ¬í˜„\n",
      "   ë…¸íŠ¸ë¶: 02_langchain_agent_text_to_sql.ipynb\n",
      "\n",
      "âœ… 01ë‹¨ê³„ ì™„ë£Œ: Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° ê²€ì¦\n",
      "ğŸ” Northwind ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ë° í…ŒìŠ¤íŠ¸\n",
      "=============================================\n",
      "\n",
      "ğŸ“‹ ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©:\n",
      "   ğŸ“‚ ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤: default, information_schema, northwind\n",
      "   âœ… northwind ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ë¨\n",
      "   ğŸ“‚ ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤: default, information_schema, northwind\n",
      "   âœ… northwind ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ë¨\n",
      "   ğŸ“Š northwind í…Œì´ë¸” ìˆ˜: 8ê°œ\n",
      "\n",
      "ğŸ“ˆ í…Œì´ë¸”ë³„ ë°ì´í„° í˜„í™©:\n",
      "   ğŸ“Š northwind í…Œì´ë¸” ìˆ˜: 8ê°œ\n",
      "\n",
      "ğŸ“ˆ í…Œì´ë¸”ë³„ ë°ì´í„° í˜„í™©:\n",
      "   ğŸ“‚ categories: 8ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ categories: 8ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ customers: 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ customers: 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ employees: 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ employees: 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ order_details: 290ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ order_details: 290ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ orders: 100ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ orders: 100ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ products: 10ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ products: 10ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ shippers: 3ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ shippers: 3ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ“‚ suppliers: 5ê°œ ë ˆì½”ë“œ\n",
      "\n",
      "ğŸ“Š ì „ì²´ ë ˆì½”ë“œ ìˆ˜: 426ê°œ\n",
      "\n",
      "ğŸ§ª ê¸°ë³¸ SQL ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸:\n",
      "\n",
      "   ğŸ§ª í…ŒìŠ¤íŠ¸ 1: ê³ ê° ìˆ˜ ì¡°íšŒ\n",
      "      ğŸ“ ì „ì²´ ê³ ê° ìˆ˜ í™•ì¸\n",
      "   ğŸ“‚ suppliers: 5ê°œ ë ˆì½”ë“œ\n",
      "\n",
      "ğŸ“Š ì „ì²´ ë ˆì½”ë“œ ìˆ˜: 426ê°œ\n",
      "\n",
      "ğŸ§ª ê¸°ë³¸ SQL ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸:\n",
      "\n",
      "   ğŸ§ª í…ŒìŠ¤íŠ¸ 1: ê³ ê° ìˆ˜ ì¡°íšŒ\n",
      "      ğŸ“ ì „ì²´ ê³ ê° ìˆ˜ í™•ì¸\n",
      "      âœ… ì„±ê³µ (1ê°œ ê²°ê³¼)\n",
      "      âœ… ì„±ê³µ (1ê°œ ê²°ê³¼)\n",
      "         1. customer_count=5\n",
      "\n",
      "   ğŸ§ª í…ŒìŠ¤íŠ¸ 2: ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜\n",
      "      ğŸ“ ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ìˆ˜ ì§‘ê³„\n",
      "         1. customer_count=5\n",
      "\n",
      "   ğŸ§ª í…ŒìŠ¤íŠ¸ 2: ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜\n",
      "      ğŸ“ ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ìˆ˜ ì§‘ê³„\n",
      "      âŒ ì‹¤íŒ¨: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `c`.`categoryid` cannot be resolved. Did you mean one of the following? [`c`.`category_id`, `p`.`category_id`, `c`.`category_name`, `c`.`description`, `p`.`product_id`]. SQLSTATE: 42703; line 4 pos 42;\n",
      "'Sort ['product_count DESC NULLS LAST], true\n",
      "+- 'Aggregate ['c.categoryname], ['c.categoryname, 'COUNT('p.productid) AS product_count#27342]\n",
      "   +- 'Join LeftOuter, ('c.categoryid = 'p.categoryid)\n",
      "      :- SubqueryAlias c\n",
      "      :  +- SubqueryAlias workspace.northwind.categories\n",
      "      :     +- Relation workspace.northwind.categories[category_id#27354,category_name#27355,description#27356] parquet\n",
      "      +- SubqueryAlias p\n",
      "         +- SubqueryAlias workspace.northwind.products\n",
      "            +- Relation workspace.northwind.products[product_id#27357,product_name#27358,supplier_id#27359,category_id#27360,unit_price#27361,units_in_stock#27362,units_on_order#27363,discontinued#27364] parquet\n",
      "\n",
      "\n",
      "JVM stacktrace:\n",
      "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n",
      "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n",
      "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1130)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1082)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
      "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
      "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
      "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
      "\tat scala.util.Using$.resource(Using.scala:269)\n",
      "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
      "\n",
      "   ğŸ§ª í…ŒìŠ¤íŠ¸ 3: ê°€ê²©ì´ ê°€ì¥ ë¹„ì‹¼ ìƒí’ˆ TOP 3\n",
      "      ğŸ“ ìµœê³ ê°€ ìƒí’ˆ ì¡°íšŒ\n",
      "      âŒ ì‹¤íŒ¨: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `c`.`categoryid` cannot be resolved. Did you mean one of the following? [`c`.`category_id`, `p`.`category_id`, `c`.`category_name`, `c`.`description`, `p`.`product_id`]. SQLSTATE: 42703; line 4 pos 42;\n",
      "'Sort ['product_count DESC NULLS LAST], true\n",
      "+- 'Aggregate ['c.categoryname], ['c.categoryname, 'COUNT('p.productid) AS product_count#27342]\n",
      "   +- 'Join LeftOuter, ('c.categoryid = 'p.categoryid)\n",
      "      :- SubqueryAlias c\n",
      "      :  +- SubqueryAlias workspace.northwind.categories\n",
      "      :     +- Relation workspace.northwind.categories[category_id#27354,category_name#27355,description#27356] parquet\n",
      "      +- SubqueryAlias p\n",
      "         +- SubqueryAlias workspace.northwind.products\n",
      "            +- Relation workspace.northwind.products[product_id#27357,product_name#27358,supplier_id#27359,category_id#27360,unit_price#27361,units_in_stock#27362,units_on_order#27363,discontinued#27364] parquet\n",
      "\n",
      "\n",
      "JVM stacktrace:\n",
      "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n",
      "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n",
      "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1130)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1082)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
      "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
      "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
      "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
      "\tat scala.util.Using$.resource(Using.scala:269)\n",
      "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
      "\n",
      "   ğŸ§ª í…ŒìŠ¤íŠ¸ 3: ê°€ê²©ì´ ê°€ì¥ ë¹„ì‹¼ ìƒí’ˆ TOP 3\n",
      "      ğŸ“ ìµœê³ ê°€ ìƒí’ˆ ì¡°íšŒ\n",
      "      âŒ ì‹¤íŒ¨: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `unitprice` cannot be resolved. Did you mean one of the following? [`unit_price`, `category_id`, `supplier_id`, `units_in_stock`, `discontinued`]. SQLSTATE: 42703; line 4 pos 14;\n",
      "'GlobalLimit 3\n",
      "+- 'LocalLimit 3\n",
      "   +- 'Sort ['unitprice DESC NULLS LAST], true\n",
      "      +- 'Project ['productname, 'unitprice, 'categoryid]\n",
      "         +- 'Filter isnotnull('unitprice)\n",
      "            +- SubqueryAlias workspace.northwind.products\n",
      "               +- Relation workspace.northwind.products[product_id#27376,product_name#27377,supplier_id#27378,category_id#27379,unit_price#27380,units_in_stock#27381,units_on_order#27382,discontinued#27383] parquet\n",
      "\n",
      "\n",
      "JVM stacktrace:\n",
      "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n",
      "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n",
      "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1130)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1082)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
      "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
      "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
      "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
      "\tat scala.util.Using$.resource(Using.scala:269)\n",
      "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
      "\n",
      "   ğŸ§ª í…ŒìŠ¤íŠ¸ 4: êµ­ê°€ë³„ ê³ ê° ë¶„í¬\n",
      "      ğŸ“ ì£¼ìš” êµ­ê°€ë³„ ê³ ê° ìˆ˜\n",
      "      âŒ ì‹¤íŒ¨: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `unitprice` cannot be resolved. Did you mean one of the following? [`unit_price`, `category_id`, `supplier_id`, `units_in_stock`, `discontinued`]. SQLSTATE: 42703; line 4 pos 14;\n",
      "'GlobalLimit 3\n",
      "+- 'LocalLimit 3\n",
      "   +- 'Sort ['unitprice DESC NULLS LAST], true\n",
      "      +- 'Project ['productname, 'unitprice, 'categoryid]\n",
      "         +- 'Filter isnotnull('unitprice)\n",
      "            +- SubqueryAlias workspace.northwind.products\n",
      "               +- Relation workspace.northwind.products[product_id#27376,product_name#27377,supplier_id#27378,category_id#27379,unit_price#27380,units_in_stock#27381,units_on_order#27382,discontinued#27383] parquet\n",
      "\n",
      "\n",
      "JVM stacktrace:\n",
      "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n",
      "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n",
      "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1130)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1082)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
      "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
      "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
      "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
      "\tat scala.util.Using$.resource(Using.scala:269)\n",
      "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
      "\n",
      "   ğŸ§ª í…ŒìŠ¤íŠ¸ 4: êµ­ê°€ë³„ ê³ ê° ë¶„í¬\n",
      "      ğŸ“ ì£¼ìš” êµ­ê°€ë³„ ê³ ê° ìˆ˜\n",
      "      âœ… ì„±ê³µ (5ê°œ ê²°ê³¼)\n",
      "      âœ… ì„±ê³µ (5ê°œ ê²°ê³¼)\n",
      "         1. country=í•œêµ­, customer_count=1\n",
      "         2. country=ìºë‚˜ë‹¤, customer_count=1\n",
      "         3. country=ì¤‘êµ­, customer_count=1\n",
      "         ... ê·¸ ì™¸ 2ê°œ ê²°ê³¼\n",
      "\n",
      "ğŸ“Š SQL í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n",
      "   âœ… ì„±ê³µ: 2ê°œ\n",
      "   âŒ ì‹¤íŒ¨: 2ê°œ\n",
      "\n",
      "âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n",
      "         1. country=í•œêµ­, customer_count=1\n",
      "         2. country=ìºë‚˜ë‹¤, customer_count=1\n",
      "         3. country=ì¤‘êµ­, customer_count=1\n",
      "         ... ê·¸ ì™¸ 2ê°œ ê²°ê³¼\n",
      "\n",
      "ğŸ“Š SQL í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n",
      "   âœ… ì„±ê³µ: 2ê°œ\n",
      "   âŒ ì‹¤íŒ¨: 2ê°œ\n",
      "\n",
      "âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ë³¸ Text-to-SQL í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "basic_sql_prompt_template = \"\"\"\n",
    "ë‹¹ì‹ ì€ SQL ì¿¼ë¦¬ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ì–´ ì§ˆë¬¸ì„ ì •í™•í•œ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.\n",
    "\n",
    "{schema_context}\n",
    "\n",
    "ê·œì¹™:\n",
    "1. ì •í™•í•œ í…Œì´ë¸”ëª…ê³¼ ì»¬ëŸ¼ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "2. ì ì ˆí•œ JOINì„ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "3. WHERE ì ˆì„ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”\n",
    "4. ê²°ê³¼ëŠ” SQL ì¿¼ë¦¬ë§Œ ë°˜í™˜í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´)\n",
    "5. ì¿¼ë¦¬ëŠ” ```sqlê³¼ ``` ì‚¬ì´ì— ì‘ì„±í•˜ì„¸ìš”\n",
    "\n",
    "ì˜ˆì‹œ:\n",
    "ì§ˆë¬¸: \"ëª¨ë“  ê³ ê°ì˜ ì´ë¦„ê³¼ ì´ë©”ì¼ì„ ë³´ì—¬ì£¼ì„¸ìš”\"\n",
    "ë‹µë³€:\n",
    "```sql\n",
    "SELECT name, email FROM customers;\n",
    "```\n",
    "\n",
    "ì§ˆë¬¸: \"2024ë…„ 1ì›”ì— ì£¼ë¬¸í•œ ê³ ê°ë“¤ì˜ ì´ ì£¼ë¬¸ ê¸ˆì•¡ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”\"\n",
    "ë‹µë³€:\n",
    "```sql\n",
    "SELECT c.name, SUM(o.amount) as total_amount\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "WHERE o.order_date >= '2024-01-01' AND o.order_date < '2024-02-01'\n",
    "GROUP BY c.customer_id, c.name;\n",
    "```\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… ê¸°ë³¸ Text-to-SQL í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ì™„ë£Œ\")\n",
    "\n",
    "class SQLExecutor:\n",
    "    \"\"\"ì•ˆì „í•œ SQL ì‹¤í–‰ ë° ê²°ê³¼ ê²€ì¦\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session, max_rows: int = 100):\n",
    "        self.spark = spark_session\n",
    "        self.max_rows = max_rows\n",
    "        self.execution_history = []\n",
    "    \n",
    "    def validate_sql_safety(self, sql_query: str) -> Tuple[bool, str]:\n",
    "        \"\"\"SQL ì¿¼ë¦¬ ì•ˆì „ì„± ê²€ì¦\"\"\"\n",
    "        sql_upper = sql_query.upper().strip()\n",
    "        \n",
    "        # ìœ„í—˜í•œ í‚¤ì›Œë“œ í™•ì¸\n",
    "        dangerous_keywords = [\n",
    "            'DROP', 'DELETE', 'TRUNCATE', 'ALTER', 'CREATE', \n",
    "            'INSERT', 'UPDATE', 'MERGE', 'COPY'\n",
    "        ]\n",
    "        \n",
    "        for keyword in dangerous_keywords:\n",
    "            if keyword in sql_upper:\n",
    "                return False, f\"ìœ„í—˜í•œ í‚¤ì›Œë“œ '{keyword}'ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        # ê¸°ë³¸ì ì¸ êµ¬ë¬¸ ê²€ì¦\n",
    "        if not any(keyword in sql_upper for keyword in ['SELECT', 'WITH', 'SHOW', 'DESCRIBE']):\n",
    "            return False, \"í—ˆìš©ëœ SQL í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        # ê´„í˜¸ ë§¤ì¹­ í™•ì¸\n",
    "        if sql_query.count('(') != sql_query.count(')'):\n",
    "            return False, \"ê´„í˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        return True, \"ì•ˆì „í•œ ì¿¼ë¦¬ì…ë‹ˆë‹¤.\"\n",
    "    \n",
    "    def execute_sql(self, sql_query: str, dry_run: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"SQL ì¿¼ë¦¬ ì‹¤í–‰\"\"\"\n",
    "        execution_start = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # 1. ì•ˆì „ì„± ê²€ì¦\n",
    "            is_safe, safety_message = self.validate_sql_safety(sql_query)\n",
    "            if not is_safe:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": f\"ì•ˆì „ì„± ê²€ì¦ ì‹¤íŒ¨: {safety_message}\",\n",
    "                    \"results\": None,\n",
    "                    \"execution_time\": 0\n",
    "                }\n",
    "            \n",
    "            # 2. Dry run (ì¿¼ë¦¬ íŒŒì‹± ê²€ì¦)\n",
    "            if dry_run:\n",
    "                try:\n",
    "                    self.spark.sql(f\"EXPLAIN {sql_query}\")\n",
    "                    return {\n",
    "                        \"success\": True,\n",
    "                        \"message\": \"ì¿¼ë¦¬ êµ¬ë¬¸ì´ ìœ íš¨í•©ë‹ˆë‹¤.\",\n",
    "                        \"dry_run\": True\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    return {\n",
    "                        \"success\": False,\n",
    "                        \"error\": f\"ì¿¼ë¦¬ êµ¬ë¬¸ ì˜¤ë¥˜: {str(e)}\",\n",
    "                        \"dry_run\": True\n",
    "                    }\n",
    "            \n",
    "            # 3. ì‹¤ì œ ì‹¤í–‰\n",
    "            if not self.spark:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"Spark ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.\",\n",
    "                    \"results\": None\n",
    "                }\n",
    "            \n",
    "            # LIMIT ì¶”ê°€ (ì•ˆì „ì„±ì„ ìœ„í•´)\n",
    "            if \"LIMIT\" not in sql_query.upper():\n",
    "                sql_query += f\" LIMIT {self.max_rows}\"\n",
    "            \n",
    "            # ì¿¼ë¦¬ ì‹¤í–‰\n",
    "            result_df = self.spark.sql(sql_query)\n",
    "            results = result_df.collect()\n",
    "            \n",
    "            # ê²°ê³¼ ì²˜ë¦¬\n",
    "            execution_time = (datetime.now() - execution_start).total_seconds()\n",
    "            \n",
    "            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "            results_data = [row.asDict() for row in results]\n",
    "            \n",
    "            # ì‹¤í–‰ ê¸°ë¡ ì €ì¥\n",
    "            execution_record = {\n",
    "                \"timestamp\": execution_start.isoformat(),\n",
    "                \"sql_query\": sql_query,\n",
    "                \"success\": True,\n",
    "                \"row_count\": len(results_data),\n",
    "                \"execution_time\": execution_time\n",
    "            }\n",
    "            self.execution_history.append(execution_record)\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"results\": results_data,\n",
    "                \"row_count\": len(results_data),\n",
    "                \"columns\": result_df.columns,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"sql_query\": sql_query\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_time = (datetime.now() - execution_start).total_seconds()\n",
    "            \n",
    "            # ì˜¤ë¥˜ ê¸°ë¡ ì €ì¥\n",
    "            error_record = {\n",
    "                \"timestamp\": execution_start.isoformat(),\n",
    "                \"sql_query\": sql_query,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"execution_time\": execution_time\n",
    "            }\n",
    "            self.execution_history.append(error_record)\n",
    "            \n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"SQL ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}\",\n",
    "                \"results\": None,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"sql_query\": sql_query\n",
    "            }\n",
    "    \n",
    "    def format_results(self, execution_result: Dict[str, Any]) -> str:\n",
    "        \"\"\"ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ í¬ë§·íŒ…\"\"\"\n",
    "        if not execution_result[\"success\"]:\n",
    "            return f\"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {execution_result['error']}\"\n",
    "        \n",
    "        results = execution_result[\"results\"]\n",
    "        row_count = execution_result[\"row_count\"]\n",
    "        execution_time = execution_result[\"execution_time\"]\n",
    "        \n",
    "        if not results:\n",
    "            return f\"âœ… ì¿¼ë¦¬ ì‹¤í–‰ ì„±ê³µ (ê²°ê³¼ ì—†ìŒ) - ì‹¤í–‰ì‹œê°„: {execution_time:.3f}ì´ˆ\"\n",
    "        \n",
    "        # í…Œì´ë¸” í˜•íƒœë¡œ í¬ë§·íŒ…\n",
    "        if row_count <= 10:\n",
    "            # ì ì€ ê²°ê³¼ëŠ” ì „ì²´ í‘œì‹œ\n",
    "            df = pd.DataFrame(results)\n",
    "            formatted_table = df.to_string(index=False)\n",
    "        else:\n",
    "            # ë§ì€ ê²°ê³¼ëŠ” ì¼ë¶€ë§Œ í‘œì‹œ\n",
    "            df = pd.DataFrame(results[:10])\n",
    "            formatted_table = df.to_string(index=False)\n",
    "            formatted_table += f\"\\n... ê·¸ ì™¸ {row_count - 10}ê°œ í–‰ (ì´ {row_count}ê°œ)\"\n",
    "        \n",
    "        return f\"\"\"âœ… ì¿¼ë¦¬ ì‹¤í–‰ ì„±ê³µ\n",
    "ğŸ“Š ê²°ê³¼: {row_count}ê°œ í–‰\n",
    "â±ï¸ ì‹¤í–‰ì‹œê°„: {execution_time:.3f}ì´ˆ\n",
    "\n",
    "{formatted_table}\"\"\"\n",
    "\n",
    "    def get_execution_history(self, limit: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"ìµœê·¼ ì‹¤í–‰ ê¸°ë¡ ì¡°íšŒ\"\"\"\n",
    "        return self.execution_history[-limit:]\n",
    "\n",
    "# SQL ì‹¤í–‰ê¸° ì´ˆê¸°í™”\n",
    "if spark:\n",
    "    sql_executor = SQLExecutor(spark, max_rows=100)\n",
    "    print(\"âœ… SQL ì‹¤í–‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "else:\n",
    "    sql_executor = None\n",
    "    print(\"âš ï¸ Spark ì„¸ì…˜ì´ ì—†ì–´ SQL ì‹¤í–‰ê¸°ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# Northwind ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸\n",
    "def test_northwind_queries():\n",
    "    \"\"\"Northwind ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    \n",
    "    if not spark:\n",
    "        print(\"âŒ Spark ì„¸ì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ§ª Northwind ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...\\n\")\n",
    "    \n",
    "    test_queries = [\n",
    "        {\n",
    "            \"name\": \"ì „ì²´ ê³ ê° ìˆ˜\",\n",
    "            \"sql\": \"SELECT COUNT(*) as customer_count FROM northwind.customers\",\n",
    "            \"description\": \"ë“±ë¡ëœ ê³ ê°ì˜ ì´ ê°œìˆ˜\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜\", \n",
    "            \"sql\": \"\"\"\n",
    "                SELECT c.category_name, COUNT(p.product_id) as product_count\n",
    "                FROM northwind.categories c\n",
    "                LEFT JOIN northwind.products p ON c.category_id = p.category_id\n",
    "                GROUP BY c.category_id, c.category_name\n",
    "                ORDER BY product_count DESC\n",
    "            \"\"\",\n",
    "            \"description\": \"ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ê°œìˆ˜\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ì›”ë³„ ì£¼ë¬¸ í˜„í™©\",\n",
    "            \"sql\": \"\"\"\n",
    "                SELECT \n",
    "                    YEAR(order_date) as year,\n",
    "                    MONTH(order_date) as month,\n",
    "                    COUNT(*) as order_count,\n",
    "                    ROUND(AVG(freight), 2) as avg_freight\n",
    "                FROM northwind.orders \n",
    "                WHERE order_date IS NOT NULL\n",
    "                GROUP BY YEAR(order_date), MONTH(order_date)\n",
    "                ORDER BY year DESC, month DESC\n",
    "                LIMIT 10\n",
    "            \"\"\",\n",
    "            \"description\": \"ìµœê·¼ ì›”ë³„ ì£¼ë¬¸ ê±´ìˆ˜ ë° í‰ê·  ë°°ì†¡ë¹„\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìƒí’ˆ TOP 5\",\n",
    "            \"sql\": \"\"\"\n",
    "                SELECT \n",
    "                    p.product_name,\n",
    "                    SUM(od.quantity) as total_quantity,\n",
    "                    ROUND(SUM(od.quantity * od.unit_price * (1 - od.discount)), 2) as total_revenue\n",
    "                FROM northwind.order_details od\n",
    "                JOIN northwind.products p ON od.product_id = p.product_id\n",
    "                GROUP BY p.product_id, p.product_name\n",
    "                ORDER BY total_quantity DESC\n",
    "                LIMIT 5\n",
    "            \"\"\",\n",
    "            \"description\": \"íŒë§¤ëŸ‰ ê¸°ì¤€ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìƒí’ˆ\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ê³ ê°ë³„ ì£¼ë¬¸ í†µê³„\",\n",
    "            \"sql\": \"\"\"\n",
    "                SELECT \n",
    "                    c.company_name,\n",
    "                    COUNT(o.order_id) as order_count,\n",
    "                    ROUND(SUM(od.quantity * od.unit_price * (1 - od.discount)), 2) as total_spent\n",
    "                FROM northwind.customers c\n",
    "                LEFT JOIN northwind.orders o ON c.customer_id = o.customer_id\n",
    "                LEFT JOIN northwind.order_details od ON o.order_id = od.order_id\n",
    "                GROUP BY c.customer_id, c.company_name\n",
    "                HAVING COUNT(o.order_id) > 0\n",
    "                ORDER BY total_spent DESC\n",
    "                LIMIT 5\n",
    "            \"\"\",\n",
    "            \"description\": \"êµ¬ë§¤ ê¸ˆì•¡ ê¸°ì¤€ ìƒìœ„ ê³ ê°\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # ê° ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"ğŸ” í…ŒìŠ¤íŠ¸ {i}: {query['name']}\")\n",
    "        print(f\"ğŸ“ ì„¤ëª…: {query['description']}\")\n",
    "        \n",
    "        try:\n",
    "            # ì¿¼ë¦¬ ì‹¤í–‰\n",
    "            result_df = spark.sql(query['sql'])\n",
    "            result_pandas = result_df.toPandas()\n",
    "            \n",
    "            print(f\"âœ… ì‹¤í–‰ ì„±ê³µ ({len(result_pandas)}ê°œ í–‰)\")\n",
    "            \n",
    "            # ê²°ê³¼ ì¶œë ¥ (ì²˜ìŒ 5ê°œ í–‰ë§Œ)\n",
    "            if len(result_pandas) > 0:\n",
    "                print(\"ğŸ“Š ê²°ê³¼:\")\n",
    "                display_df = result_pandas.head(5)\n",
    "                for idx, row in display_df.iterrows():\n",
    "                    row_str = \", \".join([f\"{col}: {val}\" for col, val in row.items()])\n",
    "                    print(f\"   {idx+1}. {row_str}\")\n",
    "                \n",
    "                if len(result_pandas) > 5:\n",
    "                    print(f\"   ... ê·¸ ì™¸ {len(result_pandas) - 5}ê°œ í–‰\")\n",
    "            else:\n",
    "                print(\"   (ê²°ê³¼ ì—†ìŒ)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"ğŸ‰ ê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "\n",
    "def analyze_schema_for_langchain():\n",
    "    \"\"\"LangChain Agentì—ì„œ ì‚¬ìš©í•  ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "    \n",
    "    if not spark:\n",
    "        print(\"âŒ Spark ì„¸ì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ“‹ LangChain Agentìš© ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    schema_info = {}\n",
    "    \n",
    "    try:\n",
    "        # ê° í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ ìˆ˜ì§‘\n",
    "        tables = spark.sql(\"SHOW TABLES IN northwind\").collect()\n",
    "        \n",
    "        for table_row in tables:\n",
    "            table_name = table_row.tableName\n",
    "            \n",
    "            # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ\n",
    "            describe_result = spark.sql(f\"DESCRIBE TABLE northwind.{table_name}\").collect()\n",
    "            \n",
    "            columns = []\n",
    "            for row in describe_result:\n",
    "                if row.col_name and not row.col_name.startswith('#'):\n",
    "                    columns.append({\n",
    "                        \"name\": row.col_name,\n",
    "                        \"type\": row.data_type,\n",
    "                        \"nullable\": row.comment != \"NOT NULL\" if row.comment else True\n",
    "                    })\n",
    "            \n",
    "            # ìƒ˜í”Œ ë°ì´í„° ëª‡ ê°œ ê°€ì ¸ì˜¤ê¸°\n",
    "            sample_df = spark.table(f\"northwind.{table_name}\").limit(3).toPandas()\n",
    "            sample_data = sample_df.to_dict('records') if len(sample_df) > 0 else []\n",
    "            \n",
    "            # í…Œì´ë¸” í†µê³„\n",
    "            row_count = spark.table(f\"northwind.{table_name}\").count()\n",
    "            \n",
    "            schema_info[table_name] = {\n",
    "                \"table_name\": table_name,\n",
    "                \"full_name\": f\"northwind.{table_name}\",\n",
    "                \"columns\": columns,\n",
    "                \"sample_data\": sample_data,\n",
    "                \"row_count\": row_count,\n",
    "                \"description\": get_table_description(table_name)\n",
    "            }\n",
    "        \n",
    "        print(f\"âœ… ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì™„ë£Œ: {len(schema_info)}ê°œ í…Œì´ë¸”\")\n",
    "        \n",
    "        # LangChain Agentìš© ìš”ì•½ ì •ë³´ ìƒì„±\n",
    "        schema_summary = {\n",
    "            \"database_name\": \"northwind\",\n",
    "            \"tables\": list(schema_info.keys()),\n",
    "            \"total_tables\": len(schema_info),\n",
    "            \"relationships\": get_table_relationships(),\n",
    "            \"schema_details\": schema_info\n",
    "        }\n",
    "        \n",
    "        return schema_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_table_description(table_name):\n",
    "    \"\"\"í…Œì´ë¸”ë³„ ì„¤ëª… ë°˜í™˜\"\"\"\n",
    "    descriptions = {\n",
    "        \"categories\": \"ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì •ë³´ (ìŒë£Œ, ì¡°ë¯¸ë£Œ, ê³¼ìë¥˜ ë“±)\",\n",
    "        \"suppliers\": \"ê³µê¸‰ì—…ì²´ ì •ë³´ ë° ì—°ë½ì²˜\",\n",
    "        \"products\": \"ìƒí’ˆ ì •ë³´ (ì´ë¦„, ê°€ê²©, ì¬ê³ , ì¹´í…Œê³ ë¦¬, ê³µê¸‰ì—…ì²´)\",\n",
    "        \"customers\": \"ê³ ê° ì •ë³´ ë° ì—°ë½ì²˜\",\n",
    "        \"employees\": \"ì§ì› ì •ë³´ ë° ì¡°ì§ êµ¬ì¡°\",\n",
    "        \"shippers\": \"ë°°ì†¡ì—…ì²´ ì •ë³´\",\n",
    "        \"orders\": \"ì£¼ë¬¸ ì •ë³´ (ê³ ê°, ì§ì›, ë‚ ì§œ, ë°°ì†¡)\",\n",
    "        \"order_details\": \"ì£¼ë¬¸ ìƒì„¸ ì •ë³´ (ìƒí’ˆ, ìˆ˜ëŸ‰, ê°€ê²©, í• ì¸)\"\n",
    "    }\n",
    "    return descriptions.get(table_name, \"\")\n",
    "\n",
    "def get_table_relationships():\n",
    "    \"\"\"í…Œì´ë¸” ê°„ ê´€ê³„ ì •ë³´ ë°˜í™˜\"\"\"\n",
    "    return {\n",
    "        \"orders_customers\": \"orders.customer_id -> customers.customer_id\",\n",
    "        \"orders_employees\": \"orders.employee_id -> employees.employee_id\", \n",
    "        \"orders_shippers\": \"orders.ship_via -> shippers.shipper_id\",\n",
    "        \"order_details_orders\": \"order_details.order_id -> orders.order_id\",\n",
    "        \"order_details_products\": \"order_details.product_id -> products.product_id\",\n",
    "        \"products_categories\": \"products.category_id -> categories.category_id\",\n",
    "        \"products_suppliers\": \"products.supplier_id -> suppliers.supplier_id\",\n",
    "        \"employees_manager\": \"employees.reports_to -> employees.employee_id\"\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "try:\n",
    "    test_northwind_queries()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    schema_info = analyze_schema_for_langchain()\n",
    "    \n",
    "    if schema_info:\n",
    "        print(f\"\\nğŸ“‹ LangChain Agentìš© ìŠ¤í‚¤ë§ˆ ìš”ì•½:\")\n",
    "        print(f\"   ë°ì´í„°ë² ì´ìŠ¤: {schema_info['database_name']}\")\n",
    "        print(f\"   í…Œì´ë¸” ìˆ˜: {schema_info['total_tables']}\")\n",
    "        print(f\"   ê´€ê³„ ìˆ˜: {len(schema_info['relationships'])}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„: LangChain Agent êµ¬í˜„\")\n",
    "        print(f\"   ë…¸íŠ¸ë¶: 02_langchain_agent_text_to_sql.ipynb\")\n",
    "        \n",
    "        # ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥\n",
    "        globals()['northwind_schema'] = schema_info\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "print(f\"\\nâœ… 01ë‹¨ê³„ ì™„ë£Œ: Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° ê²€ì¦\")\n",
    "\n",
    "# ë°ì´í„° ê²€ì¦ ë° ê¸°ë³¸ SQL í…ŒìŠ¤íŠ¸\n",
    "\n",
    "print(\"ğŸ” Northwind ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ë° í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” í™•ì¸\n",
    "print(\"\\nğŸ“‹ ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©:\")\n",
    "databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "db_names = [row.databaseName for row in databases]\n",
    "print(f\"   ğŸ“‚ ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤: {', '.join(db_names)}\")\n",
    "\n",
    "if \"northwind\" in db_names:\n",
    "    print(\"   âœ… northwind ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ë¨\")\n",
    "    \n",
    "    # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ\n",
    "    tables = spark.sql(\"SHOW TABLES IN northwind\").collect()\n",
    "    table_names = [row.tableName for row in tables]\n",
    "    print(f\"   ğŸ“Š northwind í…Œì´ë¸” ìˆ˜: {len(table_names)}ê°œ\")\n",
    "    \n",
    "    # ê° í…Œì´ë¸”ì˜ ë ˆì½”ë“œ ìˆ˜ í™•ì¸\n",
    "    print(f\"\\nğŸ“ˆ í…Œì´ë¸”ë³„ ë°ì´í„° í˜„í™©:\")\n",
    "    total_records = 0\n",
    "    \n",
    "    for table_name in sorted(table_names):\n",
    "        try:\n",
    "            count = spark.table(f\"northwind.{table_name}\").count()\n",
    "            total_records += count\n",
    "            print(f\"   ğŸ“‚ {table_name}: {count:,}ê°œ ë ˆì½”ë“œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ {table_name}: ì¡°íšŒ ì‹¤íŒ¨ ({e})\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê°œ\")\n",
    "    \n",
    "else:\n",
    "    print(\"   âŒ northwind ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# 2. ê¸°ë³¸ SQL ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\nğŸ§ª ê¸°ë³¸ SQL ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸:\")\n",
    "\n",
    "test_queries = [\n",
    "    {\n",
    "        \"name\": \"ê³ ê° ìˆ˜ ì¡°íšŒ\",\n",
    "        \"sql\": \"SELECT COUNT(*) as customer_count FROM northwind.customers\",\n",
    "        \"description\": \"ì „ì²´ ê³ ê° ìˆ˜ í™•ì¸\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜\",\n",
    "        \"sql\": \"\"\"\n",
    "        SELECT c.categoryname, COUNT(p.productid) as product_count\n",
    "        FROM northwind.categories c\n",
    "        LEFT JOIN northwind.products p ON c.categoryid = p.categoryid\n",
    "        GROUP BY c.categoryname\n",
    "        ORDER BY product_count DESC\n",
    "        \"\"\",\n",
    "        \"description\": \"ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ìˆ˜ ì§‘ê³„\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ê°€ê²©ì´ ê°€ì¥ ë¹„ì‹¼ ìƒí’ˆ TOP 3\",\n",
    "        \"sql\": \"\"\"\n",
    "        SELECT productname, unitprice, categoryid\n",
    "        FROM northwind.products\n",
    "        WHERE unitprice IS NOT NULL\n",
    "        ORDER BY unitprice DESC\n",
    "        LIMIT 3\n",
    "        \"\"\",\n",
    "        \"description\": \"ìµœê³ ê°€ ìƒí’ˆ ì¡°íšŒ\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"êµ­ê°€ë³„ ê³ ê° ë¶„í¬\",\n",
    "        \"sql\": \"\"\"\n",
    "        SELECT country, COUNT(*) as customer_count\n",
    "        FROM northwind.customers\n",
    "        GROUP BY country\n",
    "        ORDER BY customer_count DESC\n",
    "        LIMIT 5\n",
    "        \"\"\",\n",
    "        \"description\": \"ì£¼ìš” êµ­ê°€ë³„ ê³ ê° ìˆ˜\"\n",
    "    }\n",
    "]\n",
    "\n",
    "successful_tests = 0\n",
    "failed_tests = 0\n",
    "\n",
    "for i, test in enumerate(test_queries, 1):\n",
    "    try:\n",
    "        print(f\"\\n   ğŸ§ª í…ŒìŠ¤íŠ¸ {i}: {test['name']}\")\n",
    "        print(f\"      ğŸ“ {test['description']}\")\n",
    "        \n",
    "        result_df = spark.sql(test['sql'])\n",
    "        results = result_df.collect()\n",
    "        \n",
    "        if results:\n",
    "            print(f\"      âœ… ì„±ê³µ ({len(results)}ê°œ ê²°ê³¼)\")\n",
    "            \n",
    "            # ê²°ê³¼ ìƒ˜í”Œ í‘œì‹œ (ì²˜ìŒ 3ê°œ)\n",
    "            for j, row in enumerate(results[:3]):\n",
    "                row_data = [f\"{col}={row[col]}\" for col in result_df.columns]\n",
    "                print(f\"         {j+1}. {', '.join(row_data)}\")\n",
    "            \n",
    "            if len(results) > 3:\n",
    "                print(f\"         ... ê·¸ ì™¸ {len(results)-3}ê°œ ê²°ê³¼\")\n",
    "        else:\n",
    "            print(f\"      âš ï¸ ê²°ê³¼ ì—†ìŒ\")\n",
    "            \n",
    "        successful_tests += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ ì‹¤íŒ¨: {str(e)}\")\n",
    "        failed_tests += 1\n",
    "\n",
    "print(f\"\\nğŸ“Š SQL í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
    "print(f\"   âœ… ì„±ê³µ: {successful_tests}ê°œ\")\n",
    "print(f\"   âŒ ì‹¤íŒ¨: {failed_tests}ê°œ\")\n",
    "\n",
    "if successful_tests == len(test_queries):\n",
    "    print(f\"\\nğŸ‰ ëª¨ë“  SQL í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"âœ… Northwind ë°ì´í„°ë² ì´ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecff4cc",
   "metadata": {},
   "source": [
    "## ğŸ”— ë‹¤ìŒ ë‹¨ê³„: LangChain Agent êµ¬í˜„\n",
    "\n",
    "ì´ì œ Northwind ë°ì´í„°ë² ì´ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤! \n",
    "\n",
    "### ğŸ“ ì™„ë£Œëœ ì‘ì—…\n",
    "âœ… **Databricks í™˜ê²½ ì„¤ì •** - Spark ì„¸ì…˜ ë° northwind ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±  \n",
    "âœ… **Northwind ìƒ˜í”Œ ë°ì´í„°** - 8ê°œ í…Œì´ë¸”, ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ë°˜ì˜  \n",
    "âœ… **Delta Lake í…Œì´ë¸”** - ê³ ì„±ëŠ¥ ë¶„ì„ì„ ìœ„í•œ ìµœì í™”ëœ ì €ì¥ì†Œ  \n",
    "âœ… **ìŠ¤í‚¤ë§ˆ ë¶„ì„** - LangChain Agentì—ì„œ í™œìš©í•  ë©”íƒ€ë°ì´í„° ì¶”ì¶œ  \n",
    "âœ… **ê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸** - ë°ì´í„° ë¬´ê²°ì„± ë° ê´€ê³„ ê²€ì¦ ì™„ë£Œ  \n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ êµ¬í˜„í•  ë‚´ìš©\n",
    "\n",
    "**`02_langchain_agent_text_to_sql.ipynb`**\n",
    "- ğŸ¤– **LangChain Agent ì•„í‚¤í…ì²˜** ì„¤ê³„\n",
    "- ğŸ”§ **Function Tools** êµ¬í˜„ (ìŠ¤í‚¤ë§ˆ ì¡°íšŒ, SQL ì‹¤í–‰, ê²°ê³¼ ê²€ì¦)\n",
    "- ğŸ§  **ì¶”ë¡  ëª¨ë¸ ì—°ë™** (Databricks Foundation Models)\n",
    "- ğŸ’¬ **ìì—°ì–´ â†’ SQL ë³€í™˜** íŒŒì´í”„ë¼ì¸\n",
    "- ğŸ” **ë™ì  ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰** ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "- ğŸ›¡ï¸ **ì•ˆì „ì„± ê²€ì¦** ë° ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "\n",
    "## 6. ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ë° LangChain Agent ì—°ë™ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1238b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•œêµ­ì–´ íŠ¹í™” ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ì™„ë£Œ\n",
      "âœ… Text-to-SQL RAG ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n",
      "ğŸ‰ Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!\n",
      "\n",
      "============================================================\n",
      "âœ… êµ¬ì¶• ì™„ë£Œ ìƒíƒœ:\n",
      "   ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤: northwind\n",
      "   ğŸ“‹ í…Œì´ë¸” ìˆ˜: 8ê°œ\n",
      "\n",
      "ğŸ“‹ í…Œì´ë¸”ë³„ ë°ì´í„° í˜„í™©:\n",
      "   ğŸ“„ categories: 8ê°œ ë ˆì½”ë“œ, 3ê°œ ì»¬ëŸ¼\n",
      "   ğŸ“„ customers: 5ê°œ ë ˆì½”ë“œ, 8ê°œ ì»¬ëŸ¼\n",
      "   ğŸ“„ employees: 5ê°œ ë ˆì½”ë“œ, 9ê°œ ì»¬ëŸ¼\n",
      "   ğŸ“„ order_details: 290ê°œ ë ˆì½”ë“œ, 5ê°œ ì»¬ëŸ¼\n",
      "   ğŸ“„ orders: 100ê°œ ë ˆì½”ë“œ, 12ê°œ ì»¬ëŸ¼\n",
      "   ğŸ“„ products: 10ê°œ ë ˆì½”ë“œ, 8ê°œ ì»¬ëŸ¼\n",
      "   ğŸ“„ shippers: 3ê°œ ë ˆì½”ë“œ, 3ê°œ ì»¬ëŸ¼\n",
      "   ğŸ“„ suppliers: 5ê°œ ë ˆì½”ë“œ, 8ê°œ ì»¬ëŸ¼\n",
      "\n",
      "ğŸ“ˆ ì´ ë°ì´í„°: 426ê°œ ë ˆì½”ë“œ\n",
      "ğŸ”— í…Œì´ë¸” ê´€ê³„: 8ê°œ\n",
      "   â€¢ orders.customer_id -> customers.customer_id\n",
      "   â€¢ orders.employee_id -> employees.employee_id\n",
      "   â€¢ orders.ship_via -> shippers.shipper_id\n",
      "   ... ê·¸ ì™¸ 5ê°œ\n",
      "\n",
      "ğŸ¯ Text-to-SQL í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ!\n",
      "\n",
      "ğŸ’¬ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” ìì—°ì–´ ì§ˆë¬¸ ì˜ˆì‹œ:\n",
      "   1. ì „ì²´ ê³ ê° ìˆ˜ëŠ” ëª‡ ëª…ì¸ê°€ìš”?\n",
      "   2. ê°€ì¥ ë§ì´ íŒ”ë¦° ìƒí’ˆ 5ê°œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”\n",
      "   3. ì›”ë³„ ì£¼ë¬¸ í˜„í™©ì„ ì•Œë ¤ì£¼ì„¸ìš”\n",
      "   4. ê³ ê°ë³„ ì´ êµ¬ë§¤ ê¸ˆì•¡ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”\n",
      "   5. ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ê°œìˆ˜ëŠ”?\n",
      "   6. ì§ì›ë³„ ë‹´ë‹¹ ì£¼ë¬¸ ê±´ìˆ˜ëŠ”?\n",
      "   7. ë°°ì†¡ë¹„ê°€ ê°€ì¥ ë¹„ì‹¼ ì£¼ë¬¸ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”\n",
      "   8. í• ì¸ì´ ì ìš©ëœ ì£¼ë¬¸ ìƒì„¸ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”\n",
      "\n",
      "ğŸ”§ ë‹¤ìŒ ë…¸íŠ¸ë¶ ì‹¤í–‰ ë°©ë²•:\n",
      "   1. ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±: 02_langchain_agent_text_to_sql.ipynb\n",
      "   2. ì´ ë…¸íŠ¸ë¶ì˜ ë³€ìˆ˜ë“¤ í™œìš©:\n",
      "      - spark: Spark ì„¸ì…˜\n",
      "      - northwind_schema: ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°\n",
      "   3. LangChain Agent êµ¬í˜„ ì‹œì‘\n",
      "\n",
      "ğŸ› ï¸ í˜„ì¬ ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤:\n",
      "   â€¢ test_northwind_queries() - ê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰\n",
      "   â€¢ verify_northwind_database() - ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì¬í™•ì¸\n",
      "   â€¢ analyze_schema_for_langchain() - ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¬ë¶„ì„\n",
      "   â€¢ spark.sql('YOUR_SQL_QUERY') - ì§ì ‘ SQL ì‹¤í–‰\n",
      "\n",
      "ğŸ¯ ëª©í‘œ ë‹¬ì„±!\n",
      "   âœ… Databricksì— ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•\n",
      "   âœ… LangChain Agent êµ¬í˜„ì„ ìœ„í•œ ê¸°ë°˜ í™˜ê²½ ì™„ë£Œ\n",
      "   âœ… Text-to-SQL ì‹œìŠ¤í…œì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ ì„±ê³µ\n",
      "\n",
      "ğŸš€ ë‹¤ìŒ: LangChain Agent ê¸°ë°˜ Text-to-SQL êµ¬í˜„!\n",
      "   ë…¸íŠ¸ë¶: 02_langchain_agent_text_to_sql.ipynb\n",
      "ğŸ”— LangChain Agent ì—°ë™ì„ ìœ„í•œ ìŠ¤í‚¤ë§ˆ ë¶„ì„\n",
      "=============================================\n",
      "\n",
      "ğŸ“Š í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ì¤‘...\n",
      "   ğŸ” categories ë¶„ì„ ì¤‘...\n",
      "   ğŸ” categories ë¶„ì„ ì¤‘...\n",
      "      âœ… 3ê°œ ì»¬ëŸ¼, 8ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” customers ë¶„ì„ ì¤‘...\n",
      "      âœ… 3ê°œ ì»¬ëŸ¼, 8ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” customers ë¶„ì„ ì¤‘...\n",
      "      âœ… 8ê°œ ì»¬ëŸ¼, 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” employees ë¶„ì„ ì¤‘...\n",
      "      âœ… 8ê°œ ì»¬ëŸ¼, 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” employees ë¶„ì„ ì¤‘...\n",
      "      âœ… 9ê°œ ì»¬ëŸ¼, 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” order_details ë¶„ì„ ì¤‘...\n",
      "      âœ… 9ê°œ ì»¬ëŸ¼, 5ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” order_details ë¶„ì„ ì¤‘...\n",
      "      âœ… 5ê°œ ì»¬ëŸ¼, 290ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” orders ë¶„ì„ ì¤‘...\n",
      "      âœ… 5ê°œ ì»¬ëŸ¼, 290ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” orders ë¶„ì„ ì¤‘...\n",
      "      âœ… 12ê°œ ì»¬ëŸ¼, 100ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” products ë¶„ì„ ì¤‘...\n",
      "      âœ… 12ê°œ ì»¬ëŸ¼, 100ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” products ë¶„ì„ ì¤‘...\n",
      "      âœ… 8ê°œ ì»¬ëŸ¼, 10ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” shippers ë¶„ì„ ì¤‘...\n",
      "      âœ… 8ê°œ ì»¬ëŸ¼, 10ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” shippers ë¶„ì„ ì¤‘...\n",
      "      âœ… 3ê°œ ì»¬ëŸ¼, 3ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” suppliers ë¶„ì„ ì¤‘...\n",
      "      âœ… 3ê°œ ì»¬ëŸ¼, 3ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ” suppliers ë¶„ì„ ì¤‘...\n",
      "      âœ… 8ê°œ ì»¬ëŸ¼, 5ê°œ ë ˆì½”ë“œ\n",
      "\n",
      "ğŸ“‹ LangChain Agentìš© ìŠ¤í‚¤ë§ˆ ìš”ì•½:\n",
      "   ğŸ“‚ ë°ì´í„°ë² ì´ìŠ¤: northwind\n",
      "   ğŸ“Š í…Œì´ë¸” ìˆ˜: 8ê°œ\n",
      "   ğŸ”— ê´€ê³„ ì •ì˜: 8ê°œ í…Œì´ë¸”\n",
      "   ğŸ“ˆ ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 56ê°œ\n",
      "   ğŸ“Š ì „ì²´ ë ˆì½”ë“œ ìˆ˜: 426ê°œ\n",
      "\n",
      "âœ… LangChain Agent ì—°ë™ ì¤€ë¹„ ì™„ë£Œ!\n",
      "ğŸ“‹ ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°:\n",
      "   ğŸ—„ï¸ northwind_schema_info: ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´\n",
      "   ğŸ“Š agent_schema_summary: Agentìš© ìš”ì•½ ì •ë³´\n",
      "   ğŸ”— table_relationships: í…Œì´ë¸” ê´€ê³„ ì •ë³´\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\n",
      "   1. 02_langchain_agent_text_to_sql.ipynb ë…¸íŠ¸ë¶ ì—´ê¸°\n",
      "   2. ìœ„ ë³€ìˆ˜ë“¤ì„ í™œìš©í•˜ì—¬ LangChain Agent êµ¬í˜„\n",
      "   3. Text-to-SQL ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë°ëª¨\n",
      "\n",
      "ğŸ‰ Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° Agent ì—°ë™ ì¤€ë¹„ ì™„ë£Œ!\n",
      "      âœ… 8ê°œ ì»¬ëŸ¼, 5ê°œ ë ˆì½”ë“œ\n",
      "\n",
      "ğŸ“‹ LangChain Agentìš© ìŠ¤í‚¤ë§ˆ ìš”ì•½:\n",
      "   ğŸ“‚ ë°ì´í„°ë² ì´ìŠ¤: northwind\n",
      "   ğŸ“Š í…Œì´ë¸” ìˆ˜: 8ê°œ\n",
      "   ğŸ”— ê´€ê³„ ì •ì˜: 8ê°œ í…Œì´ë¸”\n",
      "   ğŸ“ˆ ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 56ê°œ\n",
      "   ğŸ“Š ì „ì²´ ë ˆì½”ë“œ ìˆ˜: 426ê°œ\n",
      "\n",
      "âœ… LangChain Agent ì—°ë™ ì¤€ë¹„ ì™„ë£Œ!\n",
      "ğŸ“‹ ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°:\n",
      "   ğŸ—„ï¸ northwind_schema_info: ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´\n",
      "   ğŸ“Š agent_schema_summary: Agentìš© ìš”ì•½ ì •ë³´\n",
      "   ğŸ”— table_relationships: í…Œì´ë¸” ê´€ê³„ ì •ë³´\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\n",
      "   1. 02_langchain_agent_text_to_sql.ipynb ë…¸íŠ¸ë¶ ì—´ê¸°\n",
      "   2. ìœ„ ë³€ìˆ˜ë“¤ì„ í™œìš©í•˜ì—¬ LangChain Agent êµ¬í˜„\n",
      "   3. Text-to-SQL ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë°ëª¨\n",
      "\n",
      "ğŸ‰ Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° Agent ì—°ë™ ì¤€ë¹„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í•œêµ­ì–´ íŠ¹í™” ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "advanced_korean_sql_prompt_template = \"\"\"\n",
    "ë‹¹ì‹ ì€ í•œêµ­ì–´ë¥¼ ì´í•´í•˜ëŠ” SQL ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ ì§ˆë¬¸ì„ ì •í™•í•œ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.\n",
    "\n",
    "{schema_context}\n",
    "\n",
    "ë³€í™˜ ê·œì¹™:\n",
    "1. í…Œì´ë¸”ëª…ê³¼ ì»¬ëŸ¼ëª…ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "2. í•œêµ­ì–´ ìˆ«ì í‘œí˜„ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ì„¸ìš” (ì˜ˆ: \"ì—´ ê°œ\" â†’ 10)\n",
    "3. ë‚ ì§œ í‘œí˜„ì„ SQL í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš” (ì˜ˆ: \"ì‘ë…„\" â†’ í•´ë‹¹ ì—°ë„)\n",
    "4. ì§‘ê³„ í•¨ìˆ˜ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "5. ê²°ê³¼ëŠ” SQL ì¿¼ë¦¬ë§Œ ë°˜í™˜í•˜ê³  ```sql ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ì„¸ìš”\n",
    "\n",
    "í•œêµ­ì–´ ì§ˆë¬¸ ì˜ˆì‹œ:\n",
    "\n",
    "ì§ˆë¬¸: \"ê³ ê° ìˆ˜ë¥¼ ì„¸ì–´ì£¼ì„¸ìš”\"\n",
    "```sql\n",
    "SELECT COUNT(*) as customer_count FROM customers;\n",
    "```\n",
    "\n",
    "ì§ˆë¬¸: \"ê°€ì¥ ë§ì´ ì£¼ë¬¸í•œ ê³ ê° ë‹¤ì„¯ ëª…ì„ ì°¾ì•„ì£¼ì„¸ìš”\"\n",
    "```sql\n",
    "SELECT c.name, COUNT(o.order_id) as order_count\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.name\n",
    "ORDER BY order_count DESC\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "ì§ˆë¬¸: \"ì›”ë³„ ë§¤ì¶œì„ ë³´ì—¬ì£¼ì„¸ìš”\"\n",
    "```sql\n",
    "SELECT \n",
    "    DATE_FORMAT(order_date, '%Y-%m') as month,\n",
    "    SUM(amount) as monthly_revenue\n",
    "FROM orders\n",
    "GROUP BY DATE_FORMAT(order_date, '%Y-%m')\n",
    "ORDER BY month;\n",
    "```\n",
    "\n",
    "ì§ˆë¬¸: \"{question}\"\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… í•œêµ­ì–´ íŠ¹í™” ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ì™„ë£Œ\")\n",
    "\n",
    "# ê°„ë‹¨í•œ í™˜ê²½ í´ë˜ìŠ¤ ì •ì˜ (LangChain Agent ë…¸íŠ¸ë¶ì—ì„œ ë” ìì„¸íˆ êµ¬í˜„ë¨)\n",
    "class DatabricksEnvironment:\n",
    "    \"\"\"ê¸°ë³¸ Databricks í™˜ê²½ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        self.database_name = \"northwind\"\n",
    "\n",
    "class TextToSQLRAGApp:\n",
    "    \"\"\"í†µí•© Text-to-SQL RAG ì• í”Œë¦¬ì¼€ì´ì…˜\"\"\"\n",
    "    \n",
    "    def __init__(self, databricks_env: DatabricksEnvironment):\n",
    "        self.env = databricks_env\n",
    "        self.schema_analyzer = None\n",
    "        self.schema_retriever = None\n",
    "        self.sql_generator = None\n",
    "        self.sql_executor = None\n",
    "        self.vectorized_schemas = []\n",
    "        \n",
    "        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”\n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”\"\"\"\n",
    "        try:\n",
    "            # ìŠ¤í‚¤ë§ˆ ë¶„ì„ê¸°\n",
    "            if self.env.spark:\n",
    "                self.schema_analyzer = DatabaseSchemaAnalyzer(\n",
    "                    self.env.spark, \n",
    "                    self.env.embedding_model\n",
    "                )\n",
    "                print(\"âœ… ìŠ¤í‚¤ë§ˆ ë¶„ì„ê¸° ì´ˆê¸°í™”\")\n",
    "            \n",
    "            # SQL ì‹¤í–‰ê¸°\n",
    "            if self.env.spark:\n",
    "                self.sql_executor = SQLExecutor(self.env.spark)\n",
    "                print(\"âœ… SQL ì‹¤í–‰ê¸° ì´ˆê¸°í™”\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}\")\n",
    "    \n",
    "    def setup_database(self, database_name: str = \"default\", limit_tables: int = 10):\n",
    "        \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë° ìŠ¤í‚¤ë§ˆ ë¶„ì„\"\"\"\n",
    "        if not self.schema_analyzer:\n",
    "            return False, \"ìŠ¤í‚¤ë§ˆ ë¶„ì„ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ '{database_name}' ì„¤ì • ì¤‘...\")\n",
    "            \n",
    "            # 1. ìŠ¤í‚¤ë§ˆ ë¶„ì„\n",
    "            schemas = self.schema_analyzer.analyze_database_schema(database_name, limit_tables)\n",
    "            \n",
    "            if not schemas:\n",
    "                return False, f\"ë°ì´í„°ë² ì´ìŠ¤ '{database_name}'ì—ì„œ ìŠ¤í‚¤ë§ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "            \n",
    "            # 2. ìŠ¤í‚¤ë§ˆ ë²¡í„°í™”\n",
    "            self.vectorized_schemas = self.schema_analyzer.vectorize_schemas(schemas)\n",
    "            \n",
    "            # 3. RAG ê²€ìƒ‰ê¸° ì´ˆê¸°í™”\n",
    "            self.schema_retriever = RAGSchemaRetriever(\n",
    "                self.vectorized_schemas, \n",
    "                self.env.embedding_model\n",
    "            )\n",
    "            \n",
    "            # 4. SQL ìƒì„±ê¸° ì´ˆê¸°í™”\n",
    "            if self.env.foundation_model:\n",
    "                self.sql_generator = TextToSQLGenerator(\n",
    "                    self.env.foundation_model,\n",
    "                    self.schema_retriever\n",
    "                )\n",
    "            \n",
    "            print(f\"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì™„ë£Œ: {len(schemas)}ê°œ í…Œì´ë¸”\")\n",
    "            return True, f\"{len(schemas)}ê°œ í…Œì´ë¸” ì„¤ì • ì™„ë£Œ\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False, f\"ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {str(e)}\"\n",
    "    \n",
    "    def ask_question(self, question: str, execute_sql: bool = True, dry_run: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"ìì—°ì–´ ì§ˆë¬¸ì„ SQLë¡œ ë³€í™˜í•˜ê³  ì‹¤í–‰\"\"\"\n",
    "        if not self.sql_generator:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"SQL ìƒì„±ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. setup_database()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\",\n",
    "                \"question\": question\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ¤” ì§ˆë¬¸ ë¶„ì„ ì¤‘: '{question}'\")\n",
    "            \n",
    "            # 1. SQL ìƒì„±\n",
    "            sql_result = self.sql_generator.generate_sql(question)\n",
    "            \n",
    "            if not sql_result[\"success\"]:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": sql_result[\"error\"],\n",
    "                    \"question\": question\n",
    "                }\n",
    "            \n",
    "            sql_query = sql_result[\"sql_query\"]\n",
    "            relevant_schemas = sql_result[\"relevant_schemas\"]\n",
    "            \n",
    "            print(f\"ğŸ” ê´€ë ¨ í…Œì´ë¸” {len(relevant_schemas)}ê°œ ë°œê²¬\")\n",
    "            print(f\"âš¡ SQL ìƒì„± ì™„ë£Œ\")\n",
    "            \n",
    "            result = {\n",
    "                \"success\": True,\n",
    "                \"question\": question,\n",
    "                \"sql_query\": sql_query,\n",
    "                \"relevant_schemas\": [\n",
    "                    {\n",
    "                        \"table\": schema[\"full_name\"],\n",
    "                        \"score\": schema.get(\"similarity_score\", 0)\n",
    "                    }\n",
    "                    for schema in relevant_schemas\n",
    "                ],\n",
    "                \"execution_result\": None\n",
    "            }\n",
    "            \n",
    "            # 2. SQL ì‹¤í–‰ (ì˜µì…˜)\n",
    "            if execute_sql and self.sql_executor:\n",
    "                print(\"ğŸš€ SQL ì‹¤í–‰ ì¤‘...\")\n",
    "                execution_result = self.sql_executor.execute_sql(sql_query, dry_run=dry_run)\n",
    "                result[\"execution_result\"] = execution_result\n",
    "                \n",
    "                if execution_result[\"success\"]:\n",
    "                    print(\"âœ… SQL ì‹¤í–‰ ì„±ê³µ\")\n",
    "                else:\n",
    "                    print(f\"âŒ SQL ì‹¤í–‰ ì‹¤íŒ¨: {execution_result['error']}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\",\n",
    "                \"question\": question\n",
    "            }\n",
    "    \n",
    "    def get_database_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"í˜„ì¬ ì„¤ì •ëœ ë°ì´í„°ë² ì´ìŠ¤ ìš”ì•½ ì •ë³´\"\"\"\n",
    "        if not self.vectorized_schemas:\n",
    "            return {\"message\": \"ì„¤ì •ëœ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\"}\n",
    "        \n",
    "        summary = {\n",
    "            \"total_tables\": len(self.vectorized_schemas),\n",
    "            \"tables\": []\n",
    "        }\n",
    "        \n",
    "        for schema in self.vectorized_schemas:\n",
    "            table_info = {\n",
    "                \"name\": schema[\"full_name\"],\n",
    "                \"row_count\": schema.get(\"row_count\", 0),\n",
    "                \"column_count\": len(schema[\"columns\"]),\n",
    "                \"has_embedding\": \"embedding\" in schema\n",
    "            }\n",
    "            summary[\"tables\"].append(table_info)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def interactive_session(self):\n",
    "        \"\"\"ëŒ€í™”í˜• ì„¸ì…˜ ì‹œì‘\"\"\"\n",
    "        print(\"ğŸ¯ Text-to-SQL RAG ëŒ€í™”í˜• ì„¸ì…˜ ì‹œì‘\")\n",
    "        print(\"   ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "        print(\"   ë„ì›€ë§ì€ 'help'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                question = input(\"\\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "                \n",
    "                if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:\n",
    "                    print(\"ğŸ‘‹ ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "                    break\n",
    "                \n",
    "                if question.lower() in ['help', 'ë„ì›€ë§']:\n",
    "                    self._show_help()\n",
    "                    continue\n",
    "                \n",
    "                if question.lower() in ['summary', 'ìš”ì•½']:\n",
    "                    summary = self.get_database_summary()\n",
    "                    print(f\"\\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìš”ì•½: {summary['total_tables']}ê°œ í…Œì´ë¸”\")\n",
    "                    for table in summary['tables'][:5]:\n",
    "                        print(f\"   ğŸ“‹ {table['name']}: {table['row_count']:,}í–‰, {table['column_count']}ì»¬ëŸ¼\")\n",
    "                    continue\n",
    "                \n",
    "                if not question:\n",
    "                    continue\n",
    "                \n",
    "                # ì§ˆë¬¸ ì²˜ë¦¬\n",
    "                result = self.ask_question(question)\n",
    "                self._display_result(result)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nğŸ‘‹ ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "    \n",
    "    def _show_help(self):\n",
    "        \"\"\"ë„ì›€ë§ í‘œì‹œ\"\"\"\n",
    "        help_text = \"\"\"\n",
    "ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:\n",
    "- help, ë„ì›€ë§: ì´ ë„ì›€ë§ í‘œì‹œ\n",
    "- summary, ìš”ì•½: ë°ì´í„°ë² ì´ìŠ¤ ìš”ì•½ ì •ë³´\n",
    "- quit, exit, ì¢…ë£Œ: ì„¸ì…˜ ì¢…ë£Œ\n",
    "\n",
    "ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ:\n",
    "- \"ê³ ê° ìˆ˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”\"\n",
    "- \"ì›”ë³„ ë§¤ì¶œì„ ë³´ì—¬ì£¼ì„¸ìš”\"\n",
    "- \"ê°€ì¥ ë§ì´ íŒ”ë¦° ìƒí’ˆ 10ê°œëŠ”?\"\n",
    "- \"ìµœê·¼ ì¼ì£¼ì¼ ì£¼ë¬¸ í˜„í™©ì€?\"\n",
    "\"\"\"\n",
    "        print(help_text)\n",
    "    \n",
    "    def _display_result(self, result: Dict[str, Any]):\n",
    "        \"\"\"ê²°ê³¼ í‘œì‹œ\"\"\"\n",
    "        if not result[\"success\"]:\n",
    "            print(f\"\\nâŒ ì˜¤ë¥˜: {result['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nğŸ” SQL ì¿¼ë¦¬:\")\n",
    "        print(f\"```sql\\n{result['sql_query']}\\n```\")\n",
    "        \n",
    "        if result[\"relevant_schemas\"]:\n",
    "            print(f\"\\nğŸ“‹ ì‚¬ìš©ëœ í…Œì´ë¸”:\")\n",
    "            for schema in result[\"relevant_schemas\"]:\n",
    "                score = schema[\"score\"]\n",
    "                print(f\"   â€¢ {schema['table']} (ë§¤ì¹­ì ìˆ˜: {score:.3f})\")\n",
    "        \n",
    "        if result[\"execution_result\"]:\n",
    "            execution = result[\"execution_result\"]\n",
    "            if execution[\"success\"]:\n",
    "                print(f\"\\nâœ… ì‹¤í–‰ ê²°ê³¼ ({execution['row_count']}ê°œ í–‰):\")\n",
    "                if execution[\"results\"]:\n",
    "                    df = pd.DataFrame(execution[\"results\"][:10])  # ì²˜ìŒ 10ê°œë§Œ\n",
    "                    print(df.to_string(index=False))\n",
    "                    if execution[\"row_count\"] > 10:\n",
    "                        print(f\"... ê·¸ ì™¸ {execution['row_count'] - 10}ê°œ í–‰\")\n",
    "            else:\n",
    "                print(f\"\\nâŒ ì‹¤í–‰ ì‹¤íŒ¨: {execution['error']}\")\n",
    "\n",
    "print(\"âœ… Text-to-SQL RAG ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
    "\n",
    "# ğŸ“‹ 01ë‹¨ê³„ ì™„ë£Œ ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„\n",
    "\n",
    "print(\"ğŸ‰ Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# í˜„ì¬ ìƒíƒœ í™•ì¸\n",
    "if spark and 'northwind_schema' in globals():\n",
    "    \n",
    "    print(\"âœ… êµ¬ì¶• ì™„ë£Œ ìƒíƒœ:\")\n",
    "    schema = northwind_schema\n",
    "    \n",
    "    print(f\"   ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤: {schema['database_name']}\")\n",
    "    print(f\"   ğŸ“‹ í…Œì´ë¸” ìˆ˜: {schema['total_tables']}ê°œ\")\n",
    "    \n",
    "    # ê° í…Œì´ë¸” ìƒíƒœ\n",
    "    print(f\"\\nğŸ“‹ í…Œì´ë¸”ë³„ ë°ì´í„° í˜„í™©:\")\n",
    "    for table_name, info in schema['schema_details'].items():\n",
    "        print(f\"   ğŸ“„ {table_name}: {info['row_count']:,}ê°œ ë ˆì½”ë“œ, {len(info['columns'])}ê°œ ì»¬ëŸ¼\")\n",
    "    \n",
    "    # ì´ ë°ì´í„° ê·œëª¨\n",
    "    import builtins\n",
    "    total_records = builtins.sum(info['row_count'] for info in schema['schema_details'].values())\n",
    "    print(f\"\\nğŸ“ˆ ì´ ë°ì´í„°: {total_records:,}ê°œ ë ˆì½”ë“œ\")\n",
    "    \n",
    "    # ê´€ê³„ í™•ì¸\n",
    "    print(f\"ğŸ”— í…Œì´ë¸” ê´€ê³„: {len(schema['relationships'])}ê°œ\")\n",
    "    for rel_name, rel_desc in list(schema['relationships'].items())[:3]:\n",
    "        print(f\"   â€¢ {rel_desc}\")\n",
    "    if len(schema['relationships']) > 3:\n",
    "        print(f\"   ... ê·¸ ì™¸ {len(schema['relationships']) - 3}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Text-to-SQL í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤\n",
    "    sample_questions = [\n",
    "        \"ì „ì²´ ê³ ê° ìˆ˜ëŠ” ëª‡ ëª…ì¸ê°€ìš”?\",\n",
    "        \"ê°€ì¥ ë§ì´ íŒ”ë¦° ìƒí’ˆ 5ê°œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”\",\n",
    "        \"ì›”ë³„ ì£¼ë¬¸ í˜„í™©ì„ ì•Œë ¤ì£¼ì„¸ìš”\", \n",
    "        \"ê³ ê°ë³„ ì´ êµ¬ë§¤ ê¸ˆì•¡ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”\",\n",
    "        \"ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ê°œìˆ˜ëŠ”?\",\n",
    "        \"ì§ì›ë³„ ë‹´ë‹¹ ì£¼ë¬¸ ê±´ìˆ˜ëŠ”?\",\n",
    "        \"ë°°ì†¡ë¹„ê°€ ê°€ì¥ ë¹„ì‹¼ ì£¼ë¬¸ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”\",\n",
    "        \"í• ì¸ì´ ì ìš©ëœ ì£¼ë¬¸ ìƒì„¸ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nğŸ’¬ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” ìì—°ì–´ ì§ˆë¬¸ ì˜ˆì‹œ:\")\n",
    "    for i, question in enumerate(sample_questions, 1):\n",
    "        print(f\"   {i}. {question}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”§ ë‹¤ìŒ ë…¸íŠ¸ë¶ ì‹¤í–‰ ë°©ë²•:\")\n",
    "    print(f\"   1. ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±: 02_langchain_agent_text_to_sql.ipynb\")\n",
    "    print(f\"   2. ì´ ë…¸íŠ¸ë¶ì˜ ë³€ìˆ˜ë“¤ í™œìš©:\")\n",
    "    print(f\"      - spark: Spark ì„¸ì…˜\")\n",
    "    print(f\"      - northwind_schema: ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°\")\n",
    "    print(f\"   3. LangChain Agent êµ¬í˜„ ì‹œì‘\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   ìœ„ì˜ ëª¨ë“  ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# í˜„ì¬ ë…¸íŠ¸ë¶ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë“¤\n",
    "print(f\"\\nğŸ› ï¸ í˜„ì¬ ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤:\")\n",
    "available_functions = [\n",
    "    \"test_northwind_queries() - ê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰\",\n",
    "    \"verify_northwind_database() - ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì¬í™•ì¸\", \n",
    "    \"analyze_schema_for_langchain() - ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¬ë¶„ì„\",\n",
    "    \"spark.sql('YOUR_SQL_QUERY') - ì§ì ‘ SQL ì‹¤í–‰\"\n",
    "]\n",
    "\n",
    "for func in available_functions:\n",
    "    print(f\"   â€¢ {func}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ëª©í‘œ ë‹¬ì„±!\")\n",
    "print(f\"   âœ… Databricksì— ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•\")\n",
    "print(f\"   âœ… LangChain Agent êµ¬í˜„ì„ ìœ„í•œ ê¸°ë°˜ í™˜ê²½ ì™„ë£Œ\")\n",
    "print(f\"   âœ… Text-to-SQL ì‹œìŠ¤í…œì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ ì„±ê³µ\")\n",
    "\n",
    "print(f\"\\nğŸš€ ë‹¤ìŒ: LangChain Agent ê¸°ë°˜ Text-to-SQL êµ¬í˜„!\")\n",
    "print(f\"   ë…¸íŠ¸ë¶: 02_langchain_agent_text_to_sql.ipynb\")\n",
    "\n",
    "# ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ë° LangChain Agent ì—°ë™ ì¤€ë¹„\n",
    "\n",
    "print(\"ğŸ”— LangChain Agent ì—°ë™ì„ ìœ„í•œ ìŠ¤í‚¤ë§ˆ ë¶„ì„\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ\n",
    "def extract_table_schema(table_name):\n",
    "    \"\"\"í…Œì´ë¸”ì˜ ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        # ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ\n",
    "        describe_result = spark.sql(f\"DESCRIBE TABLE northwind.{table_name}\").collect()\n",
    "        \n",
    "        columns = []\n",
    "        for row in describe_result:\n",
    "            if row.col_name and not row.col_name.startswith('#'):\n",
    "                columns.append({\n",
    "                    \"name\": row.col_name,\n",
    "                    \"type\": row.data_type,\n",
    "                    \"nullable\": True  # Delta LakeëŠ” ê¸°ë³¸ì ìœ¼ë¡œ nullable\n",
    "                })\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ\n",
    "        sample_df = spark.table(f\"northwind.{table_name}\").limit(3)\n",
    "        sample_data = [row.asDict() for row in sample_df.collect()]\n",
    "        \n",
    "        # í…Œì´ë¸” í†µê³„\n",
    "        total_count = spark.table(f\"northwind.{table_name}\").count()\n",
    "        \n",
    "        return {\n",
    "            \"table_name\": table_name,\n",
    "            \"columns\": columns,\n",
    "            \"sample_data\": sample_data,\n",
    "            \"total_records\": total_count,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"table_name\": table_name,\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"failed\"\n",
    "        }\n",
    "\n",
    "# 2. ëª¨ë“  í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ\n",
    "print(\"\\nğŸ“Š í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ì¤‘...\")\n",
    "northwind_schema_info = {}\n",
    "\n",
    "tables = spark.sql(\"SHOW TABLES IN northwind\").collect()\n",
    "table_names = [row.tableName for row in tables]\n",
    "\n",
    "for table_name in sorted(table_names):\n",
    "    print(f\"   ğŸ” {table_name} ë¶„ì„ ì¤‘...\")\n",
    "    schema_info = extract_table_schema(table_name)\n",
    "    northwind_schema_info[table_name] = schema_info\n",
    "    \n",
    "    if schema_info[\"status\"] == \"success\":\n",
    "        col_count = len(schema_info[\"columns\"])\n",
    "        record_count = schema_info[\"total_records\"]\n",
    "        print(f\"      âœ… {col_count}ê°œ ì»¬ëŸ¼, {record_count:,}ê°œ ë ˆì½”ë“œ\")\n",
    "    else:\n",
    "        print(f\"      âŒ ì‹¤íŒ¨: {schema_info['error']}\")\n",
    "\n",
    "# 3. í…Œì´ë¸” ê°„ ê´€ê³„ ì •ì˜ (Northwind í‘œì¤€ ê´€ê³„)\n",
    "table_relationships = {\n",
    "    \"customers\": {\n",
    "        \"primary_key\": \"customerid\",\n",
    "        \"related_tables\": [\"orders\"],\n",
    "        \"description\": \"ê³ ê° ì •ë³´ (ì£¼ë¬¸ê³¼ 1:N ê´€ê³„)\"\n",
    "    },\n",
    "    \"suppliers\": {\n",
    "        \"primary_key\": \"supplierid\", \n",
    "        \"related_tables\": [\"products\"],\n",
    "        \"description\": \"ê³µê¸‰ì—…ì²´ ì •ë³´ (ìƒí’ˆê³¼ 1:N ê´€ê³„)\"\n",
    "    },\n",
    "    \"categories\": {\n",
    "        \"primary_key\": \"categoryid\",\n",
    "        \"related_tables\": [\"products\"],\n",
    "        \"description\": \"ìƒí’ˆ ì¹´í…Œê³ ë¦¬ (ìƒí’ˆê³¼ 1:N ê´€ê³„)\"\n",
    "    },\n",
    "    \"products\": {\n",
    "        \"primary_key\": \"productid\",\n",
    "        \"foreign_keys\": [\"supplierid\", \"categoryid\"],\n",
    "        \"related_tables\": [\"order_details\", \"suppliers\", \"categories\"],\n",
    "        \"description\": \"ìƒí’ˆ ì •ë³´ (ì£¼ë¬¸ìƒì„¸ì™€ 1:N, ê³µê¸‰ì—…ì²´/ì¹´í…Œê³ ë¦¬ì™€ N:1 ê´€ê³„)\"\n",
    "    },\n",
    "    \"employees\": {\n",
    "        \"primary_key\": \"employeeid\",\n",
    "        \"related_tables\": [\"orders\"],\n",
    "        \"description\": \"ì§ì› ì •ë³´ (ì£¼ë¬¸ê³¼ 1:N ê´€ê³„)\"\n",
    "    },\n",
    "    \"shippers\": {\n",
    "        \"primary_key\": \"shipperid\",\n",
    "        \"related_tables\": [\"orders\"],\n",
    "        \"description\": \"ìš´ì†¡ì—…ì²´ ì •ë³´ (ì£¼ë¬¸ê³¼ 1:N ê´€ê³„)\"\n",
    "    },\n",
    "    \"orders\": {\n",
    "        \"primary_key\": \"orderid\",\n",
    "        \"foreign_keys\": [\"customerid\", \"employeeid\", \"shipvia\"],\n",
    "        \"related_tables\": [\"order_details\", \"customers\", \"employees\", \"shippers\"],\n",
    "        \"description\": \"ì£¼ë¬¸ ì •ë³´ (ì£¼ë¬¸ìƒì„¸ì™€ 1:N, ê³ ê°/ì§ì›/ìš´ì†¡ì—…ì²´ì™€ N:1 ê´€ê³„)\"\n",
    "    },\n",
    "    \"order_details\": {\n",
    "        \"primary_key\": [\"orderid\", \"productid\"],\n",
    "        \"foreign_keys\": [\"orderid\", \"productid\"],\n",
    "        \"related_tables\": [\"orders\", \"products\"],\n",
    "        \"description\": \"ì£¼ë¬¸ ìƒì„¸ ì •ë³´ (ì£¼ë¬¸/ìƒí’ˆê³¼ N:1 ê´€ê³„)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 4. Agentìš© ìŠ¤í‚¤ë§ˆ ìš”ì•½ ìƒì„±\n",
    "agent_schema_summary = {\n",
    "    \"database_name\": \"northwind\",\n",
    "    \"description\": \"Northwind ìƒ˜í”Œ ë¬´ì—­ íšŒì‚¬ ë°ì´í„°ë² ì´ìŠ¤ - ê³ ê°, ì£¼ë¬¸, ìƒí’ˆ, ì§ì› ë“±ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë°ì´í„°\",\n",
    "    \"total_tables\": len(table_names),\n",
    "    \"tables\": {},\n",
    "    \"relationships\": table_relationships,\n",
    "    \"common_queries\": [\n",
    "        \"ê³ ê°ë³„ ì£¼ë¬¸ í†µê³„\",\n",
    "        \"ìƒí’ˆë³„ ë§¤ì¶œ ë¶„ì„\", \n",
    "        \"ì§ì›ë³„ ì„±ê³¼ ë¶„ì„\",\n",
    "        \"ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ í˜„í™©\",\n",
    "        \"êµ­ê°€ë³„ ê³ ê° ë¶„í¬\",\n",
    "        \"ì›”ë³„ ë§¤ì¶œ ì¶”ì´\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ê° í…Œì´ë¸” ì •ë³´ë¥¼ Agentìš©ìœ¼ë¡œ ìš”ì•½\n",
    "for table_name, schema_info in northwind_schema_info.items():\n",
    "    if schema_info[\"status\"] == \"success\":\n",
    "        agent_schema_summary[\"tables\"][table_name] = {\n",
    "            \"description\": table_relationships.get(table_name, {}).get(\"description\", f\"{table_name} í…Œì´ë¸”\"),\n",
    "            \"columns\": [f\"{col['name']} ({col['type']})\" for col in schema_info[\"columns\"]],\n",
    "            \"primary_key\": table_relationships.get(table_name, {}).get(\"primary_key\"),\n",
    "            \"foreign_keys\": table_relationships.get(table_name, {}).get(\"foreign_keys\", []),\n",
    "            \"record_count\": schema_info[\"total_records\"],\n",
    "            \"sample_data\": schema_info[\"sample_data\"][:1]  # ì²« ë²ˆì§¸ ë ˆì½”ë“œë§Œ\n",
    "        }\n",
    "\n",
    "print(f\"\\nğŸ“‹ LangChain Agentìš© ìŠ¤í‚¤ë§ˆ ìš”ì•½:\")\n",
    "print(f\"   ğŸ“‚ ë°ì´í„°ë² ì´ìŠ¤: {agent_schema_summary['database_name']}\")\n",
    "print(f\"   ğŸ“Š í…Œì´ë¸” ìˆ˜: {agent_schema_summary['total_tables']}ê°œ\")\n",
    "print(f\"   ğŸ”— ê´€ê³„ ì •ì˜: {len(table_relationships)}ê°œ í…Œì´ë¸”\")\n",
    "\n",
    "total_columns = builtins.sum(len(info.get('columns', [])) for info in agent_schema_summary['tables'].values())\n",
    "total_records = builtins.sum(info.get('record_count', 0) for info in agent_schema_summary['tables'].values())\n",
    "\n",
    "print(f\"   ğŸ“ˆ ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {total_columns}ê°œ\")\n",
    "print(f\"   ğŸ“Š ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê°œ\")\n",
    "\n",
    "# 5. ì—°ë™ ì¤€ë¹„ ì™„ë£Œ ìƒíƒœ í™•ì¸\n",
    "print(f\"\\nâœ… LangChain Agent ì—°ë™ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“‹ ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°:\")\n",
    "print(f\"   ğŸ—„ï¸ northwind_schema_info: ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´\")\n",
    "print(f\"   ğŸ“Š agent_schema_summary: Agentìš© ìš”ì•½ ì •ë³´\")\n",
    "print(f\"   ğŸ”— table_relationships: í…Œì´ë¸” ê´€ê³„ ì •ë³´\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(f\"   1. 02_langchain_agent_text_to_sql.ipynb ë…¸íŠ¸ë¶ ì—´ê¸°\")\n",
    "print(f\"   2. ìœ„ ë³€ìˆ˜ë“¤ì„ í™œìš©í•˜ì—¬ LangChain Agent êµ¬í˜„\")\n",
    "print(f\"   3. Text-to-SQL ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë°ëª¨\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Northwind ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° Agent ì—°ë™ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
