{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f61094",
   "metadata": {},
   "source": [
    "# 01. Databricks 환경 설정 및 Northwind 데이터베이스 구축\n",
    "\n",
    "이 노트북은 **Databricks Text-to-SQL RAG 시스템**의 첫 번째 단계로, 실제 Databricks Lakehouse 환경에서 Northwind 샘플 데이터베이스를 구축합니다.\n",
    "\n",
    "## 🎯 목표\n",
    "- **Databricks 환경 설정** 및 Spark 세션 초기화\n",
    "- **Northwind 샘플 데이터베이스** 스키마 설계 및 구축\n",
    "- **Delta Lake 테이블** 생성 및 데이터 로드\n",
    "- **데이터 검증** 및 기본 SQL 테스트\n",
    "- **LangChain Agent 연동** 준비\n",
    "\n",
    "## 🏗️ Northwind 데이터베이스 구조\n",
    "```\n",
    "Northwind Database (8개 테이블)\n",
    "├── customers      # 고객 정보\n",
    "├── suppliers      # 공급업체 정보\n",
    "├── categories     # 상품 카테고리\n",
    "├── products       # 상품 정보\n",
    "├── employees      # 직원 정보\n",
    "├── shippers       # 운송업체 정보\n",
    "├── orders         # 주문 정보\n",
    "└── order_details  # 주문 상세 정보\n",
    "```\n",
    "\n",
    "## 📋 전제조건\n",
    "- Databricks Workspace 및 클러스터 준비\n",
    "- Delta Lake 테이블 생성 권한\n",
    "- Spark SQL 실행 권한\n",
    "\n",
    "## 🚀 다음 단계\n",
    "이 노트북 완료 후 → `02_langchain_agent_text_to_sql.ipynb`에서 LangChain Agent 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027e64f",
   "metadata": {},
   "source": [
    "## 🗄️ Northwind 샘플 데이터베이스 소개\n",
    "\n",
    "Northwind는 **가상의 무역회사 데이터베이스**로 Text-to-SQL 학습에 이상적입니다.\n",
    "\n",
    "### 📊 Northwind 데이터베이스 구조\n",
    "- **Customers** (고객): 회사명, 연락처, 주소 등\n",
    "- **Orders** (주문): 주문일자, 고객정보, 배송정보 등  \n",
    "- **Order_Details** (주문상세): 상품별 수량, 가격, 할인 등\n",
    "- **Products** (상품): 상품명, 카테고리, 재고, 가격 등\n",
    "- **Categories** (카테고리): 상품 분류 정보\n",
    "- **Suppliers** (공급업체): 공급업체 정보 및 연락처\n",
    "- **Employees** (직원): 직원 정보 및 관리자 관계\n",
    "- **Shippers** (배송업체): 배송 회사 정보\n",
    "\n",
    "### 🎯 Text-to-SQL에 적합한 이유\n",
    "1. **실무 친화적**: 실제 비즈니스 시나리오 반영\n",
    "2. **다양한 관계**: JOIN, 집계, 필터링 등 다양한 SQL 패턴\n",
    "3. **직관적 데이터**: 도메인 지식이 쉽게 이해 가능\n",
    "4. **적당한 크기**: 학습 및 테스트에 최적화된 데이터 규모\n",
    "\n",
    "## 1. Databricks 환경 초기화 및 Spark 세션 설정\n",
    "1. **CSV 파일 업로드** → Delta Lake 테이블 생성\n",
    "2. **PostgreSQL 연결** → Databricks로 데이터 이관  \n",
    "3. **SQL 스크립트 실행** → 직접 테이블 생성\n",
    "4. **샘플 데이터 생성** → 프로그래밍 방식으로 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a417291",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 및 Databricks 서비스 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745aa3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Databricks Text-to-SQL RAG 시스템 초기화\n",
      "==================================================\n",
      "\n",
      "📋 환경 정보:\n",
      "Python 버전: 3.11.6\n",
      "Databricks Runtime: N/A\n",
      "Spark 버전: 3.5.2\n",
      "Spark 버전: 3.5.2\n",
      "현재 데이터베이스: default\n",
      "현재 사용자: saintphs@gmail.com\n",
      "현재 데이터베이스: default\n",
      "현재 사용자: saintphs@gmail.com\n",
      "현재 카탈로그: workspace\n",
      "현재 스키마: default\n",
      "현재 카탈로그: workspace\n",
      "현재 스키마: default\n",
      "Adaptive Query Execution: ❌ 비활성화됨\n",
      "\n",
      "✅ Databricks 환경 및 Spark 세션 준비 완료!\n",
      "📅 초기화 시간: 2025-06-30 08:43:26\n",
      "\n",
      "💡 참고: Spark Connect 환경에서 일부 JVM 종속 기능들은 제한될 수 있습니다.\n",
      "Adaptive Query Execution: ❌ 비활성화됨\n",
      "\n",
      "✅ Databricks 환경 및 Spark 세션 준비 완료!\n",
      "📅 초기화 시간: 2025-06-30 08:43:26\n",
      "\n",
      "💡 참고: Spark Connect 환경에서 일부 JVM 종속 기능들은 제한될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# Databricks 환경 및 Spark 세션 초기화\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🚀 Databricks Text-to-SQL RAG 시스템 초기화\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 환경 정보 확인\n",
    "print(\"\\n📋 환경 정보:\")\n",
    "print(f\"Python 버전: {sys.version.split()[0]}\")\n",
    "print(f\"Databricks Runtime: {os.environ.get('DATABRICKS_RUNTIME_VERSION', 'N/A')}\")\n",
    "\n",
    "# Spark 세션 초기화\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Northwind-TextToSQL-RAG\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(f\"Spark 버전: {spark.version}\")\n",
    "\n",
    "# Spark Connect 환경에서 안전한 방식으로 애플리케이션 정보 조회\n",
    "try:\n",
    "    # Spark Connect에서는 sparkContext에 직접 접근할 수 없으므로 다른 방법 사용\n",
    "    app_info = spark.sql(\"SELECT current_database(), current_user()\").collect()[0]\n",
    "    print(f\"현재 데이터베이스: {app_info[0] if app_info[0] else 'default'}\")\n",
    "    print(f\"현재 사용자: {app_info[1] if app_info[1] else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"애플리케이션 정보 조회 제한됨 (Spark Connect 환경)\")\n",
    "\n",
    "# 현재 카탈로그 및 스키마 확인\n",
    "try:\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "    print(f\"현재 카탈로그: {current_catalog}\")\n",
    "    print(f\"현재 스키마: {current_schema}\")\n",
    "except Exception as e:\n",
    "    print(f\"카탈로그/스키마 정보 조회 실패: {e}\")\n",
    "\n",
    "# Spark 설정 확인 (Spark Connect에서 지원되는 방식)\n",
    "try:\n",
    "    spark_configs = spark.sql(\"SET\").collect()\n",
    "    adaptive_enabled = any(\"spark.sql.adaptive.enabled\" in row.key and \"true\" in str(row.value).lower() \n",
    "                          for row in spark_configs if hasattr(row, 'key'))\n",
    "    print(f\"Adaptive Query Execution: {'✅ 활성화됨' if adaptive_enabled else '❌ 비활성화됨'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Spark 설정 확인 제한됨\")\n",
    "\n",
    "print(\"\\n✅ Databricks 환경 및 Spark 세션 준비 완료!\")\n",
    "print(f\"📅 초기화 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n💡 참고: Spark Connect 환경에서 일부 JVM 종속 기능들은 제한될 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83eac58",
   "metadata": {},
   "source": [
    "## 2. Northwind 데이터베이스 생성 및 스키마 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa4e298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Databricks 환경 정보:\n",
      "   is_databricks: False\n",
      "   spark_version: 3.5.2\n",
      "   runtime_version: 로컬\n",
      "   cluster_id: 로컬\n",
      "✅ Databricks 환경 설정 완료\n",
      "📂 northwind 데이터베이스 생성/선택 완료\n",
      "\n",
      "🚀 Spark 세션 활성화 성공!\n",
      "   Spark Connect 모드로 실행 중\n",
      "📂 northwind 데이터베이스 생성/선택 완료\n",
      "\n",
      "🚀 Spark 세션 활성화 성공!\n",
      "   Spark Connect 모드로 실행 중\n",
      "   현재 데이터베이스: northwind\n",
      "   현재 데이터베이스: northwind\n",
      "   세션 시작 시간: 2025-06-30 08:43:27.854717\n",
      "   세션 시작 시간: 2025-06-30 08:43:27.854717\n",
      "   현재 카탈로그: workspace\n",
      "   현재 스키마: northwind\n",
      "\n",
      "✅ Spark 세션이 성공적으로 설정되었습니다!\n",
      "🏗️ Northwind 데이터베이스 생성 시작\n",
      "========================================\n",
      "   현재 카탈로그: workspace\n",
      "   현재 스키마: northwind\n",
      "\n",
      "✅ Spark 세션이 성공적으로 설정되었습니다!\n",
      "🏗️ Northwind 데이터베이스 생성 시작\n",
      "========================================\n",
      "✅ northwind 데이터베이스 생성 완료\n",
      "✅ northwind 데이터베이스 생성 완료\n",
      "📂 현재 활성 데이터베이스: northwind\n",
      "📂 현재 활성 데이터베이스: northwind\n",
      "📊 기존 테이블: 8개\n",
      "   기존 테이블 목록:\n",
      "     📋 categories\n",
      "     📋 customers\n",
      "     📋 employees\n",
      "     📋 order_details\n",
      "     📋 orders\n",
      "     ... 그 외 3개\n",
      "\n",
      "📋 총 8개 테이블 스키마 정의됨:\n",
      "   📂 categories\n",
      "   📂 suppliers\n",
      "   📂 products\n",
      "   📂 customers\n",
      "   📂 employees\n",
      "   📂 shippers\n",
      "   📂 orders\n",
      "   📂 order_details\n",
      "\n",
      "✅ Northwind 스키마 정의 완료!\n",
      "📊 기존 테이블: 8개\n",
      "   기존 테이블 목록:\n",
      "     📋 categories\n",
      "     📋 customers\n",
      "     📋 employees\n",
      "     📋 order_details\n",
      "     📋 orders\n",
      "     ... 그 외 3개\n",
      "\n",
      "📋 총 8개 테이블 스키마 정의됨:\n",
      "   📂 categories\n",
      "   📂 suppliers\n",
      "   📂 products\n",
      "   📂 customers\n",
      "   📂 employees\n",
      "   📂 shippers\n",
      "   📂 orders\n",
      "   📂 order_details\n",
      "\n",
      "✅ Northwind 스키마 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "# Databricks 환경 확인 및 기본 설정\n",
    "def setup_databricks_environment():\n",
    "    \"\"\"Databricks 환경 설정 및 확인\"\"\"\n",
    "    try:\n",
    "        # Spark 세션 확인/생성\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark is None:\n",
    "            spark = SparkSession.builder.appName(\"Northwind-TextToSQL\").getOrCreate()\n",
    "        \n",
    "        # 환경 정보 수집\n",
    "        is_databricks = \"DATABRICKS_RUNTIME_VERSION\" in os.environ\n",
    "        \n",
    "        env_info = {\n",
    "            \"is_databricks\": is_databricks,\n",
    "            \"spark_version\": spark.version,\n",
    "            \"runtime_version\": os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", \"로컬\"),\n",
    "            \"cluster_id\": os.environ.get(\"SPARK_LOCAL_HOSTNAME\", \"로컬\"),\n",
    "        }\n",
    "        \n",
    "        print(\"🔧 Databricks 환경 정보:\")\n",
    "        for key, value in env_info.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        print(\"✅ Databricks 환경 설정 완료\")\n",
    "        \n",
    "        # 기본 데이터베이스 설정\n",
    "        spark.sql(\"CREATE DATABASE IF NOT EXISTS northwind\")\n",
    "        spark.sql(\"USE northwind\")\n",
    "        \n",
    "        print(\"📂 northwind 데이터베이스 생성/선택 완료\")\n",
    "        \n",
    "        return spark, env_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Databricks 환경 설정 실패: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# 환경 설정 실행\n",
    "spark, env_info = setup_databricks_environment()\n",
    "\n",
    "if spark:\n",
    "    print(\"\\n🚀 Spark 세션 활성화 성공!\")\n",
    "    # Spark Connect 호환 - sparkContext 사용하지 않음\n",
    "    print(\"   Spark Connect 모드로 실행 중\")\n",
    "\n",
    "    # 현재 데이터베이스 확인\n",
    "    current_db = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "    print(f\"   현재 데이터베이스: {current_db}\")\n",
    "    \n",
    "    # Spark 세션 정보 (Spark Connect 호환)\n",
    "    app_info = spark.sql(\"SELECT 'session_active' as status, current_timestamp() as timestamp\").collect()[0]\n",
    "    print(f\"   세션 시작 시간: {app_info['timestamp']}\")\n",
    "    \n",
    "    # 현재 카탈로그 및 스키마 정보\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "    print(f\"   현재 카탈로그: {current_catalog}\")\n",
    "    print(f\"   현재 스키마: {current_schema}\")\n",
    "    \n",
    "    print(\"\\n✅ Spark 세션이 성공적으로 설정되었습니다!\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Spark 세션이 활성화되지 않았습니다.\")\n",
    "\n",
    "# Northwind 데이터베이스 생성 및 스키마 정의\n",
    "\n",
    "print(\"🏗️ Northwind 데이터베이스 생성 시작\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. 데이터베이스 생성\n",
    "try:\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS northwind\")\n",
    "    spark.sql(\"USE northwind\")\n",
    "    print(\"✅ northwind 데이터베이스 생성 완료\")\n",
    "    \n",
    "    # 현재 데이터베이스 확인\n",
    "    current_db = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "    print(f\"📂 현재 활성 데이터베이스: {current_db}\")\n",
    "    \n",
    "    # 기존 테이블 확인\n",
    "    existing_tables = [row.tableName for row in spark.sql(\"SHOW TABLES\").collect()]\n",
    "    print(f\"📊 기존 테이블: {len(existing_tables)}개\")\n",
    "    if existing_tables:\n",
    "        print(\"   기존 테이블 목록:\")\n",
    "        for table in existing_tables[:5]:  # 최대 5개만 표시\n",
    "            print(f\"     📋 {table}\")\n",
    "        if len(existing_tables) > 5:\n",
    "            print(f\"     ... 그 외 {len(existing_tables) - 5}개\")\n",
    "    else:\n",
    "        print(\"   새로운 데이터베이스입니다. 테이블을 생성할 준비가 되었습니다.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터베이스 생성 실패: {e}\")\n",
    "\n",
    "# 2. Northwind 테이블 스키마 정의\n",
    "northwind_schemas = {\n",
    "    \"categories\": \"\"\"\n",
    "        categoryid INT,\n",
    "        categoryname STRING,\n",
    "        description STRING\n",
    "    \"\"\",\n",
    "    \n",
    "    \"suppliers\": \"\"\"\n",
    "        supplierid INT,\n",
    "        companyname STRING,\n",
    "        contactname STRING,\n",
    "        contacttitle STRING,\n",
    "        address STRING,\n",
    "        city STRING,\n",
    "        region STRING,\n",
    "        postalcode STRING,\n",
    "        country STRING,\n",
    "        phone STRING,\n",
    "        fax STRING,\n",
    "        homepage STRING\n",
    "    \"\"\",\n",
    "    \n",
    "    \"products\": \"\"\"\n",
    "        productid INT,\n",
    "        productname STRING,\n",
    "        supplierid INT,\n",
    "        categoryid INT,\n",
    "        quantityperunit STRING,\n",
    "        unitprice DECIMAL(10,2),\n",
    "        unitsinstock INT,\n",
    "        unitsonorder INT,\n",
    "        reorderlevel INT,\n",
    "        discontinued BOOLEAN\n",
    "    \"\"\",\n",
    "    \n",
    "    \"customers\": \"\"\"\n",
    "        customerid STRING,\n",
    "        companyname STRING,\n",
    "        contactname STRING,\n",
    "        contacttitle STRING,\n",
    "        address STRING,\n",
    "        city STRING,\n",
    "        region STRING,\n",
    "        postalcode STRING,\n",
    "        country STRING,\n",
    "        phone STRING,\n",
    "        fax STRING\n",
    "    \"\"\",\n",
    "    \n",
    "    \"employees\": \"\"\"\n",
    "        employeeid INT,\n",
    "        lastname STRING,\n",
    "        firstname STRING,\n",
    "        title STRING,\n",
    "        titleofcourtesy STRING,\n",
    "        birthdate DATE,\n",
    "        hiredate DATE,\n",
    "        address STRING,\n",
    "        city STRING,\n",
    "        region STRING,\n",
    "        postalcode STRING,\n",
    "        country STRING,\n",
    "        homephone STRING,\n",
    "        extension STRING,\n",
    "        notes STRING,\n",
    "        reportsto INT\n",
    "    \"\"\",\n",
    "    \n",
    "    \"shippers\": \"\"\"\n",
    "        shipperid INT,\n",
    "        companyname STRING,\n",
    "        phone STRING\n",
    "    \"\"\",\n",
    "    \n",
    "    \"orders\": \"\"\"\n",
    "        orderid INT,\n",
    "        customerid STRING,\n",
    "        employeeid INT,\n",
    "        orderdate DATE,\n",
    "        requireddate DATE,\n",
    "        shippeddate DATE,\n",
    "        shipvia INT,\n",
    "        freight DECIMAL(10,2),\n",
    "        shipname STRING,\n",
    "        shipaddress STRING,\n",
    "        shipcity STRING,\n",
    "        shipregion STRING,\n",
    "        shippostalcode STRING,\n",
    "        shipcountry STRING\n",
    "    \"\"\",\n",
    "    \n",
    "    \"order_details\": \"\"\"\n",
    "        orderid INT,\n",
    "        productid INT,\n",
    "        unitprice DECIMAL(10,2),\n",
    "        quantity INT,\n",
    "        discount DECIMAL(3,2)\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(f\"\\n📋 총 {len(northwind_schemas)}개 테이블 스키마 정의됨:\")\n",
    "for table_name in northwind_schemas.keys():\n",
    "    print(f\"   📂 {table_name}\")\n",
    "\n",
    "print(\"\\n✅ Northwind 스키마 정의 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb36ee4",
   "metadata": {},
   "source": [
    "## 🏗️ Northwind 데이터베이스 구축\n",
    "\n",
    "## 3. Delta Lake 테이블 생성 및 샘플 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d97ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터베이스 스키마 분석기 초기화 완료\n",
      "✅ 테이블 스키마 정의 완료\n",
      "   정의된 테이블: ['categories', 'suppliers', 'products', 'customers', 'employees', 'shippers', 'orders', 'order_details']\n",
      "✅ 샘플 데이터 생성 완료\n",
      "   categories: 8개 레코드\n",
      "   suppliers: 5개 레코드\n",
      "   products: 10개 레코드\n",
      "   customers: 5개 레코드\n",
      "   employees: 5개 레코드\n",
      "   shippers: 3개 레코드\n",
      "   orders: 100개 레코드\n",
      "   order_details: 290개 레코드\n",
      "\n",
      "🏗️ Northwind 데이터 빌더 준비 완료!\n",
      "📦 Delta Lake 테이블 생성 및 데이터 로드\n",
      "=============================================\n",
      "\n",
      "🔄 기존 테이블 정리 중...\n",
      "   🗑️ categories 테이블 삭제됨\n",
      "   🗑️ categories 테이블 삭제됨\n",
      "   🗑️ suppliers 테이블 삭제됨\n",
      "   🗑️ suppliers 테이블 삭제됨\n",
      "   🗑️ products 테이블 삭제됨\n",
      "   🗑️ products 테이블 삭제됨\n",
      "   🗑️ customers 테이블 삭제됨\n",
      "   🗑️ customers 테이블 삭제됨\n",
      "   🗑️ employees 테이블 삭제됨\n",
      "   🗑️ employees 테이블 삭제됨\n",
      "   🗑️ shippers 테이블 삭제됨\n",
      "   🗑️ shippers 테이블 삭제됨\n",
      "   🗑️ orders 테이블 삭제됨\n",
      "   🗑️ orders 테이블 삭제됨\n",
      "   🗑️ order_details 테이블 삭제됨\n",
      "\n",
      "📊 샘플 데이터 정의 완료:\n",
      "   📈 categories: 8개 레코드\n",
      "   📈 suppliers: 5개 레코드\n",
      "   📈 products: 10개 레코드\n",
      "   📈 customers: 10개 레코드\n",
      "   📈 employees: 5개 레코드\n",
      "   📈 shippers: 3개 레코드\n",
      "   📈 orders: 10개 레코드\n",
      "   📈 order_details: 17개 레코드\n",
      "\n",
      "✅ 샘플 데이터 준비 완료!\n",
      "   🗑️ order_details 테이블 삭제됨\n",
      "\n",
      "📊 샘플 데이터 정의 완료:\n",
      "   📈 categories: 8개 레코드\n",
      "   📈 suppliers: 5개 레코드\n",
      "   📈 products: 10개 레코드\n",
      "   📈 customers: 10개 레코드\n",
      "   📈 employees: 5개 레코드\n",
      "   📈 shippers: 3개 레코드\n",
      "   📈 orders: 10개 레코드\n",
      "   📈 order_details: 17개 레코드\n",
      "\n",
      "✅ 샘플 데이터 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, BooleanType, DateType\n",
    "\n",
    "class DatabaseSchemaAnalyzer:\n",
    "    \"\"\"실제 Databricks 환경에서 스키마 분석 및 벡터화\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session, embedding_model=None):\n",
    "        self.spark = spark_session\n",
    "        self.embedding_model = embedding_model\n",
    "        self.schema_cache = {}\n",
    "        \n",
    "    def extract_table_schema(self, database_name: str, table_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"실제 테이블에서 스키마 정보 추출\"\"\"\n",
    "        try:\n",
    "            full_table_name = f\"{database_name}.{table_name}\"\n",
    "            \n",
    "            # 테이블 설명 정보 가져오기\n",
    "            describe_df = self.spark.sql(f\"DESCRIBE TABLE EXTENDED {full_table_name}\")\n",
    "            columns_info = []\n",
    "            table_properties = {}\n",
    "            \n",
    "            # 컬럼 정보 처리\n",
    "            for row in describe_df.collect():\n",
    "                col_name = row.col_name\n",
    "                data_type = row.data_type\n",
    "                comment = row.comment\n",
    "                \n",
    "                if col_name and not col_name.startswith('#') and col_name != '':\n",
    "                    if col_name == '# Detailed Table Information':\n",
    "                        break\n",
    "                    \n",
    "                    columns_info.append({\n",
    "                        'name': col_name,\n",
    "                        'type': data_type,\n",
    "                        'comment': comment or ''\n",
    "                    })\n",
    "            \n",
    "            # 샘플 데이터 가져오기 (처음 3개 행)\n",
    "            sample_data = []\n",
    "            try:\n",
    "                sample_df = self.spark.sql(f\"SELECT * FROM {full_table_name} LIMIT 3\")\n",
    "                sample_data = [row.asDict() for row in sample_df.collect()]\n",
    "            except Exception:\n",
    "                sample_data = []\n",
    "            \n",
    "            # 테이블 통계 정보\n",
    "            try:\n",
    "                count_result = self.spark.sql(f\"SELECT COUNT(*) as row_count FROM {full_table_name}\").collect()\n",
    "                row_count = count_result[0].row_count if count_result else 0\n",
    "            except Exception:\n",
    "                row_count = 0\n",
    "            \n",
    "            schema_info = {\n",
    "                'database_name': database_name,\n",
    "                'table_name': table_name,\n",
    "                'full_name': full_table_name,\n",
    "                'columns': columns_info,\n",
    "                'sample_data': sample_data,\n",
    "                'row_count': row_count,\n",
    "                'extracted_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # 캐시에 저장\n",
    "            self.schema_cache[full_table_name] = schema_info\n",
    "            \n",
    "            return schema_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 테이블 {database_name}.{table_name} 스키마 추출 실패: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def analyze_database_schema(self, database_name: str, limit_tables: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"데이터베이스 전체 스키마 분석\"\"\"\n",
    "        if not self.spark:\n",
    "            print(\"❌ Spark 세션이 없습니다.\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # 데이터베이스의 모든 테이블 가져오기\n",
    "            tables_df = self.spark.sql(f\"SHOW TABLES IN {database_name}\")\n",
    "            tables = [(row.database, row.tableName) for row in tables_df.collect()]\n",
    "            \n",
    "            if limit_tables:\n",
    "                tables = tables[:limit_tables]\n",
    "            \n",
    "            print(f\"📊 {database_name} 데이터베이스 분석 시작 ({len(tables)}개 테이블)\")\n",
    "            \n",
    "            schemas = []\n",
    "            for db, table in tables:\n",
    "                print(f\"   🔍 {table} 분석 중...\")\n",
    "                schema = self.extract_table_schema(db, table)\n",
    "                if schema:\n",
    "                    schemas.append(schema)\n",
    "            \n",
    "            print(f\"✅ 스키마 분석 완료: {len(schemas)}개 테이블\")\n",
    "            return schemas\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 데이터베이스 스키마 분석 실패: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def create_schema_text_for_embedding(self, schema_info: Dict[str, Any]) -> str:\n",
    "        \"\"\"스키마 정보를 임베딩을 위한 텍스트로 변환\"\"\"\n",
    "        table_name = schema_info['table_name']\n",
    "        columns = schema_info['columns']\n",
    "        \n",
    "        # 테이블 설명 텍스트 생성\n",
    "        text_parts = [\n",
    "            f\"테이블명: {table_name}\",\n",
    "            f\"데이터베이스: {schema_info['database_name']}\",\n",
    "            f\"행 수: {schema_info.get('row_count', 0):,}\"\n",
    "        ]\n",
    "        \n",
    "        # 컬럼 정보 추가\n",
    "        text_parts.append(\"컬럼 정보:\")\n",
    "        for col in columns:\n",
    "            col_desc = f\"- {col['name']} ({col['type']})\"\n",
    "            if col.get('comment'):\n",
    "                col_desc += f\": {col['comment']}\"\n",
    "            text_parts.append(col_desc)\n",
    "        \n",
    "        # 샘플 데이터 키 추가 (검색에 도움)\n",
    "        if schema_info.get('sample_data'):\n",
    "            sample_keys = list(schema_info['sample_data'][0].keys()) if schema_info['sample_data'] else []\n",
    "            if sample_keys:\n",
    "                text_parts.append(f\"주요 필드: {', '.join(sample_keys[:5])}\")\n",
    "        \n",
    "        return \"\\n\".join(text_parts)\n",
    "    \n",
    "    def vectorize_schemas(self, schemas: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"스키마 정보를 벡터화\"\"\"\n",
    "        if not self.embedding_model:\n",
    "            print(\"⚠️ 임베딩 모델이 없어 벡터화를 건너뜁니다.\")\n",
    "            return schemas\n",
    "        \n",
    "        vectorized_schemas = []\n",
    "        \n",
    "        for schema in schemas:\n",
    "            try:\n",
    "                # 텍스트 생성\n",
    "                schema_text = self.create_schema_text_for_embedding(schema)\n",
    "                \n",
    "                # 벡터화\n",
    "                embedding = self.embedding_model.embed_query(schema_text)\n",
    "                \n",
    "                # 벡터 정보 추가\n",
    "                schema['schema_text'] = schema_text\n",
    "                schema['embedding'] = embedding\n",
    "                schema['embedding_dimension'] = len(embedding)\n",
    "                \n",
    "                vectorized_schemas.append(schema)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ {schema['table_name']} 벡터화 실패: {str(e)}\")\n",
    "                vectorized_schemas.append(schema)  # 벡터 없이라도 추가\n",
    "        \n",
    "        print(f\"✅ 스키마 벡터화 완료: {len(vectorized_schemas)}개\")\n",
    "        return vectorized_schemas\n",
    "\n",
    "class NorthwindDataBuilder:\n",
    "    \"\"\"Northwind 샘플 데이터베이스 구축 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        self.schemas = {}\n",
    "        self.sample_data = {}\n",
    "        \n",
    "    def define_schemas(self):\n",
    "        \"\"\"테이블 스키마 정의\"\"\"\n",
    "        \n",
    "        # Categories (카테고리)\n",
    "        self.schemas['categories'] = StructType([\n",
    "            StructField(\"category_id\", IntegerType(), False),\n",
    "            StructField(\"category_name\", StringType(), False),\n",
    "            StructField(\"description\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Suppliers (공급업체)\n",
    "        self.schemas['suppliers'] = StructType([\n",
    "            StructField(\"supplier_id\", IntegerType(), False),\n",
    "            StructField(\"company_name\", StringType(), False),\n",
    "            StructField(\"contact_name\", StringType(), True),\n",
    "            StructField(\"contact_title\", StringType(), True),\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"country\", StringType(), True),\n",
    "            StructField(\"phone\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Products (상품)\n",
    "        self.schemas['products'] = StructType([\n",
    "            StructField(\"product_id\", IntegerType(), False),\n",
    "            StructField(\"product_name\", StringType(), False),\n",
    "            StructField(\"supplier_id\", IntegerType(), True),\n",
    "            StructField(\"category_id\", IntegerType(), True),\n",
    "            StructField(\"unit_price\", DoubleType(), True),\n",
    "            StructField(\"units_in_stock\", IntegerType(), True),\n",
    "            StructField(\"units_on_order\", IntegerType(), True),\n",
    "            StructField(\"discontinued\", BooleanType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Customers (고객)\n",
    "        self.schemas['customers'] = StructType([\n",
    "            StructField(\"customer_id\", StringType(), False),\n",
    "            StructField(\"company_name\", StringType(), False),\n",
    "            StructField(\"contact_name\", StringType(), True),\n",
    "            StructField(\"contact_title\", StringType(), True),\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"country\", StringType(), True),\n",
    "            StructField(\"phone\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Employees (직원)\n",
    "        self.schemas['employees'] = StructType([\n",
    "            StructField(\"employee_id\", IntegerType(), False),\n",
    "            StructField(\"first_name\", StringType(), False),\n",
    "            StructField(\"last_name\", StringType(), False),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"birth_date\", DateType(), True),\n",
    "            StructField(\"hire_date\", DateType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"country\", StringType(), True),\n",
    "            StructField(\"reports_to\", IntegerType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Shippers (배송업체)\n",
    "        self.schemas['shippers'] = StructType([\n",
    "            StructField(\"shipper_id\", IntegerType(), False),\n",
    "            StructField(\"company_name\", StringType(), False),\n",
    "            StructField(\"phone\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Orders (주문)\n",
    "        self.schemas['orders'] = StructType([\n",
    "            StructField(\"order_id\", IntegerType(), False),\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"employee_id\", IntegerType(), True),\n",
    "            StructField(\"order_date\", DateType(), True),\n",
    "            StructField(\"required_date\", DateType(), True),\n",
    "            StructField(\"shipped_date\", DateType(), True),\n",
    "            StructField(\"ship_via\", IntegerType(), True),\n",
    "            StructField(\"freight\", DoubleType(), True),\n",
    "            StructField(\"ship_name\", StringType(), True),\n",
    "            StructField(\"ship_address\", StringType(), True),\n",
    "            StructField(\"ship_city\", StringType(), True),\n",
    "            StructField(\"ship_country\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Order Details (주문 상세)\n",
    "        self.schemas['order_details'] = StructType([\n",
    "            StructField(\"order_id\", IntegerType(), False),\n",
    "            StructField(\"product_id\", IntegerType(), False),\n",
    "            StructField(\"unit_price\", DoubleType(), False),\n",
    "            StructField(\"quantity\", IntegerType(), False),\n",
    "            StructField(\"discount\", DoubleType(), True)\n",
    "        ])\n",
    "        \n",
    "        print(\"✅ 테이블 스키마 정의 완료\")\n",
    "        print(f\"   정의된 테이블: {list(self.schemas.keys())}\")\n",
    "        \n",
    "    def generate_sample_data(self):\n",
    "        \"\"\"샘플 데이터 생성\"\"\"\n",
    "        \n",
    "        # Categories\n",
    "        self.sample_data['categories'] = [\n",
    "            (1, \"음료\", \"청량음료, 커피, 차, 맥주, 에일\"),\n",
    "            (2, \"조미료\", \"달콤하고 맛있는 소스, 양념, 조미료, 향신료\"),\n",
    "            (3, \"과자류\", \"디저트, 사탕, 달콤한 빵\"),\n",
    "            (4, \"유제품\", \"치즈\"),\n",
    "            (5, \"곡물/시리얼\", \"빵, 크래커, 파스타, 시리얼\"),\n",
    "            (6, \"육류/가금류\", \"조리된 육류\"),\n",
    "            (7, \"농산물\", \"건조 과일과 콩 두부\"),\n",
    "            (8, \"해산물\", \"해초와 생선\")\n",
    "        ]\n",
    "        \n",
    "        # Suppliers\n",
    "        self.sample_data['suppliers'] = [\n",
    "            (1, \"삼성식품\", \"김삼성\", \"영업 담당자\", \"서울시 강남구\", \"서울\", \"한국\", \"02-123-4567\"),\n",
    "            (2, \"LG무역\", \"이엘지\", \"구매 관리자\", \"부산시 해운대구\", \"부산\", \"한국\", \"051-234-5678\"),\n",
    "            (3, \"현대물산\", \"박현대\", \"마케팅 이사\", \"인천시 남동구\", \"인천\", \"한국\", \"032-345-6789\"),\n",
    "            (4, \"롯데상사\", \"최롯데\", \"영업 이사\", \"대구시 중구\", \"대구\", \"한국\", \"053-456-7890\"),\n",
    "            (5, \"SK케미칼\", \"정에스케이\", \"품질 관리자\", \"광주시 서구\", \"광주\", \"한국\", \"062-567-8901\")\n",
    "        ]\n",
    "        \n",
    "        # Products\n",
    "        self.sample_data['products'] = [\n",
    "            (1, \"김치\", 1, 2, 15000.0, 100, 0, False),\n",
    "            (2, \"비빔밥\", 2, 5, 12000.0, 50, 10, False),\n",
    "            (3, \"불고기\", 3, 6, 25000.0, 30, 5, False),\n",
    "            (4, \"된장찌개\", 1, 2, 8000.0, 80, 20, False),\n",
    "            (5, \"갈비탕\", 3, 6, 18000.0, 25, 0, False),\n",
    "            (6, \"냉면\", 2, 5, 10000.0, 60, 15, False),\n",
    "            (7, \"삼겹살\", 4, 6, 22000.0, 40, 8, False),\n",
    "            (8, \"해물파전\", 5, 8, 16000.0, 35, 12, False),\n",
    "            (9, \"떡볶이\", 1, 3, 7000.0, 90, 25, False),\n",
    "            (10, \"순두부찌개\", 2, 4, 9000.0, 70, 18, False)\n",
    "        ]\n",
    "        \n",
    "        # Customers  \n",
    "        self.sample_data['customers'] = [\n",
    "            (\"KOREA\", \"한국식당\", \"김한국\", \"사장\", \"서울시 종로구\", \"서울\", \"한국\", \"02-111-2222\"),\n",
    "            (\"JAPAN\", \"일본레스토랑\", \"사토 다카시\", \"매니저\", \"도쿄도 시부야구\", \"도쿄\", \"일본\", \"+81-3-1234\"),\n",
    "            (\"CHINA\", \"중국반점\", \"왕중국\", \"사장\", \"베이징시 차오양구\", \"베이징\", \"중국\", \"+86-10-5678\"),\n",
    "            (\"USA\", \"코리안 BBQ\", \"John Kim\", \"Owner\", \"LA California\", \"로스앤젤레스\", \"미국\", \"+1-213-9999\"),\n",
    "            (\"CANAD\", \"한식당\", \"이민수\", \"사장\", \"토론토 온타리오\", \"토론토\", \"캐나다\", \"+1-416-8888\")\n",
    "        ]\n",
    "        \n",
    "        # Employees\n",
    "        today = date.today()\n",
    "        self.sample_data['employees'] = [\n",
    "            (1, \"김\", \"사장\", \"CEO\", date(1970, 1, 1), date(2020, 1, 1), \"서울\", \"한국\", None),\n",
    "            (2, \"이\", \"부장\", \"영업부장\", date(1975, 6, 15), date(2020, 3, 1), \"서울\", \"한국\", 1),\n",
    "            (3, \"박\", \"과장\", \"마케팅과장\", date(1980, 3, 20), date(2021, 1, 15), \"부산\", \"한국\", 2),\n",
    "            (4, \"최\", \"대리\", \"영업대리\", date(1985, 9, 10), date(2022, 6, 1), \"대구\", \"한국\", 2),\n",
    "            (5, \"정\", \"사원\", \"영업사원\", date(1990, 12, 5), date(2023, 2, 1), \"광주\", \"한국\", 4)\n",
    "        ]\n",
    "        \n",
    "        # Shippers\n",
    "        self.sample_data['shippers'] = [\n",
    "            (1, \"한진택배\", \"1588-0011\"),\n",
    "            (2, \"CJ대한통운\", \"1588-1255\"),\n",
    "            (3, \"롯데택배\", \"1588-2121\")\n",
    "        ]\n",
    "        \n",
    "        # Orders (최근 6개월 데이터)\n",
    "        base_date = today - timedelta(days=180)\n",
    "        self.sample_data['orders'] = []\n",
    "        for i in range(1, 101):  # 100개 주문\n",
    "            order_date = base_date + timedelta(days=np.random.randint(0, 180))\n",
    "            required_date = order_date + timedelta(days=np.random.randint(3, 14))\n",
    "            shipped_date = order_date + timedelta(days=np.random.randint(1, 10)) if np.random.random() > 0.1 else None\n",
    "            \n",
    "            customer_id = np.random.choice([\"KOREA\", \"JAPAN\", \"CHINA\", \"USA\", \"CANAD\"])\n",
    "            employee_id = np.random.randint(1, 6)\n",
    "            shipper_id = np.random.randint(1, 4)\n",
    "            \n",
    "            self.sample_data['orders'].append((\n",
    "                i, customer_id, employee_id, order_date, required_date, shipped_date,\n",
    "                shipper_id, np.round(np.random.uniform(5000, 50000), 2),\n",
    "                f\"배송지_{i}\", f\"주소_{i}\", f\"도시_{i}\", \"한국\"\n",
    "            ))\n",
    "        \n",
    "        # Order Details\n",
    "        self.sample_data['order_details'] = []\n",
    "        for order_id in range(1, 101):\n",
    "            # 각 주문당 1-5개 상품\n",
    "            num_products = np.random.randint(1, 6)\n",
    "            products = np.random.choice(range(1, 11), num_products, replace=False)\n",
    "            \n",
    "            for product_id in products:\n",
    "                # 상품 가격 가져오기\n",
    "                product_price = next(p[4] for p in self.sample_data['products'] if p[0] == product_id)\n",
    "                quantity = np.random.randint(1, 10)\n",
    "                discount = np.round(np.random.choice([0.0, 0.05, 0.1, 0.15], p=[0.7, 0.15, 0.1, 0.05]), 2)\n",
    "                \n",
    "                self.sample_data['order_details'].append((\n",
    "                    order_id, int(product_id), product_price, quantity, discount\n",
    "                ))\n",
    "        \n",
    "        print(\"✅ 샘플 데이터 생성 완료\")\n",
    "        for table, data in self.sample_data.items():\n",
    "            print(f\"   {table}: {len(data)}개 레코드\")\n",
    "\n",
    "# 스키마 분석기 초기화 (embedding_model은 선택사항)\n",
    "if spark:\n",
    "    # 임베딩 모델 없이 초기화 (기본 스키마 생성용)\n",
    "    schema_analyzer = DatabaseSchemaAnalyzer(spark, embedding_model=None)\n",
    "    print(\"✅ 데이터베이스 스키마 분석기 초기화 완료\")\n",
    "\n",
    "    # Northwind 데이터 빌더 초기화 및 샘플 데이터 생성\n",
    "    northwind_builder = NorthwindDataBuilder(spark)\n",
    "    northwind_builder.define_schemas()\n",
    "    northwind_builder.generate_sample_data()\n",
    "    print(\"\\n🏗️ Northwind 데이터 빌더 준비 완료!\")\n",
    "else:\n",
    "    schema_analyzer = None\n",
    "    print(\"⚠️ Spark 세션이 없어 스키마 분석기를 초기화할 수 없습니다.\")\n",
    "\n",
    "# Delta Lake 테이블 생성 및 샘플 데이터 로드\n",
    "\n",
    "print(\"📦 Delta Lake 테이블 생성 및 데이터 로드\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. 기존 테이블 삭제 (초기화)\n",
    "print(\"\\n🔄 기존 테이블 정리 중...\")\n",
    "for table_name in northwind_builder.schemas.keys():\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS northwind.{table_name}\")\n",
    "        print(f\"   🗑️ {table_name} 테이블 삭제됨\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ {table_name} 삭제 중 오류: {e}\")\n",
    "\n",
    "# 2. 샘플 데이터 정의\n",
    "sample_data = {\n",
    "    \"categories\": [\n",
    "        (1, \"Beverages\", \"Soft drinks, coffees, teas, beers, and ales\"),\n",
    "        (2, \"Condiments\", \"Sweet and savory sauces, relishes, spreads, and seasonings\"),\n",
    "        (3, \"Dairy Products\", \"Cheeses\"),\n",
    "        (4, \"Grains/Cereals\", \"Breads, crackers, pasta, and cereal\"),\n",
    "        (5, \"Meat/Poultry\", \"Prepared meats\"),\n",
    "        (6, \"Produce\", \"Dried fruit and bean curd\"),\n",
    "        (7, \"Seafood\", \"Seaweed and fish\"),\n",
    "        (8, \"Confections\", \"Desserts, candies, and sweet breads\")\n",
    "    ],\n",
    "    \n",
    "    \"suppliers\": [\n",
    "        (1, \"Exotic Liquids\", \"Charlotte Cooper\", \"Purchasing Manager\", \"49 Gilbert St.\", \"London\", None, \"EC1 4SD\", \"UK\", \"(171) 555-2222\", None, None),\n",
    "        (2, \"New Orleans Cajun Delights\", \"Shelley Burke\", \"Order Administrator\", \"P.O. Box 78934\", \"New Orleans\", \"LA\", \"70117\", \"USA\", \"(100) 555-4822\", None, \"#CAJUN.HTM#\"),\n",
    "        (3, \"Grandma Kelly's Homestead\", \"Regina Murphy\", \"Sales Representative\", \"707 Oxford Rd.\", \"Ann Arbor\", \"MI\", \"48104\", \"USA\", \"(313) 555-5735\", \"(313) 555-3349\", None),\n",
    "        (4, \"Tokyo Traders\", \"Yoshi Nagase\", \"Marketing Manager\", \"9-8 Sekimai Musashino-shi\", \"Tokyo\", None, \"100\", \"Japan\", \"(03) 3555-5011\", None, None),\n",
    "        (5, \"Cooperativa de Quesos 'Las Cabras'\", \"Antonio del Valle Saavedra\", \"Export Administrator\", \"Calle del Rosal 4\", \"Oviedo\", \"Asturias\", \"33007\", \"Spain\", \"(98) 598 76 98\", None, None)\n",
    "    ],\n",
    "    \n",
    "    \"products\": [\n",
    "        (1, \"Chai\", 1, 1, \"10 boxes x 20 bags\", 18.00, 39, 0, 10, False),\n",
    "        (2, \"Chang\", 1, 1, \"24 - 12 oz bottles\", 19.00, 17, 40, 25, False),\n",
    "        (3, \"Aniseed Syrup\", 1, 2, \"12 - 550 ml bottles\", 10.00, 13, 70, 25, False),\n",
    "        (4, \"Chef Anton's Cajun Seasoning\", 2, 2, \"48 - 6 oz jars\", 22.00, 53, 0, 0, False),\n",
    "        (5, \"Chef Anton's Gumbo Mix\", 2, 2, \"36 boxes\", 21.35, 0, 0, 0, True),\n",
    "        (6, \"Grandma's Boysenberry Spread\", 3, 2, \"12 - 8 oz jars\", 25.00, 120, 0, 25, False),\n",
    "        (7, \"Uncle Bob's Organic Dried Pears\", 3, 7, \"12 - 1 lb pkgs.\", 30.00, 15, 0, 10, False),\n",
    "        (8, \"Northwoods Cranberry Sauce\", 3, 2, \"12 - 12 oz jars\", 40.00, 6, 0, 0, False),\n",
    "        (9, \"Mishi Kobe Niku\", 4, 6, \"18 - 500 g pkgs.\", 97.00, 29, 0, 0, True),\n",
    "        (10, \"Ikura\", 4, 8, \"12 - 200 ml jars\", 31.00, 31, 0, 0, False)\n",
    "    ],\n",
    "    \n",
    "    \"customers\": [\n",
    "        (\"ALFKI\", \"Alfreds Futterkiste\", \"Maria Anders\", \"Sales Representative\", \"Obere Str. 57\", \"Berlin\", None, \"12209\", \"Germany\", \"030-0074321\", \"030-0076545\"),\n",
    "        (\"ANATR\", \"Ana Trujillo Emparedados y helados\", \"Ana Trujillo\", \"Owner\", \"Avda. de la Constitución 2222\", \"México D.F.\", None, \"05021\", \"Mexico\", \"(5) 555-4729\", \"(5) 555-3745\"),\n",
    "        (\"ANTON\", \"Antonio Moreno Taquería\", \"Antonio Moreno\", \"Owner\", \"Mataderos 2312\", \"México D.F.\", None, \"05023\", \"Mexico\", \"(5) 555-3932\", None),\n",
    "        (\"AROUT\", \"Around the Horn\", \"Thomas Hardy\", \"Sales Representative\", \"120 Hanover Sq.\", \"London\", None, \"WA1 1DP\", \"UK\", \"(171) 555-7788\", \"(171) 555-6750\"),\n",
    "        (\"BERGS\", \"Berglunds snabbköp\", \"Christina Berglund\", \"Order Administrator\", \"Berguvsvägen 8\", \"Luleå\", None, \"S-958 22\", \"Sweden\", \"0921-12 34 65\", \"0921-12 34 67\"),\n",
    "        (\"BLAUS\", \"Blauer See Delikatessen\", \"Hanna Moos\", \"Sales Representative\", \"Forsterstr. 57\", \"Mannheim\", None, \"68306\", \"Germany\", \"0621-08460\", \"0621-08924\"),\n",
    "        (\"BLONP\", \"Blondesddsl père et fils\", \"Frédérique Citeaux\", \"Marketing Manager\", \"24, place Kléber\", \"Strasbourg\", None, \"67000\", \"France\", \"88.60.15.31\", \"88.60.15.32\"),\n",
    "        (\"BOLID\", \"Bólido Comidas preparadas\", \"Martín Sommer\", \"Owner\", \"C/ Araquil, 67\", \"Madrid\", None, \"28023\", \"Spain\", \"(91) 555 22 82\", \"(91) 555 91 99\"),\n",
    "        (\"BONAP\", \"Bon app'\", \"Laurence Lebihan\", \"Owner\", \"12, rue des Bouchers\", \"Marseille\", None, \"13008\", \"France\", \"91.24.45.40\", \"91.24.45.41\"),\n",
    "        (\"BOTTM\", \"Bottom-Dollar Markets\", \"Elizabeth Lincoln\", \"Accounting Manager\", \"23 Tsawassen Blvd.\", \"Tsawassen\", \"BC\", \"T2F 8M4\", \"Canada\", \"(604) 555-4729\", \"(604) 555-3745\")\n",
    "    ],\n",
    "    \n",
    "    \"employees\": [\n",
    "        (1, \"Davolio\", \"Nancy\", \"Sales Representative\", \"Ms.\", date(1948, 12, 8), date(1992, 5, 1), \"507 - 20th Ave. E. Apt. 2A\", \"Seattle\", \"WA\", \"98122\", \"USA\", \"(206) 555-9857\", \"5467\", \"Education includes a BA in psychology from Colorado State University in 1970.\", None),\n",
    "        (2, \"Fuller\", \"Andrew\", \"Vice President, Sales\", \"Dr.\", date(1952, 2, 19), date(1992, 8, 14), \"908 W. Capital Way\", \"Tacoma\", \"WA\", \"98401\", \"USA\", \"(206) 555-9482\", \"3457\", \"Andrew received his BTS commercial in 1974 and a Ph.D. in international marketing from the University of Dallas in 1981.\", None),\n",
    "        (3, \"Leverling\", \"Janet\", \"Sales Representative\", \"Ms.\", date(1963, 8, 30), date(1992, 4, 1), \"722 Moss Bay Blvd.\", \"Kirkland\", \"WA\", \"98033\", \"USA\", \"(206) 555-3412\", \"3355\", \"Janet has a BS degree in chemistry from Boston College (1984).\", 2),\n",
    "        (4, \"Peacock\", \"Margaret\", \"Sales Representative\", \"Mrs.\", date(1937, 9, 19), date(1993, 5, 3), \"4110 Old Redmond Rd.\", \"Redmond\", \"WA\", \"98052\", \"USA\", \"(206) 555-8122\", \"5176\", \"Margaret holds a BA in English literature from Concordia College (1958) and an MA from the American Institute of Culinary Arts (1966).\", 2),\n",
    "        (5, \"Buchanan\", \"Steven\", \"Sales Manager\", \"Mr.\", date(1955, 3, 4), date(1993, 10, 17), \"14 Garrett Hill\", \"London\", None, \"SW1 8JR\", \"UK\", \"(71) 555-4848\", \"3453\", \"Steven Buchanan graduated from St. Andrews University, Scotland, with a BSC degree in 1976.\", 2)\n",
    "    ],\n",
    "    \n",
    "    \"shippers\": [\n",
    "        (1, \"Speedy Express\", \"(503) 555-9831\"),\n",
    "        (2, \"United Package\", \"(503) 555-3199\"),\n",
    "        (3, \"Federal Shipping\", \"(503) 555-9931\")\n",
    "    ],\n",
    "    \n",
    "    \"orders\": [\n",
    "        (10248, \"VINET\", 5, date(1996, 7, 4), date(1996, 8, 1), date(1996, 7, 16), 3, 32.38, \"Vins et alcools Chevalier\", \"59 rue de l'Abbaye\", \"Reims\", None, \"51100\", \"France\"),\n",
    "        (10249, \"TOMSP\", 6, date(1996, 7, 5), date(1996, 8, 16), date(1996, 7, 10), 1, 11.61, \"Toms Spezialitäten\", \"Luisenstr. 48\", \"Münster\", None, \"44087\", \"Germany\"),\n",
    "        (10250, \"HANAR\", 4, date(1996, 7, 8), date(1996, 8, 5), date(1996, 7, 12), 2, 65.83, \"Hanari Carnes\", \"Rua do Paço, 67\", \"Rio de Janeiro\", \"RJ\", \"05454-876\", \"Brazil\"),\n",
    "        (10251, \"VICTE\", 3, date(1996, 7, 8), date(1996, 8, 5), date(1996, 7, 15), 1, 41.34, \"Victuailles en stock\", \"2, rue du Commerce\", \"Lyon\", None, \"69004\", \"France\"),\n",
    "        (10252, \"SUPRD\", 4, date(1996, 7, 9), date(1996, 8, 6), date(1996, 7, 11), 2, 51.30, \"Suprêmes délices\", \"Boulevard Tirou, 255\", \"Charleroi\", None, \"B-6000\", \"Belgium\"),\n",
    "        (10253, \"HANAR\", 3, date(1996, 7, 10), date(1996, 7, 24), date(1996, 7, 16), 2, 58.17, \"Hanari Carnes\", \"Rua do Paço, 67\", \"Rio de Janeiro\", \"RJ\", \"05454-876\", \"Brazil\"),\n",
    "        (10254, \"CHOPS\", 5, date(1996, 7, 11), date(1996, 8, 8), date(1996, 7, 23), 2, 22.98, \"Chop-suey Chinese\", \"Hauptstr. 31\", \"Bern\", None, \"3012\", \"Switzerland\"),\n",
    "        (10255, \"RICSU\", 9, date(1996, 7, 12), date(1996, 8, 9), date(1996, 7, 15), 3, 148.33, \"Richter Supermarkt\", \"Starenweg 5\", \"Genève\", None, \"1204\", \"Switzerland\"),\n",
    "        (10256, \"WELLI\", 3, date(1996, 7, 15), date(1996, 8, 12), date(1996, 7, 17), 2, 13.97, \"Wellington Importadora\", \"Rua do Mercado, 12\", \"Resende\", \"SP\", \"08737-363\", \"Brazil\"),\n",
    "        (10257, \"HILAA\", 4, date(1996, 7, 16), date(1996, 8, 13), date(1996, 7, 22), 3, 81.91, \"HILARION-Abastos\", \"Carrera 22 con Ave. Carlos Soublette #8-35\", \"San Cristóbal\", \"Táchira\", \"5022\", \"Venezuela\")\n",
    "    ],\n",
    "    \n",
    "    \"order_details\": [\n",
    "        (10248, 11, 14.00, 12, 0.00),\n",
    "        (10248, 42, 9.80, 10, 0.00),\n",
    "        (10248, 72, 34.80, 5, 0.00),\n",
    "        (10249, 14, 18.60, 9, 0.00),\n",
    "        (10249, 51, 42.40, 40, 0.00),\n",
    "        (10250, 41, 7.70, 10, 0.00),\n",
    "        (10250, 51, 42.40, 35, 0.15),\n",
    "        (10250, 65, 16.80, 15, 0.15),\n",
    "        (10251, 22, 16.80, 6, 0.05),\n",
    "        (10251, 57, 15.60, 15, 0.05),\n",
    "        (10251, 65, 16.80, 20, 0.00),\n",
    "        (10252, 20, 64.80, 40, 0.05),\n",
    "        (10252, 33, 2.00, 25, 0.05),\n",
    "        (10252, 60, 27.20, 40, 0.00),\n",
    "        (10253, 31, 10.00, 20, 0.00),\n",
    "        (10253, 39, 14.40, 42, 0.00),\n",
    "        (10253, 49, 16.00, 40, 0.00)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\n📊 샘플 데이터 정의 완료:\")\n",
    "for table_name, data in sample_data.items():\n",
    "    print(f\"   📈 {table_name}: {len(data)}개 레코드\")\n",
    "\n",
    "print(\"\\n✅ 샘플 데이터 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d014cd3",
   "metadata": {},
   "source": [
    "## 4. 테이블 생성 및 데이터 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d651501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG 스키마 검색 및 SQL 생성기 정의 완료\n",
      "🚀 Northwind 데이터베이스 구축 시작...\n",
      "🔧 Delta Lake 테이블 생성 시작...\n",
      "\n",
      "📋 categories 테이블 생성 중...\n",
      "   ✅ categories: 8개 레코드 로드\n",
      "\n",
      "📋 suppliers 테이블 생성 중...\n",
      "   ✅ categories: 8개 레코드 로드\n",
      "\n",
      "📋 suppliers 테이블 생성 중...\n",
      "   ✅ suppliers: 5개 레코드 로드\n",
      "\n",
      "📋 products 테이블 생성 중...\n",
      "   ✅ suppliers: 5개 레코드 로드\n",
      "\n",
      "📋 products 테이블 생성 중...\n",
      "   ✅ products: 10개 레코드 로드\n",
      "\n",
      "📋 customers 테이블 생성 중...\n",
      "   ✅ products: 10개 레코드 로드\n",
      "\n",
      "📋 customers 테이블 생성 중...\n",
      "   ✅ customers: 5개 레코드 로드\n",
      "\n",
      "📋 employees 테이블 생성 중...\n",
      "   ✅ customers: 5개 레코드 로드\n",
      "\n",
      "📋 employees 테이블 생성 중...\n",
      "   ✅ employees: 5개 레코드 로드\n",
      "\n",
      "📋 shippers 테이블 생성 중...\n",
      "   ✅ employees: 5개 레코드 로드\n",
      "\n",
      "📋 shippers 테이블 생성 중...\n",
      "   ✅ shippers: 3개 레코드 로드\n",
      "\n",
      "📋 orders 테이블 생성 중...\n",
      "   ✅ shippers: 3개 레코드 로드\n",
      "\n",
      "📋 orders 테이블 생성 중...\n",
      "   ✅ orders: 100개 레코드 로드\n",
      "\n",
      "📋 order_details 테이블 생성 중...\n",
      "   ✅ orders: 100개 레코드 로드\n",
      "\n",
      "📋 order_details 테이블 생성 중...\n",
      "   ✅ order_details: 290개 레코드 로드\n",
      "\n",
      "📈 테이블 생성 결과:\n",
      "   ✅ 성공: 8개 테이블\n",
      "   ❌ 실패: 0개 테이블\n",
      "\n",
      "✅ 생성된 테이블:\n",
      "   📂 northwind.categories\n",
      "   📂 northwind.suppliers\n",
      "   📂 northwind.products\n",
      "   📂 northwind.customers\n",
      "   📂 northwind.employees\n",
      "   📂 northwind.shippers\n",
      "   📂 northwind.orders\n",
      "   📂 northwind.order_details\n",
      "\n",
      "🎉 총 8개 Northwind 테이블이 Delta Lake에 생성되었습니다!\n",
      "❌ 테이블 생성 실패\n",
      "   ✅ order_details: 290개 레코드 로드\n",
      "\n",
      "📈 테이블 생성 결과:\n",
      "   ✅ 성공: 8개 테이블\n",
      "   ❌ 실패: 0개 테이블\n",
      "\n",
      "✅ 생성된 테이블:\n",
      "   📂 northwind.categories\n",
      "   📂 northwind.suppliers\n",
      "   📂 northwind.products\n",
      "   📂 northwind.customers\n",
      "   📂 northwind.employees\n",
      "   📂 northwind.shippers\n",
      "   📂 northwind.orders\n",
      "   📂 northwind.order_details\n",
      "\n",
      "🎉 총 8개 Northwind 테이블이 Delta Lake에 생성되었습니다!\n",
      "❌ 테이블 생성 실패\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class RAGSchemaRetriever:\n",
    "    \"\"\"RAG 기반 스키마 검색 및 매칭\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorized_schemas: List[Dict[str, Any]], embedding_model=None):\n",
    "        self.vectorized_schemas = vectorized_schemas\n",
    "        self.embedding_model = embedding_model\n",
    "        self.schema_embeddings = None\n",
    "        self.schema_texts = None\n",
    "        \n",
    "        # 임베딩 매트릭스 구성\n",
    "        self._build_embedding_matrix()\n",
    "    \n",
    "    def _build_embedding_matrix(self):\n",
    "        \"\"\"스키마 임베딩 매트릭스 구성\"\"\"\n",
    "        embeddings = []\n",
    "        texts = []\n",
    "        \n",
    "        for schema in self.vectorized_schemas:\n",
    "            if 'embedding' in schema:\n",
    "                embeddings.append(schema['embedding'])\n",
    "                texts.append(schema.get('schema_text', ''))\n",
    "            \n",
    "        if embeddings:\n",
    "            self.schema_embeddings = np.array(embeddings)\n",
    "            self.schema_texts = texts\n",
    "            print(f\"✅ 임베딩 매트릭스 구성 완료: {self.schema_embeddings.shape}\")\n",
    "        else:\n",
    "            print(\"⚠️ 사용 가능한 임베딩이 없습니다.\")\n",
    "    \n",
    "    def search_relevant_schemas(self, question: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"질문과 관련된 스키마 검색\"\"\"\n",
    "        if not self.embedding_model or self.schema_embeddings is None:\n",
    "            print(\"⚠️ 임베딩 기반 검색을 사용할 수 없습니다. 키워드 검색으로 대체합니다.\")\n",
    "            return self._keyword_based_search(question, top_k)\n",
    "        \n",
    "        try:\n",
    "            # 질문 임베딩\n",
    "            question_embedding = self.embedding_model.embed_query(question)\n",
    "            question_vector = np.array(question_embedding).reshape(1, -1)\n",
    "            \n",
    "            # 코사인 유사도 계산\n",
    "            similarities = cosine_similarity(question_vector, self.schema_embeddings)[0]\n",
    "            \n",
    "            # 상위 k개 인덱스\n",
    "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "            \n",
    "            # 결과 구성\n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                schema = self.vectorized_schemas[idx].copy()\n",
    "                schema['similarity_score'] = float(similarities[idx])\n",
    "                results.append(schema)\n",
    "            \n",
    "            print(f\"🔍 벡터 검색 완료: {len(results)}개 스키마 매칭\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 벡터 검색 실패: {str(e)}\")\n",
    "            return self._keyword_based_search(question, top_k)\n",
    "    \n",
    "    def _keyword_based_search(self, question: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"키워드 기반 스키마 검색 (백업 방법)\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # 키워드 매칭 스코어 계산\n",
    "        scored_schemas = []\n",
    "        \n",
    "        for schema in self.vectorized_schemas:\n",
    "            score = 0\n",
    "            \n",
    "            # 테이블명 매칭\n",
    "            if question_lower in schema['table_name'].lower():\n",
    "                score += 10\n",
    "            \n",
    "            # 컬럼명 매칭\n",
    "            for col in schema['columns']:\n",
    "                if question_lower in col['name'].lower():\n",
    "                    score += 5\n",
    "                if col.get('comment') and question_lower in col['comment'].lower():\n",
    "                    score += 3\n",
    "            \n",
    "            # 샘플 데이터 매칭\n",
    "            sample_text = str(schema.get('sample_data', '')).lower()\n",
    "            if question_lower in sample_text:\n",
    "                score += 2\n",
    "            \n",
    "            if score > 0:\n",
    "                schema_copy = schema.copy()\n",
    "                schema_copy['similarity_score'] = score\n",
    "                scored_schemas.append(schema_copy)\n",
    "        \n",
    "        # 스코어로 정렬\n",
    "        scored_schemas.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        \n",
    "        print(f\"🔍 키워드 검색 완료: {len(scored_schemas[:top_k])}개 스키마 매칭\")\n",
    "        return scored_schemas[:top_k]\n",
    "\n",
    "class TextToSQLGenerator:\n",
    "    \"\"\"Text-to-SQL 생성기\"\"\"\n",
    "    \n",
    "    def __init__(self, foundation_model, schema_retriever: RAGSchemaRetriever):\n",
    "        self.foundation_model = foundation_model\n",
    "        self.schema_retriever = schema_retriever\n",
    "        \n",
    "    def generate_sql(self, question: str, top_k_schemas: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"자연어 질문을 SQL로 변환\"\"\"\n",
    "        try:\n",
    "            # 1. 관련 스키마 검색\n",
    "            relevant_schemas = self.schema_retriever.search_relevant_schemas(question, top_k_schemas)\n",
    "            \n",
    "            if not relevant_schemas:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"관련된 테이블을 찾을 수 없습니다.\",\n",
    "                    \"sql_query\": None\n",
    "                }\n",
    "            \n",
    "            # 2. 프롬프트 생성\n",
    "            prompt = self._create_prompt(question, relevant_schemas)\n",
    "            \n",
    "            # 3. LLM으로 SQL 생성\n",
    "            if self.foundation_model:\n",
    "                response = self.foundation_model.predict(prompt)\n",
    "                sql_query = self._extract_sql_from_response(response)\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"언어 모델을 사용할 수 없습니다.\",\n",
    "                    \"sql_query\": None\n",
    "                }\n",
    "            \n",
    "            # 4. 결과 구성\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"sql_query\": sql_query,\n",
    "                \"relevant_schemas\": relevant_schemas,\n",
    "                \"raw_response\": response,\n",
    "                \"prompt_used\": prompt\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"SQL 생성 실패: {str(e)}\",\n",
    "                \"sql_query\": None\n",
    "            }\n",
    "    \n",
    "    def _create_prompt(self, question: str, schemas: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"RAG 기반 프롬프트 생성\"\"\"\n",
    "        schema_context = \"\"\n",
    "        \n",
    "        for i, schema in enumerate(schemas, 1):\n",
    "            schema_context += f\"\\n=== 테이블 {i}: {schema['full_name']} ===\\n\"\n",
    "            schema_context += f\"행 수: {schema.get('row_count', 0):,}\\n\"\n",
    "            schema_context += \"컬럼:\\n\"\n",
    "            \n",
    "            for col in schema['columns']:\n",
    "                comment = f\" -- {col['comment']}\" if col.get('comment') else \"\"\n",
    "                schema_context += f\"  - {col['name']} ({col['type']}){comment}\\n\"\n",
    "            \n",
    "            # 샘플 데이터 일부 포함\n",
    "            if schema.get('sample_data'):\n",
    "                schema_context += \"샘플 데이터 (일부):\\n\"\n",
    "                for sample in schema['sample_data'][:2]:\n",
    "                    # 중요한 필드만 표시\n",
    "                    key_fields = list(sample.keys())[:3]\n",
    "                    sample_str = {k: sample[k] for k in key_fields}\n",
    "                    schema_context += f\"  {sample_str}\\n\"\n",
    "            \n",
    "            if 'similarity_score' in schema:\n",
    "                schema_context += f\"매칭 점수: {schema['similarity_score']:.3f}\\n\"\n",
    "        \n",
    "        prompt = f\"\"\"당신은 Databricks 환경의 SQL 전문가입니다. 주어진 데이터베이스 스키마를 바탕으로 자연어 질문을 정확한 SQL 쿼리로 변환하세요.\n",
    "\n",
    "=== 데이터베이스 스키마 정보 ===\n",
    "{schema_context}\n",
    "\n",
    "=== 변환 규칙 ===\n",
    "1. 위에 제공된 테이블과 컬럼명만 사용하세요\n",
    "2. Databricks SQL 문법을 사용하세요\n",
    "3. 적절한 JOIN, WHERE, GROUP BY, ORDER BY를 활용하세요\n",
    "4. 결과는 실행 가능한 SQL 쿼리만 반환하세요\n",
    "5. 쿼리를 ```sql과 ``` 사이에 작성하세요\n",
    "\n",
    "=== 예시 ===\n",
    "질문: \"고객별 주문 건수를 보여주세요\"\n",
    "```sql\n",
    "SELECT customer_id, COUNT(*) as order_count\n",
    "FROM orders\n",
    "GROUP BY customer_id\n",
    "ORDER BY order_count DESC;\n",
    "```\n",
    "\n",
    "=== 질문 ===\n",
    "{question}\n",
    "\n",
    "=== 답변 ===\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _extract_sql_from_response(self, response: str) -> str:\n",
    "        \"\"\"LLM 응답에서 SQL 쿼리 추출\"\"\"\n",
    "        # ```sql과 ``` 사이의 내용 추출\n",
    "        sql_pattern = r'```sql\\s*(.*?)\\s*```'\n",
    "        matches = re.findall(sql_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if matches:\n",
    "            return matches[0].strip()\n",
    "        \n",
    "        # SQL 키워드로 시작하는 부분 찾기\n",
    "        lines = response.strip().split('\\n')\n",
    "        sql_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'WITH', 'CREATE']\n",
    "        \n",
    "        for line in lines:\n",
    "            line_upper = line.strip().upper()\n",
    "            if any(line_upper.startswith(keyword) for keyword in sql_keywords):\n",
    "                return line.strip()\n",
    "        \n",
    "        return response.strip()\n",
    "\n",
    "print(\"✅ RAG 스키마 검색 및 SQL 생성기 정의 완료\")\n",
    "\n",
    "def create_delta_tables():\n",
    "    \"\"\"Delta Lake 테이블 생성 및 데이터 로드\"\"\"\n",
    "    \n",
    "    if not spark or not northwind_builder:\n",
    "        print(\"❌ Spark 세션 또는 데이터 빌더가 필요합니다.\")\n",
    "        return False\n",
    "    \n",
    "    created_tables = []\n",
    "    failed_tables = []\n",
    "    \n",
    "    try:\n",
    "        print(\"🔧 Delta Lake 테이블 생성 시작...\")\n",
    "        \n",
    "        # 각 테이블별로 생성\n",
    "        for table_name, schema in northwind_builder.schemas.items():\n",
    "            try:\n",
    "                print(f\"\\n📋 {table_name} 테이블 생성 중...\")\n",
    "                \n",
    "                # 데이터프레임 생성\n",
    "                data = northwind_builder.sample_data[table_name]\n",
    "                df = spark.createDataFrame(data, schema)\n",
    "                \n",
    "                # Delta 테이블로 저장 (덮어쓰기)\n",
    "                df.write \\\n",
    "                  .format(\"delta\") \\\n",
    "                  .mode(\"overwrite\") \\\n",
    "                  .option(\"mergeSchema\", \"true\") \\\n",
    "                  .saveAsTable(f\"northwind.{table_name}\")\n",
    "                \n",
    "                # 데이터 확인\n",
    "                count = spark.table(f\"northwind.{table_name}\").count()\n",
    "                print(f\"   ✅ {table_name}: {count}개 레코드 로드\")\n",
    "                \n",
    "                created_tables.append(table_name)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ {table_name} 테이블 생성/삽입 실패: {e}\")\n",
    "                failed_tables.append((table_name, str(e)))\n",
    "\n",
    "        print(f\"\\n📈 테이블 생성 결과:\")\n",
    "        print(f\"   ✅ 성공: {len(created_tables)}개 테이블\")\n",
    "        print(f\"   ❌ 실패: {len(failed_tables)}개 테이블\")\n",
    "\n",
    "        if created_tables:\n",
    "            print(f\"\\n✅ 생성된 테이블:\")\n",
    "            for table in created_tables:\n",
    "                print(f\"   📂 northwind.{table}\")\n",
    "\n",
    "        if failed_tables:\n",
    "            print(f\"\\n❌ 실패한 테이블:\")\n",
    "            for table, error in failed_tables:\n",
    "                print(f\"   📂 {table}: {error}\")\n",
    "\n",
    "        print(f\"\\n🎉 총 {len(created_tables)}개 Northwind 테이블이 Delta Lake에 생성되었습니다!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 테이블 생성 실패: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def verify_northwind_database():\n",
    "    \"\"\"Northwind 데이터베이스 검증\"\"\"\n",
    "    \n",
    "    if not spark:\n",
    "        print(\"❌ Spark 세션이 필요합니다.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(\"🔍 Northwind 데이터베이스 검증 중...\")\n",
    "        \n",
    "        # 테이블 목록 확인\n",
    "        tables = spark.sql(\"SHOW TABLES IN northwind\").collect()\n",
    "        table_names = [row.tableName for row in tables]\n",
    "        \n",
    "        print(f\"\\n📊 생성된 테이블 ({len(table_names)}개):\")\n",
    "        \n",
    "        verification_results = {}\n",
    "        \n",
    "        for table_name in sorted(table_names):\n",
    "            # 테이블 정보 조회\n",
    "            df = spark.table(f\"northwind.{table_name}\")\n",
    "            count = df.count()\n",
    "            columns = len(df.columns)\n",
    "            \n",
    "            print(f\"   📋 {table_name}:\")\n",
    "            print(f\"      - 레코드 수: {count:,}\")\n",
    "            print(f\"      - 컬럼 수: {columns}\")\n",
    "            \n",
    "            # 샘플 데이터 미리보기\n",
    "            sample = df.limit(3).toPandas()\n",
    "            print(f\"      - 샘플: {sample.iloc[0].to_dict() if len(sample) > 0 else 'No data'}\")\n",
    "            \n",
    "            verification_results[table_name] = {\n",
    "                \"record_count\": count,\n",
    "                \"column_count\": columns,\n",
    "                \"status\": \"OK\" if count > 0 else \"EMPTY\"\n",
    "            }\n",
    "        \n",
    "        # 관계 검증 (기본적인 JOIN 테스트)\n",
    "        print(f\"\\n🔗 관계 검증:\")\n",
    "        \n",
    "        # 주문-고객 관계\n",
    "        join_test = spark.sql(\"\"\"\n",
    "            SELECT COUNT(*) as order_count \n",
    "            FROM northwind.orders o \n",
    "            INNER JOIN northwind.customers c ON o.customer_id = c.customer_id\n",
    "        \"\"\").collect()[0].order_count\n",
    "        \n",
    "        print(f\"   ✅ 주문-고객 JOIN: {join_test}개 매칭\")\n",
    "        \n",
    "        # 주문상세-상품 관계  \n",
    "        join_test2 = spark.sql(\"\"\"\n",
    "            SELECT COUNT(*) as detail_count\n",
    "            FROM northwind.order_details od\n",
    "            INNER JOIN northwind.products p ON od.product_id = p.product_id\n",
    "        \"\"\").collect()[0].detail_count\n",
    "        \n",
    "        print(f\"   ✅ 주문상세-상품 JOIN: {join_test2}개 매칭\")\n",
    "        \n",
    "        print(f\"\\n🎯 검증 완료! Text-to-SQL 테스트 준비됨\")\n",
    "        return verification_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 검증 실패: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# 테이블 생성 실행\n",
    "if 'northwind_builder' in locals():\n",
    "    print(\"🚀 Northwind 데이터베이스 구축 시작...\")\n",
    "    \n",
    "    success = create_delta_tables()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        verification_results = verify_northwind_database()\n",
    "        \n",
    "        if verification_results:\n",
    "            print(\"\\n✅ Northwind 데이터베이스 구축 성공!\")\n",
    "            print(\"🔗 다음 노트북에서 LangChain Agent를 활용한 Text-to-SQL을 구현합니다.\")\n",
    "        else:\n",
    "            print(\"❌ 데이터베이스 검증 실패\")\n",
    "    else:\n",
    "        print(\"❌ 테이블 생성 실패\")\n",
    "else:\n",
    "    print(\"❌ 데이터 빌더가 초기화되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbfa738",
   "metadata": {},
   "source": [
    "## 5. 데이터 검증 및 기본 SQL 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2b8bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 기본 Text-to-SQL 프롬프트 템플릿 정의 완료\n",
      "✅ SQL 실행기 초기화 완료\n",
      "🧪 Northwind 데이터베이스 쿼리 테스트 시작...\n",
      "\n",
      "🔍 테스트 1: 전체 고객 수\n",
      "📝 설명: 등록된 고객의 총 개수\n",
      "✅ 실행 성공 (1개 행)\n",
      "📊 결과:\n",
      "   1. customer_count: 5\n",
      "--------------------------------------------------\n",
      "🔍 테스트 2: 상품 카테고리별 개수\n",
      "📝 설명: 각 카테고리별 상품 개수\n",
      "✅ 실행 성공 (1개 행)\n",
      "📊 결과:\n",
      "   1. customer_count: 5\n",
      "--------------------------------------------------\n",
      "🔍 테스트 2: 상품 카테고리별 개수\n",
      "📝 설명: 각 카테고리별 상품 개수\n",
      "✅ 실행 성공 (8개 행)\n",
      "📊 결과:\n",
      "   1. category_name: 육류/가금류, product_count: 3\n",
      "   2. category_name: 곡물/시리얼, product_count: 2\n",
      "   3. category_name: 조미료, product_count: 2\n",
      "   4. category_name: 과자류, product_count: 1\n",
      "   5. category_name: 해산물, product_count: 1\n",
      "   ... 그 외 3개 행\n",
      "--------------------------------------------------\n",
      "🔍 테스트 3: 월별 주문 현황\n",
      "📝 설명: 최근 월별 주문 건수 및 평균 배송비\n",
      "✅ 실행 성공 (8개 행)\n",
      "📊 결과:\n",
      "   1. category_name: 육류/가금류, product_count: 3\n",
      "   2. category_name: 곡물/시리얼, product_count: 2\n",
      "   3. category_name: 조미료, product_count: 2\n",
      "   4. category_name: 과자류, product_count: 1\n",
      "   5. category_name: 해산물, product_count: 1\n",
      "   ... 그 외 3개 행\n",
      "--------------------------------------------------\n",
      "🔍 테스트 3: 월별 주문 현황\n",
      "📝 설명: 최근 월별 주문 건수 및 평균 배송비\n",
      "✅ 실행 성공 (6개 행)\n",
      "📊 결과:\n",
      "   1. year: 2025.0, month: 6.0, order_count: 15.0, avg_freight: 27776.4\n",
      "   2. year: 2025.0, month: 5.0, order_count: 12.0, avg_freight: 31256.24\n",
      "   3. year: 2025.0, month: 4.0, order_count: 22.0, avg_freight: 26207.64\n",
      "   4. year: 2025.0, month: 3.0, order_count: 16.0, avg_freight: 28940.67\n",
      "   5. year: 2025.0, month: 2.0, order_count: 16.0, avg_freight: 26968.31\n",
      "   ... 그 외 1개 행\n",
      "--------------------------------------------------\n",
      "🔍 테스트 4: 베스트셀러 상품 TOP 5\n",
      "📝 설명: 판매량 기준 베스트셀러 상품\n",
      "✅ 실행 성공 (6개 행)\n",
      "📊 결과:\n",
      "   1. year: 2025.0, month: 6.0, order_count: 15.0, avg_freight: 27776.4\n",
      "   2. year: 2025.0, month: 5.0, order_count: 12.0, avg_freight: 31256.24\n",
      "   3. year: 2025.0, month: 4.0, order_count: 22.0, avg_freight: 26207.64\n",
      "   4. year: 2025.0, month: 3.0, order_count: 16.0, avg_freight: 28940.67\n",
      "   5. year: 2025.0, month: 2.0, order_count: 16.0, avg_freight: 26968.31\n",
      "   ... 그 외 1개 행\n",
      "--------------------------------------------------\n",
      "🔍 테스트 4: 베스트셀러 상품 TOP 5\n",
      "📝 설명: 판매량 기준 베스트셀러 상품\n",
      "✅ 실행 성공 (5개 행)\n",
      "📊 결과:\n",
      "   1. product_name: 해물파전, total_quantity: 184, total_revenue: 2880000.0\n",
      "   2. product_name: 순두부찌개, total_quantity: 180, total_revenue: 1591200.0\n",
      "   3. product_name: 김치, total_quantity: 156, total_revenue: 2264250.0\n",
      "   4. product_name: 삼겹살, total_quantity: 150, total_revenue: 3202100.0\n",
      "   5. product_name: 비빔밥, total_quantity: 148, total_revenue: 1724400.0\n",
      "--------------------------------------------------\n",
      "🔍 테스트 5: 고객별 주문 통계\n",
      "📝 설명: 구매 금액 기준 상위 고객\n",
      "✅ 실행 성공 (5개 행)\n",
      "📊 결과:\n",
      "   1. product_name: 해물파전, total_quantity: 184, total_revenue: 2880000.0\n",
      "   2. product_name: 순두부찌개, total_quantity: 180, total_revenue: 1591200.0\n",
      "   3. product_name: 김치, total_quantity: 156, total_revenue: 2264250.0\n",
      "   4. product_name: 삼겹살, total_quantity: 150, total_revenue: 3202100.0\n",
      "   5. product_name: 비빔밥, total_quantity: 148, total_revenue: 1724400.0\n",
      "--------------------------------------------------\n",
      "🔍 테스트 5: 고객별 주문 통계\n",
      "📝 설명: 구매 금액 기준 상위 고객\n",
      "✅ 실행 성공 (5개 행)\n",
      "📊 결과:\n",
      "   1. company_name: 한국식당, order_count: 70, total_spent: 5197600.0\n",
      "   2. company_name: 코리안 BBQ, order_count: 60, total_spent: 4863000.0\n",
      "   3. company_name: 중국반점, order_count: 66, total_spent: 4038200.0\n",
      "   4. company_name: 한식당, order_count: 51, total_spent: 3824100.0\n",
      "   5. company_name: 일본레스토랑, order_count: 43, total_spent: 2630500.0\n",
      "--------------------------------------------------\n",
      "🎉 기본 쿼리 테스트 완료!\n",
      "\n",
      "============================================================\n",
      "📋 LangChain Agent용 스키마 분석 중...\n",
      "✅ 실행 성공 (5개 행)\n",
      "📊 결과:\n",
      "   1. company_name: 한국식당, order_count: 70, total_spent: 5197600.0\n",
      "   2. company_name: 코리안 BBQ, order_count: 60, total_spent: 4863000.0\n",
      "   3. company_name: 중국반점, order_count: 66, total_spent: 4038200.0\n",
      "   4. company_name: 한식당, order_count: 51, total_spent: 3824100.0\n",
      "   5. company_name: 일본레스토랑, order_count: 43, total_spent: 2630500.0\n",
      "--------------------------------------------------\n",
      "🎉 기본 쿼리 테스트 완료!\n",
      "\n",
      "============================================================\n",
      "📋 LangChain Agent용 스키마 분석 중...\n",
      "✅ 스키마 분석 완료: 8개 테이블\n",
      "\n",
      "📋 LangChain Agent용 스키마 요약:\n",
      "   데이터베이스: northwind\n",
      "   테이블 수: 8\n",
      "   관계 수: 8\n",
      "\n",
      "🎯 다음 단계: LangChain Agent 구현\n",
      "   노트북: 02_langchain_agent_text_to_sql.ipynb\n",
      "\n",
      "✅ 01단계 완료: Northwind 데이터베이스 구축 및 검증\n",
      "🔍 Northwind 데이터베이스 검증 및 테스트\n",
      "=============================================\n",
      "\n",
      "📋 데이터베이스 현황:\n",
      "✅ 스키마 분석 완료: 8개 테이블\n",
      "\n",
      "📋 LangChain Agent용 스키마 요약:\n",
      "   데이터베이스: northwind\n",
      "   테이블 수: 8\n",
      "   관계 수: 8\n",
      "\n",
      "🎯 다음 단계: LangChain Agent 구현\n",
      "   노트북: 02_langchain_agent_text_to_sql.ipynb\n",
      "\n",
      "✅ 01단계 완료: Northwind 데이터베이스 구축 및 검증\n",
      "🔍 Northwind 데이터베이스 검증 및 테스트\n",
      "=============================================\n",
      "\n",
      "📋 데이터베이스 현황:\n",
      "   📂 전체 데이터베이스: default, information_schema, northwind\n",
      "   ✅ northwind 데이터베이스 확인됨\n",
      "   📂 전체 데이터베이스: default, information_schema, northwind\n",
      "   ✅ northwind 데이터베이스 확인됨\n",
      "   📊 northwind 테이블 수: 8개\n",
      "\n",
      "📈 테이블별 데이터 현황:\n",
      "   📊 northwind 테이블 수: 8개\n",
      "\n",
      "📈 테이블별 데이터 현황:\n",
      "   📂 categories: 8개 레코드\n",
      "   📂 categories: 8개 레코드\n",
      "   📂 customers: 5개 레코드\n",
      "   📂 customers: 5개 레코드\n",
      "   📂 employees: 5개 레코드\n",
      "   📂 employees: 5개 레코드\n",
      "   📂 order_details: 290개 레코드\n",
      "   📂 order_details: 290개 레코드\n",
      "   📂 orders: 100개 레코드\n",
      "   📂 orders: 100개 레코드\n",
      "   📂 products: 10개 레코드\n",
      "   📂 products: 10개 레코드\n",
      "   📂 shippers: 3개 레코드\n",
      "   📂 shippers: 3개 레코드\n",
      "   📂 suppliers: 5개 레코드\n",
      "\n",
      "📊 전체 레코드 수: 426개\n",
      "\n",
      "🧪 기본 SQL 쿼리 테스트:\n",
      "\n",
      "   🧪 테스트 1: 고객 수 조회\n",
      "      📝 전체 고객 수 확인\n",
      "   📂 suppliers: 5개 레코드\n",
      "\n",
      "📊 전체 레코드 수: 426개\n",
      "\n",
      "🧪 기본 SQL 쿼리 테스트:\n",
      "\n",
      "   🧪 테스트 1: 고객 수 조회\n",
      "      📝 전체 고객 수 확인\n",
      "      ✅ 성공 (1개 결과)\n",
      "      ✅ 성공 (1개 결과)\n",
      "         1. customer_count=5\n",
      "\n",
      "   🧪 테스트 2: 상품 카테고리별 개수\n",
      "      📝 카테고리별 상품 수 집계\n",
      "         1. customer_count=5\n",
      "\n",
      "   🧪 테스트 2: 상품 카테고리별 개수\n",
      "      📝 카테고리별 상품 수 집계\n",
      "      ❌ 실패: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `c`.`categoryid` cannot be resolved. Did you mean one of the following? [`c`.`category_id`, `p`.`category_id`, `c`.`category_name`, `c`.`description`, `p`.`product_id`]. SQLSTATE: 42703; line 4 pos 42;\n",
      "'Sort ['product_count DESC NULLS LAST], true\n",
      "+- 'Aggregate ['c.categoryname], ['c.categoryname, 'COUNT('p.productid) AS product_count#27342]\n",
      "   +- 'Join LeftOuter, ('c.categoryid = 'p.categoryid)\n",
      "      :- SubqueryAlias c\n",
      "      :  +- SubqueryAlias workspace.northwind.categories\n",
      "      :     +- Relation workspace.northwind.categories[category_id#27354,category_name#27355,description#27356] parquet\n",
      "      +- SubqueryAlias p\n",
      "         +- SubqueryAlias workspace.northwind.products\n",
      "            +- Relation workspace.northwind.products[product_id#27357,product_name#27358,supplier_id#27359,category_id#27360,unit_price#27361,units_in_stock#27362,units_on_order#27363,discontinued#27364] parquet\n",
      "\n",
      "\n",
      "JVM stacktrace:\n",
      "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n",
      "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n",
      "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1130)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1082)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
      "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
      "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
      "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
      "\tat scala.util.Using$.resource(Using.scala:269)\n",
      "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
      "\n",
      "   🧪 테스트 3: 가격이 가장 비싼 상품 TOP 3\n",
      "      📝 최고가 상품 조회\n",
      "      ❌ 실패: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `c`.`categoryid` cannot be resolved. Did you mean one of the following? [`c`.`category_id`, `p`.`category_id`, `c`.`category_name`, `c`.`description`, `p`.`product_id`]. SQLSTATE: 42703; line 4 pos 42;\n",
      "'Sort ['product_count DESC NULLS LAST], true\n",
      "+- 'Aggregate ['c.categoryname], ['c.categoryname, 'COUNT('p.productid) AS product_count#27342]\n",
      "   +- 'Join LeftOuter, ('c.categoryid = 'p.categoryid)\n",
      "      :- SubqueryAlias c\n",
      "      :  +- SubqueryAlias workspace.northwind.categories\n",
      "      :     +- Relation workspace.northwind.categories[category_id#27354,category_name#27355,description#27356] parquet\n",
      "      +- SubqueryAlias p\n",
      "         +- SubqueryAlias workspace.northwind.products\n",
      "            +- Relation workspace.northwind.products[product_id#27357,product_name#27358,supplier_id#27359,category_id#27360,unit_price#27361,units_in_stock#27362,units_on_order#27363,discontinued#27364] parquet\n",
      "\n",
      "\n",
      "JVM stacktrace:\n",
      "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n",
      "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n",
      "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1130)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1082)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
      "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
      "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
      "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
      "\tat scala.util.Using$.resource(Using.scala:269)\n",
      "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
      "\n",
      "   🧪 테스트 3: 가격이 가장 비싼 상품 TOP 3\n",
      "      📝 최고가 상품 조회\n",
      "      ❌ 실패: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `unitprice` cannot be resolved. Did you mean one of the following? [`unit_price`, `category_id`, `supplier_id`, `units_in_stock`, `discontinued`]. SQLSTATE: 42703; line 4 pos 14;\n",
      "'GlobalLimit 3\n",
      "+- 'LocalLimit 3\n",
      "   +- 'Sort ['unitprice DESC NULLS LAST], true\n",
      "      +- 'Project ['productname, 'unitprice, 'categoryid]\n",
      "         +- 'Filter isnotnull('unitprice)\n",
      "            +- SubqueryAlias workspace.northwind.products\n",
      "               +- Relation workspace.northwind.products[product_id#27376,product_name#27377,supplier_id#27378,category_id#27379,unit_price#27380,units_in_stock#27381,units_on_order#27382,discontinued#27383] parquet\n",
      "\n",
      "\n",
      "JVM stacktrace:\n",
      "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n",
      "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n",
      "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1130)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1082)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
      "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
      "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
      "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
      "\tat scala.util.Using$.resource(Using.scala:269)\n",
      "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
      "\n",
      "   🧪 테스트 4: 국가별 고객 분포\n",
      "      📝 주요 국가별 고객 수\n",
      "      ❌ 실패: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `unitprice` cannot be resolved. Did you mean one of the following? [`unit_price`, `category_id`, `supplier_id`, `units_in_stock`, `discontinued`]. SQLSTATE: 42703; line 4 pos 14;\n",
      "'GlobalLimit 3\n",
      "+- 'LocalLimit 3\n",
      "   +- 'Sort ['unitprice DESC NULLS LAST], true\n",
      "      +- 'Project ['productname, 'unitprice, 'categoryid]\n",
      "         +- 'Filter isnotnull('unitprice)\n",
      "            +- SubqueryAlias workspace.northwind.products\n",
      "               +- Relation workspace.northwind.products[product_id#27376,product_name#27377,supplier_id#27378,category_id#27379,unit_price#27380,units_in_stock#27381,units_on_order#27382,discontinued#27383] parquet\n",
      "\n",
      "\n",
      "JVM stacktrace:\n",
      "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n",
      "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n",
      "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1130)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1082)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
      "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
      "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
      "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
      "\tat scala.util.Using$.resource(Using.scala:269)\n",
      "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
      "\n",
      "   🧪 테스트 4: 국가별 고객 분포\n",
      "      📝 주요 국가별 고객 수\n",
      "      ✅ 성공 (5개 결과)\n",
      "      ✅ 성공 (5개 결과)\n",
      "         1. country=한국, customer_count=1\n",
      "         2. country=캐나다, customer_count=1\n",
      "         3. country=중국, customer_count=1\n",
      "         ... 그 외 2개 결과\n",
      "\n",
      "📊 SQL 테스트 결과:\n",
      "   ✅ 성공: 2개\n",
      "   ❌ 실패: 2개\n",
      "\n",
      "⚠️ 일부 테스트가 실패했습니다. 데이터를 확인해주세요.\n",
      "         1. country=한국, customer_count=1\n",
      "         2. country=캐나다, customer_count=1\n",
      "         3. country=중국, customer_count=1\n",
      "         ... 그 외 2개 결과\n",
      "\n",
      "📊 SQL 테스트 결과:\n",
      "   ✅ 성공: 2개\n",
      "   ❌ 실패: 2개\n",
      "\n",
      "⚠️ 일부 테스트가 실패했습니다. 데이터를 확인해주세요.\n"
     ]
    }
   ],
   "source": [
    "# 기본 Text-to-SQL 프롬프트 템플릿\n",
    "basic_sql_prompt_template = \"\"\"\n",
    "당신은 SQL 쿼리 생성 전문가입니다. 주어진 데이터베이스 스키마를 바탕으로 자연어 질문을 정확한 SQL 쿼리로 변환하세요.\n",
    "\n",
    "{schema_context}\n",
    "\n",
    "규칙:\n",
    "1. 정확한 테이블명과 컬럼명을 사용하세요\n",
    "2. 적절한 JOIN을 사용하세요\n",
    "3. WHERE 절을 적절히 활용하세요\n",
    "4. 결과는 SQL 쿼리만 반환하세요 (설명 없이)\n",
    "5. 쿼리는 ```sql과 ``` 사이에 작성하세요\n",
    "\n",
    "예시:\n",
    "질문: \"모든 고객의 이름과 이메일을 보여주세요\"\n",
    "답변:\n",
    "```sql\n",
    "SELECT name, email FROM customers;\n",
    "```\n",
    "\n",
    "질문: \"2024년 1월에 주문한 고객들의 총 주문 금액을 계산해주세요\"\n",
    "답변:\n",
    "```sql\n",
    "SELECT c.name, SUM(o.amount) as total_amount\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "WHERE o.order_date >= '2024-01-01' AND o.order_date < '2024-02-01'\n",
    "GROUP BY c.customer_id, c.name;\n",
    "```\n",
    "\n",
    "질문: {question}\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ 기본 Text-to-SQL 프롬프트 템플릿 정의 완료\")\n",
    "\n",
    "class SQLExecutor:\n",
    "    \"\"\"안전한 SQL 실행 및 결과 검증\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session, max_rows: int = 100):\n",
    "        self.spark = spark_session\n",
    "        self.max_rows = max_rows\n",
    "        self.execution_history = []\n",
    "    \n",
    "    def validate_sql_safety(self, sql_query: str) -> Tuple[bool, str]:\n",
    "        \"\"\"SQL 쿼리 안전성 검증\"\"\"\n",
    "        sql_upper = sql_query.upper().strip()\n",
    "        \n",
    "        # 위험한 키워드 확인\n",
    "        dangerous_keywords = [\n",
    "            'DROP', 'DELETE', 'TRUNCATE', 'ALTER', 'CREATE', \n",
    "            'INSERT', 'UPDATE', 'MERGE', 'COPY'\n",
    "        ]\n",
    "        \n",
    "        for keyword in dangerous_keywords:\n",
    "            if keyword in sql_upper:\n",
    "                return False, f\"위험한 키워드 '{keyword}'가 포함되어 있습니다.\"\n",
    "        \n",
    "        # 기본적인 구문 검증\n",
    "        if not any(keyword in sql_upper for keyword in ['SELECT', 'WITH', 'SHOW', 'DESCRIBE']):\n",
    "            return False, \"허용된 SQL 키워드가 없습니다.\"\n",
    "        \n",
    "        # 괄호 매칭 확인\n",
    "        if sql_query.count('(') != sql_query.count(')'):\n",
    "            return False, \"괄호가 일치하지 않습니다.\"\n",
    "        \n",
    "        return True, \"안전한 쿼리입니다.\"\n",
    "    \n",
    "    def execute_sql(self, sql_query: str, dry_run: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"SQL 쿼리 실행\"\"\"\n",
    "        execution_start = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # 1. 안전성 검증\n",
    "            is_safe, safety_message = self.validate_sql_safety(sql_query)\n",
    "            if not is_safe:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": f\"안전성 검증 실패: {safety_message}\",\n",
    "                    \"results\": None,\n",
    "                    \"execution_time\": 0\n",
    "                }\n",
    "            \n",
    "            # 2. Dry run (쿼리 파싱 검증)\n",
    "            if dry_run:\n",
    "                try:\n",
    "                    self.spark.sql(f\"EXPLAIN {sql_query}\")\n",
    "                    return {\n",
    "                        \"success\": True,\n",
    "                        \"message\": \"쿼리 구문이 유효합니다.\",\n",
    "                        \"dry_run\": True\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    return {\n",
    "                        \"success\": False,\n",
    "                        \"error\": f\"쿼리 구문 오류: {str(e)}\",\n",
    "                        \"dry_run\": True\n",
    "                    }\n",
    "            \n",
    "            # 3. 실제 실행\n",
    "            if not self.spark:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"Spark 세션이 없습니다.\",\n",
    "                    \"results\": None\n",
    "                }\n",
    "            \n",
    "            # LIMIT 추가 (안전성을 위해)\n",
    "            if \"LIMIT\" not in sql_query.upper():\n",
    "                sql_query += f\" LIMIT {self.max_rows}\"\n",
    "            \n",
    "            # 쿼리 실행\n",
    "            result_df = self.spark.sql(sql_query)\n",
    "            results = result_df.collect()\n",
    "            \n",
    "            # 결과 처리\n",
    "            execution_time = (datetime.now() - execution_start).total_seconds()\n",
    "            \n",
    "            # 결과를 딕셔너리 리스트로 변환\n",
    "            results_data = [row.asDict() for row in results]\n",
    "            \n",
    "            # 실행 기록 저장\n",
    "            execution_record = {\n",
    "                \"timestamp\": execution_start.isoformat(),\n",
    "                \"sql_query\": sql_query,\n",
    "                \"success\": True,\n",
    "                \"row_count\": len(results_data),\n",
    "                \"execution_time\": execution_time\n",
    "            }\n",
    "            self.execution_history.append(execution_record)\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"results\": results_data,\n",
    "                \"row_count\": len(results_data),\n",
    "                \"columns\": result_df.columns,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"sql_query\": sql_query\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_time = (datetime.now() - execution_start).total_seconds()\n",
    "            \n",
    "            # 오류 기록 저장\n",
    "            error_record = {\n",
    "                \"timestamp\": execution_start.isoformat(),\n",
    "                \"sql_query\": sql_query,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"execution_time\": execution_time\n",
    "            }\n",
    "            self.execution_history.append(error_record)\n",
    "            \n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"SQL 실행 오류: {str(e)}\",\n",
    "                \"results\": None,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"sql_query\": sql_query\n",
    "            }\n",
    "    \n",
    "    def format_results(self, execution_result: Dict[str, Any]) -> str:\n",
    "        \"\"\"실행 결과를 사용자 친화적으로 포맷팅\"\"\"\n",
    "        if not execution_result[\"success\"]:\n",
    "            return f\"❌ 실행 실패: {execution_result['error']}\"\n",
    "        \n",
    "        results = execution_result[\"results\"]\n",
    "        row_count = execution_result[\"row_count\"]\n",
    "        execution_time = execution_result[\"execution_time\"]\n",
    "        \n",
    "        if not results:\n",
    "            return f\"✅ 쿼리 실행 성공 (결과 없음) - 실행시간: {execution_time:.3f}초\"\n",
    "        \n",
    "        # 테이블 형태로 포맷팅\n",
    "        if row_count <= 10:\n",
    "            # 적은 결과는 전체 표시\n",
    "            df = pd.DataFrame(results)\n",
    "            formatted_table = df.to_string(index=False)\n",
    "        else:\n",
    "            # 많은 결과는 일부만 표시\n",
    "            df = pd.DataFrame(results[:10])\n",
    "            formatted_table = df.to_string(index=False)\n",
    "            formatted_table += f\"\\n... 그 외 {row_count - 10}개 행 (총 {row_count}개)\"\n",
    "        \n",
    "        return f\"\"\"✅ 쿼리 실행 성공\n",
    "📊 결과: {row_count}개 행\n",
    "⏱️ 실행시간: {execution_time:.3f}초\n",
    "\n",
    "{formatted_table}\"\"\"\n",
    "\n",
    "    def get_execution_history(self, limit: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"최근 실행 기록 조회\"\"\"\n",
    "        return self.execution_history[-limit:]\n",
    "\n",
    "# SQL 실행기 초기화\n",
    "if spark:\n",
    "    sql_executor = SQLExecutor(spark, max_rows=100)\n",
    "    print(\"✅ SQL 실행기 초기화 완료\")\n",
    "else:\n",
    "    sql_executor = None\n",
    "    print(\"⚠️ Spark 세션이 없어 SQL 실행기를 초기화할 수 없습니다.\")\n",
    "\n",
    "# Northwind 데이터베이스 기본 쿼리 테스트\n",
    "def test_northwind_queries():\n",
    "    \"\"\"Northwind 데이터베이스 기본 쿼리 테스트\"\"\"\n",
    "    \n",
    "    if not spark:\n",
    "        print(\"❌ Spark 세션이 필요합니다.\")\n",
    "        return\n",
    "    \n",
    "    print(\"🧪 Northwind 데이터베이스 쿼리 테스트 시작...\\n\")\n",
    "    \n",
    "    test_queries = [\n",
    "        {\n",
    "            \"name\": \"전체 고객 수\",\n",
    "            \"sql\": \"SELECT COUNT(*) as customer_count FROM northwind.customers\",\n",
    "            \"description\": \"등록된 고객의 총 개수\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"상품 카테고리별 개수\", \n",
    "            \"sql\": \"\"\"\n",
    "                SELECT c.category_name, COUNT(p.product_id) as product_count\n",
    "                FROM northwind.categories c\n",
    "                LEFT JOIN northwind.products p ON c.category_id = p.category_id\n",
    "                GROUP BY c.category_id, c.category_name\n",
    "                ORDER BY product_count DESC\n",
    "            \"\"\",\n",
    "            \"description\": \"각 카테고리별 상품 개수\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"월별 주문 현황\",\n",
    "            \"sql\": \"\"\"\n",
    "                SELECT \n",
    "                    YEAR(order_date) as year,\n",
    "                    MONTH(order_date) as month,\n",
    "                    COUNT(*) as order_count,\n",
    "                    ROUND(AVG(freight), 2) as avg_freight\n",
    "                FROM northwind.orders \n",
    "                WHERE order_date IS NOT NULL\n",
    "                GROUP BY YEAR(order_date), MONTH(order_date)\n",
    "                ORDER BY year DESC, month DESC\n",
    "                LIMIT 10\n",
    "            \"\"\",\n",
    "            \"description\": \"최근 월별 주문 건수 및 평균 배송비\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"베스트셀러 상품 TOP 5\",\n",
    "            \"sql\": \"\"\"\n",
    "                SELECT \n",
    "                    p.product_name,\n",
    "                    SUM(od.quantity) as total_quantity,\n",
    "                    ROUND(SUM(od.quantity * od.unit_price * (1 - od.discount)), 2) as total_revenue\n",
    "                FROM northwind.order_details od\n",
    "                JOIN northwind.products p ON od.product_id = p.product_id\n",
    "                GROUP BY p.product_id, p.product_name\n",
    "                ORDER BY total_quantity DESC\n",
    "                LIMIT 5\n",
    "            \"\"\",\n",
    "            \"description\": \"판매량 기준 베스트셀러 상품\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"고객별 주문 통계\",\n",
    "            \"sql\": \"\"\"\n",
    "                SELECT \n",
    "                    c.company_name,\n",
    "                    COUNT(o.order_id) as order_count,\n",
    "                    ROUND(SUM(od.quantity * od.unit_price * (1 - od.discount)), 2) as total_spent\n",
    "                FROM northwind.customers c\n",
    "                LEFT JOIN northwind.orders o ON c.customer_id = o.customer_id\n",
    "                LEFT JOIN northwind.order_details od ON o.order_id = od.order_id\n",
    "                GROUP BY c.customer_id, c.company_name\n",
    "                HAVING COUNT(o.order_id) > 0\n",
    "                ORDER BY total_spent DESC\n",
    "                LIMIT 5\n",
    "            \"\"\",\n",
    "            \"description\": \"구매 금액 기준 상위 고객\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 각 쿼리 실행 및 결과 출력\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"🔍 테스트 {i}: {query['name']}\")\n",
    "        print(f\"📝 설명: {query['description']}\")\n",
    "        \n",
    "        try:\n",
    "            # 쿼리 실행\n",
    "            result_df = spark.sql(query['sql'])\n",
    "            result_pandas = result_df.toPandas()\n",
    "            \n",
    "            print(f\"✅ 실행 성공 ({len(result_pandas)}개 행)\")\n",
    "            \n",
    "            # 결과 출력 (처음 5개 행만)\n",
    "            if len(result_pandas) > 0:\n",
    "                print(\"📊 결과:\")\n",
    "                display_df = result_pandas.head(5)\n",
    "                for idx, row in display_df.iterrows():\n",
    "                    row_str = \", \".join([f\"{col}: {val}\" for col, val in row.items()])\n",
    "                    print(f\"   {idx+1}. {row_str}\")\n",
    "                \n",
    "                if len(result_pandas) > 5:\n",
    "                    print(f\"   ... 그 외 {len(result_pandas) - 5}개 행\")\n",
    "            else:\n",
    "                print(\"   (결과 없음)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 실행 실패: {str(e)}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"🎉 기본 쿼리 테스트 완료!\")\n",
    "\n",
    "def analyze_schema_for_langchain():\n",
    "    \"\"\"LangChain Agent에서 사용할 스키마 정보 추출\"\"\"\n",
    "    \n",
    "    if not spark:\n",
    "        print(\"❌ Spark 세션이 필요합니다.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"📋 LangChain Agent용 스키마 분석 중...\")\n",
    "    \n",
    "    schema_info = {}\n",
    "    \n",
    "    try:\n",
    "        # 각 테이블의 스키마 정보 수집\n",
    "        tables = spark.sql(\"SHOW TABLES IN northwind\").collect()\n",
    "        \n",
    "        for table_row in tables:\n",
    "            table_name = table_row.tableName\n",
    "            \n",
    "            # 테이블 스키마 조회\n",
    "            describe_result = spark.sql(f\"DESCRIBE TABLE northwind.{table_name}\").collect()\n",
    "            \n",
    "            columns = []\n",
    "            for row in describe_result:\n",
    "                if row.col_name and not row.col_name.startswith('#'):\n",
    "                    columns.append({\n",
    "                        \"name\": row.col_name,\n",
    "                        \"type\": row.data_type,\n",
    "                        \"nullable\": row.comment != \"NOT NULL\" if row.comment else True\n",
    "                    })\n",
    "            \n",
    "            # 샘플 데이터 몇 개 가져오기\n",
    "            sample_df = spark.table(f\"northwind.{table_name}\").limit(3).toPandas()\n",
    "            sample_data = sample_df.to_dict('records') if len(sample_df) > 0 else []\n",
    "            \n",
    "            # 테이블 통계\n",
    "            row_count = spark.table(f\"northwind.{table_name}\").count()\n",
    "            \n",
    "            schema_info[table_name] = {\n",
    "                \"table_name\": table_name,\n",
    "                \"full_name\": f\"northwind.{table_name}\",\n",
    "                \"columns\": columns,\n",
    "                \"sample_data\": sample_data,\n",
    "                \"row_count\": row_count,\n",
    "                \"description\": get_table_description(table_name)\n",
    "            }\n",
    "        \n",
    "        print(f\"✅ 스키마 분석 완료: {len(schema_info)}개 테이블\")\n",
    "        \n",
    "        # LangChain Agent용 요약 정보 생성\n",
    "        schema_summary = {\n",
    "            \"database_name\": \"northwind\",\n",
    "            \"tables\": list(schema_info.keys()),\n",
    "            \"total_tables\": len(schema_info),\n",
    "            \"relationships\": get_table_relationships(),\n",
    "            \"schema_details\": schema_info\n",
    "        }\n",
    "        \n",
    "        return schema_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 스키마 분석 실패: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_table_description(table_name):\n",
    "    \"\"\"테이블별 설명 반환\"\"\"\n",
    "    descriptions = {\n",
    "        \"categories\": \"상품 카테고리 정보 (음료, 조미료, 과자류 등)\",\n",
    "        \"suppliers\": \"공급업체 정보 및 연락처\",\n",
    "        \"products\": \"상품 정보 (이름, 가격, 재고, 카테고리, 공급업체)\",\n",
    "        \"customers\": \"고객 정보 및 연락처\",\n",
    "        \"employees\": \"직원 정보 및 조직 구조\",\n",
    "        \"shippers\": \"배송업체 정보\",\n",
    "        \"orders\": \"주문 정보 (고객, 직원, 날짜, 배송)\",\n",
    "        \"order_details\": \"주문 상세 정보 (상품, 수량, 가격, 할인)\"\n",
    "    }\n",
    "    return descriptions.get(table_name, \"\")\n",
    "\n",
    "def get_table_relationships():\n",
    "    \"\"\"테이블 간 관계 정보 반환\"\"\"\n",
    "    return {\n",
    "        \"orders_customers\": \"orders.customer_id -> customers.customer_id\",\n",
    "        \"orders_employees\": \"orders.employee_id -> employees.employee_id\", \n",
    "        \"orders_shippers\": \"orders.ship_via -> shippers.shipper_id\",\n",
    "        \"order_details_orders\": \"order_details.order_id -> orders.order_id\",\n",
    "        \"order_details_products\": \"order_details.product_id -> products.product_id\",\n",
    "        \"products_categories\": \"products.category_id -> categories.category_id\",\n",
    "        \"products_suppliers\": \"products.supplier_id -> suppliers.supplier_id\",\n",
    "        \"employees_manager\": \"employees.reports_to -> employees.employee_id\"\n",
    "    }\n",
    "\n",
    "# 테스트 실행\n",
    "try:\n",
    "    test_northwind_queries()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    schema_info = analyze_schema_for_langchain()\n",
    "    \n",
    "    if schema_info:\n",
    "        print(f\"\\n📋 LangChain Agent용 스키마 요약:\")\n",
    "        print(f\"   데이터베이스: {schema_info['database_name']}\")\n",
    "        print(f\"   테이블 수: {schema_info['total_tables']}\")\n",
    "        print(f\"   관계 수: {len(schema_info['relationships'])}\")\n",
    "        \n",
    "        print(f\"\\n🎯 다음 단계: LangChain Agent 구현\")\n",
    "        print(f\"   노트북: 02_langchain_agent_text_to_sql.ipynb\")\n",
    "        \n",
    "        # 스키마 정보를 다음 노트북에서 사용할 수 있도록 저장\n",
    "        globals()['northwind_schema'] = schema_info\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 테스트 실행 실패: {str(e)}\")\n",
    "\n",
    "print(f\"\\n✅ 01단계 완료: Northwind 데이터베이스 구축 및 검증\")\n",
    "\n",
    "# 데이터 검증 및 기본 SQL 테스트\n",
    "\n",
    "print(\"🔍 Northwind 데이터베이스 검증 및 테스트\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. 데이터베이스 및 테이블 확인\n",
    "print(\"\\n📋 데이터베이스 현황:\")\n",
    "databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "db_names = [row.databaseName for row in databases]\n",
    "print(f\"   📂 전체 데이터베이스: {', '.join(db_names)}\")\n",
    "\n",
    "if \"northwind\" in db_names:\n",
    "    print(\"   ✅ northwind 데이터베이스 확인됨\")\n",
    "    \n",
    "    # 테이블 목록 조회\n",
    "    tables = spark.sql(\"SHOW TABLES IN northwind\").collect()\n",
    "    table_names = [row.tableName for row in tables]\n",
    "    print(f\"   📊 northwind 테이블 수: {len(table_names)}개\")\n",
    "    \n",
    "    # 각 테이블의 레코드 수 확인\n",
    "    print(f\"\\n📈 테이블별 데이터 현황:\")\n",
    "    total_records = 0\n",
    "    \n",
    "    for table_name in sorted(table_names):\n",
    "        try:\n",
    "            count = spark.table(f\"northwind.{table_name}\").count()\n",
    "            total_records += count\n",
    "            print(f\"   📂 {table_name}: {count:,}개 레코드\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ {table_name}: 조회 실패 ({e})\")\n",
    "    \n",
    "    print(f\"\\n📊 전체 레코드 수: {total_records:,}개\")\n",
    "    \n",
    "else:\n",
    "    print(\"   ❌ northwind 데이터베이스를 찾을 수 없습니다!\")\n",
    "\n",
    "# 2. 기본 SQL 쿼리 테스트\n",
    "print(f\"\\n🧪 기본 SQL 쿼리 테스트:\")\n",
    "\n",
    "test_queries = [\n",
    "    {\n",
    "        \"name\": \"고객 수 조회\",\n",
    "        \"sql\": \"SELECT COUNT(*) as customer_count FROM northwind.customers\",\n",
    "        \"description\": \"전체 고객 수 확인\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"상품 카테고리별 개수\",\n",
    "        \"sql\": \"\"\"\n",
    "        SELECT c.categoryname, COUNT(p.productid) as product_count\n",
    "        FROM northwind.categories c\n",
    "        LEFT JOIN northwind.products p ON c.categoryid = p.categoryid\n",
    "        GROUP BY c.categoryname\n",
    "        ORDER BY product_count DESC\n",
    "        \"\"\",\n",
    "        \"description\": \"카테고리별 상품 수 집계\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"가격이 가장 비싼 상품 TOP 3\",\n",
    "        \"sql\": \"\"\"\n",
    "        SELECT productname, unitprice, categoryid\n",
    "        FROM northwind.products\n",
    "        WHERE unitprice IS NOT NULL\n",
    "        ORDER BY unitprice DESC\n",
    "        LIMIT 3\n",
    "        \"\"\",\n",
    "        \"description\": \"최고가 상품 조회\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"국가별 고객 분포\",\n",
    "        \"sql\": \"\"\"\n",
    "        SELECT country, COUNT(*) as customer_count\n",
    "        FROM northwind.customers\n",
    "        GROUP BY country\n",
    "        ORDER BY customer_count DESC\n",
    "        LIMIT 5\n",
    "        \"\"\",\n",
    "        \"description\": \"주요 국가별 고객 수\"\n",
    "    }\n",
    "]\n",
    "\n",
    "successful_tests = 0\n",
    "failed_tests = 0\n",
    "\n",
    "for i, test in enumerate(test_queries, 1):\n",
    "    try:\n",
    "        print(f\"\\n   🧪 테스트 {i}: {test['name']}\")\n",
    "        print(f\"      📝 {test['description']}\")\n",
    "        \n",
    "        result_df = spark.sql(test['sql'])\n",
    "        results = result_df.collect()\n",
    "        \n",
    "        if results:\n",
    "            print(f\"      ✅ 성공 ({len(results)}개 결과)\")\n",
    "            \n",
    "            # 결과 샘플 표시 (처음 3개)\n",
    "            for j, row in enumerate(results[:3]):\n",
    "                row_data = [f\"{col}={row[col]}\" for col in result_df.columns]\n",
    "                print(f\"         {j+1}. {', '.join(row_data)}\")\n",
    "            \n",
    "            if len(results) > 3:\n",
    "                print(f\"         ... 그 외 {len(results)-3}개 결과\")\n",
    "        else:\n",
    "            print(f\"      ⚠️ 결과 없음\")\n",
    "            \n",
    "        successful_tests += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ❌ 실패: {str(e)}\")\n",
    "        failed_tests += 1\n",
    "\n",
    "print(f\"\\n📊 SQL 테스트 결과:\")\n",
    "print(f\"   ✅ 성공: {successful_tests}개\")\n",
    "print(f\"   ❌ 실패: {failed_tests}개\")\n",
    "\n",
    "if successful_tests == len(test_queries):\n",
    "    print(f\"\\n🎉 모든 SQL 테스트가 성공했습니다!\")\n",
    "    print(f\"✅ Northwind 데이터베이스가 정상적으로 구축되었습니다!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ 일부 테스트가 실패했습니다. 데이터를 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecff4cc",
   "metadata": {},
   "source": [
    "## 🔗 다음 단계: LangChain Agent 구현\n",
    "\n",
    "이제 Northwind 데이터베이스가 성공적으로 구축되었습니다! \n",
    "\n",
    "### 📝 완료된 작업\n",
    "✅ **Databricks 환경 설정** - Spark 세션 및 northwind 데이터베이스 생성  \n",
    "✅ **Northwind 샘플 데이터** - 8개 테이블, 실제 비즈니스 시나리오 반영  \n",
    "✅ **Delta Lake 테이블** - 고성능 분석을 위한 최적화된 저장소  \n",
    "✅ **스키마 분석** - LangChain Agent에서 활용할 메타데이터 추출  \n",
    "✅ **기본 쿼리 테스트** - 데이터 무결성 및 관계 검증 완료  \n",
    "\n",
    "### 🚀 다음 노트북에서 구현할 내용\n",
    "\n",
    "**`02_langchain_agent_text_to_sql.ipynb`**\n",
    "- 🤖 **LangChain Agent 아키텍처** 설계\n",
    "- 🔧 **Function Tools** 구현 (스키마 조회, SQL 실행, 결과 검증)\n",
    "- 🧠 **추론 모델 연동** (Databricks Foundation Models)\n",
    "- 💬 **자연어 → SQL 변환** 파이프라인\n",
    "- 🔍 **동적 스키마 검색** 및 컨텍스트 구성\n",
    "- 🛡️ **안전성 검증** 및 오류 처리\n",
    "\n",
    "## 6. 스키마 정보 추출 및 LangChain Agent 연동 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1238b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 한국어 특화 고급 프롬프트 템플릿 정의 완료\n",
      "✅ Text-to-SQL RAG 애플리케이션 클래스 정의 완료\n",
      "🎉 Northwind 데이터베이스 구축 완료!\n",
      "\n",
      "============================================================\n",
      "✅ 구축 완료 상태:\n",
      "   📊 데이터베이스: northwind\n",
      "   📋 테이블 수: 8개\n",
      "\n",
      "📋 테이블별 데이터 현황:\n",
      "   📄 categories: 8개 레코드, 3개 컬럼\n",
      "   📄 customers: 5개 레코드, 8개 컬럼\n",
      "   📄 employees: 5개 레코드, 9개 컬럼\n",
      "   📄 order_details: 290개 레코드, 5개 컬럼\n",
      "   📄 orders: 100개 레코드, 12개 컬럼\n",
      "   📄 products: 10개 레코드, 8개 컬럼\n",
      "   📄 shippers: 3개 레코드, 3개 컬럼\n",
      "   📄 suppliers: 5개 레코드, 8개 컬럼\n",
      "\n",
      "📈 총 데이터: 426개 레코드\n",
      "🔗 테이블 관계: 8개\n",
      "   • orders.customer_id -> customers.customer_id\n",
      "   • orders.employee_id -> employees.employee_id\n",
      "   • orders.ship_via -> shippers.shipper_id\n",
      "   ... 그 외 5개\n",
      "\n",
      "🎯 Text-to-SQL 테스트 준비 완료!\n",
      "\n",
      "💬 테스트할 수 있는 자연어 질문 예시:\n",
      "   1. 전체 고객 수는 몇 명인가요?\n",
      "   2. 가장 많이 팔린 상품 5개를 보여주세요\n",
      "   3. 월별 주문 현황을 알려주세요\n",
      "   4. 고객별 총 구매 금액을 계산해주세요\n",
      "   5. 카테고리별 상품 개수는?\n",
      "   6. 직원별 담당 주문 건수는?\n",
      "   7. 배송비가 가장 비싼 주문들을 찾아주세요\n",
      "   8. 할인이 적용된 주문 상세를 보여주세요\n",
      "\n",
      "🔧 다음 노트북 실행 방법:\n",
      "   1. 새 노트북 생성: 02_langchain_agent_text_to_sql.ipynb\n",
      "   2. 이 노트북의 변수들 활용:\n",
      "      - spark: Spark 세션\n",
      "      - northwind_schema: 스키마 메타데이터\n",
      "   3. LangChain Agent 구현 시작\n",
      "\n",
      "🛠️ 현재 노트북에서 사용 가능한 함수들:\n",
      "   • test_northwind_queries() - 기본 쿼리 테스트 재실행\n",
      "   • verify_northwind_database() - 데이터베이스 상태 재확인\n",
      "   • analyze_schema_for_langchain() - 스키마 정보 재분석\n",
      "   • spark.sql('YOUR_SQL_QUERY') - 직접 SQL 실행\n",
      "\n",
      "🎯 목표 달성!\n",
      "   ✅ Databricks에 실제 사용 가능한 샘플 데이터베이스 구축\n",
      "   ✅ LangChain Agent 구현을 위한 기반 환경 완료\n",
      "   ✅ Text-to-SQL 시스템의 첫 번째 단계 성공\n",
      "\n",
      "🚀 다음: LangChain Agent 기반 Text-to-SQL 구현!\n",
      "   노트북: 02_langchain_agent_text_to_sql.ipynb\n",
      "🔗 LangChain Agent 연동을 위한 스키마 분석\n",
      "=============================================\n",
      "\n",
      "📊 테이블 스키마 추출 중...\n",
      "   🔍 categories 분석 중...\n",
      "   🔍 categories 분석 중...\n",
      "      ✅ 3개 컬럼, 8개 레코드\n",
      "   🔍 customers 분석 중...\n",
      "      ✅ 3개 컬럼, 8개 레코드\n",
      "   🔍 customers 분석 중...\n",
      "      ✅ 8개 컬럼, 5개 레코드\n",
      "   🔍 employees 분석 중...\n",
      "      ✅ 8개 컬럼, 5개 레코드\n",
      "   🔍 employees 분석 중...\n",
      "      ✅ 9개 컬럼, 5개 레코드\n",
      "   🔍 order_details 분석 중...\n",
      "      ✅ 9개 컬럼, 5개 레코드\n",
      "   🔍 order_details 분석 중...\n",
      "      ✅ 5개 컬럼, 290개 레코드\n",
      "   🔍 orders 분석 중...\n",
      "      ✅ 5개 컬럼, 290개 레코드\n",
      "   🔍 orders 분석 중...\n",
      "      ✅ 12개 컬럼, 100개 레코드\n",
      "   🔍 products 분석 중...\n",
      "      ✅ 12개 컬럼, 100개 레코드\n",
      "   🔍 products 분석 중...\n",
      "      ✅ 8개 컬럼, 10개 레코드\n",
      "   🔍 shippers 분석 중...\n",
      "      ✅ 8개 컬럼, 10개 레코드\n",
      "   🔍 shippers 분석 중...\n",
      "      ✅ 3개 컬럼, 3개 레코드\n",
      "   🔍 suppliers 분석 중...\n",
      "      ✅ 3개 컬럼, 3개 레코드\n",
      "   🔍 suppliers 분석 중...\n",
      "      ✅ 8개 컬럼, 5개 레코드\n",
      "\n",
      "📋 LangChain Agent용 스키마 요약:\n",
      "   📂 데이터베이스: northwind\n",
      "   📊 테이블 수: 8개\n",
      "   🔗 관계 정의: 8개 테이블\n",
      "   📈 전체 컬럼 수: 56개\n",
      "   📊 전체 레코드 수: 426개\n",
      "\n",
      "✅ LangChain Agent 연동 준비 완료!\n",
      "📋 다음 노트북에서 사용할 수 있는 데이터:\n",
      "   🗄️ northwind_schema_info: 상세 스키마 정보\n",
      "   📊 agent_schema_summary: Agent용 요약 정보\n",
      "   🔗 table_relationships: 테이블 관계 정보\n",
      "\n",
      "🎯 다음 단계:\n",
      "   1. 02_langchain_agent_text_to_sql.ipynb 노트북 열기\n",
      "   2. 위 변수들을 활용하여 LangChain Agent 구현\n",
      "   3. Text-to-SQL 기능 테스트 및 데모\n",
      "\n",
      "🎉 Northwind 데이터베이스 구축 및 Agent 연동 준비 완료!\n",
      "      ✅ 8개 컬럼, 5개 레코드\n",
      "\n",
      "📋 LangChain Agent용 스키마 요약:\n",
      "   📂 데이터베이스: northwind\n",
      "   📊 테이블 수: 8개\n",
      "   🔗 관계 정의: 8개 테이블\n",
      "   📈 전체 컬럼 수: 56개\n",
      "   📊 전체 레코드 수: 426개\n",
      "\n",
      "✅ LangChain Agent 연동 준비 완료!\n",
      "📋 다음 노트북에서 사용할 수 있는 데이터:\n",
      "   🗄️ northwind_schema_info: 상세 스키마 정보\n",
      "   📊 agent_schema_summary: Agent용 요약 정보\n",
      "   🔗 table_relationships: 테이블 관계 정보\n",
      "\n",
      "🎯 다음 단계:\n",
      "   1. 02_langchain_agent_text_to_sql.ipynb 노트북 열기\n",
      "   2. 위 변수들을 활용하여 LangChain Agent 구현\n",
      "   3. Text-to-SQL 기능 테스트 및 데모\n",
      "\n",
      "🎉 Northwind 데이터베이스 구축 및 Agent 연동 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "# 한국어 특화 고급 프롬프트 템플릿\n",
    "advanced_korean_sql_prompt_template = \"\"\"\n",
    "당신은 한국어를 이해하는 SQL 전문가입니다. 주어진 데이터베이스 스키마를 바탕으로 한국어 질문을 정확한 SQL 쿼리로 변환하세요.\n",
    "\n",
    "{schema_context}\n",
    "\n",
    "변환 규칙:\n",
    "1. 테이블명과 컬럼명을 정확히 사용하세요\n",
    "2. 한국어 숫자 표현을 숫자로 변환하세요 (예: \"열 개\" → 10)\n",
    "3. 날짜 표현을 SQL 형식으로 변환하세요 (예: \"작년\" → 해당 연도)\n",
    "4. 집계 함수를 적절히 사용하세요\n",
    "5. 결과는 SQL 쿼리만 반환하고 ```sql 블록으로 감싸세요\n",
    "\n",
    "한국어 질문 예시:\n",
    "\n",
    "질문: \"고객 수를 세어주세요\"\n",
    "```sql\n",
    "SELECT COUNT(*) as customer_count FROM customers;\n",
    "```\n",
    "\n",
    "질문: \"가장 많이 주문한 고객 다섯 명을 찾아주세요\"\n",
    "```sql\n",
    "SELECT c.name, COUNT(o.order_id) as order_count\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.name\n",
    "ORDER BY order_count DESC\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "질문: \"월별 매출을 보여주세요\"\n",
    "```sql\n",
    "SELECT \n",
    "    DATE_FORMAT(order_date, '%Y-%m') as month,\n",
    "    SUM(amount) as monthly_revenue\n",
    "FROM orders\n",
    "GROUP BY DATE_FORMAT(order_date, '%Y-%m')\n",
    "ORDER BY month;\n",
    "```\n",
    "\n",
    "질문: \"{question}\"\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ 한국어 특화 고급 프롬프트 템플릿 정의 완료\")\n",
    "\n",
    "# 간단한 환경 클래스 정의 (LangChain Agent 노트북에서 더 자세히 구현됨)\n",
    "class DatabricksEnvironment:\n",
    "    \"\"\"기본 Databricks 환경 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        self.database_name = \"northwind\"\n",
    "\n",
    "class TextToSQLRAGApp:\n",
    "    \"\"\"통합 Text-to-SQL RAG 애플리케이션\"\"\"\n",
    "    \n",
    "    def __init__(self, databricks_env: DatabricksEnvironment):\n",
    "        self.env = databricks_env\n",
    "        self.schema_analyzer = None\n",
    "        self.schema_retriever = None\n",
    "        self.sql_generator = None\n",
    "        self.sql_executor = None\n",
    "        self.vectorized_schemas = []\n",
    "        \n",
    "        # 컴포넌트 초기화\n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"애플리케이션 컴포넌트 초기화\"\"\"\n",
    "        try:\n",
    "            # 스키마 분석기\n",
    "            if self.env.spark:\n",
    "                self.schema_analyzer = DatabaseSchemaAnalyzer(\n",
    "                    self.env.spark, \n",
    "                    self.env.embedding_model\n",
    "                )\n",
    "                print(\"✅ 스키마 분석기 초기화\")\n",
    "            \n",
    "            # SQL 실행기\n",
    "            if self.env.spark:\n",
    "                self.sql_executor = SQLExecutor(self.env.spark)\n",
    "                print(\"✅ SQL 실행기 초기화\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 컴포넌트 초기화 중 오류: {str(e)}\")\n",
    "    \n",
    "    def setup_database(self, database_name: str = \"default\", limit_tables: int = 10):\n",
    "        \"\"\"데이터베이스 설정 및 스키마 분석\"\"\"\n",
    "        if not self.schema_analyzer:\n",
    "            return False, \"스키마 분석기가 없습니다.\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"🔧 데이터베이스 '{database_name}' 설정 중...\")\n",
    "            \n",
    "            # 1. 스키마 분석\n",
    "            schemas = self.schema_analyzer.analyze_database_schema(database_name, limit_tables)\n",
    "            \n",
    "            if not schemas:\n",
    "                return False, f\"데이터베이스 '{database_name}'에서 스키마를 찾을 수 없습니다.\"\n",
    "            \n",
    "            # 2. 스키마 벡터화\n",
    "            self.vectorized_schemas = self.schema_analyzer.vectorize_schemas(schemas)\n",
    "            \n",
    "            # 3. RAG 검색기 초기화\n",
    "            self.schema_retriever = RAGSchemaRetriever(\n",
    "                self.vectorized_schemas, \n",
    "                self.env.embedding_model\n",
    "            )\n",
    "            \n",
    "            # 4. SQL 생성기 초기화\n",
    "            if self.env.foundation_model:\n",
    "                self.sql_generator = TextToSQLGenerator(\n",
    "                    self.env.foundation_model,\n",
    "                    self.schema_retriever\n",
    "                )\n",
    "            \n",
    "            print(f\"✅ 데이터베이스 설정 완료: {len(schemas)}개 테이블\")\n",
    "            return True, f\"{len(schemas)}개 테이블 설정 완료\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False, f\"데이터베이스 설정 실패: {str(e)}\"\n",
    "    \n",
    "    def ask_question(self, question: str, execute_sql: bool = True, dry_run: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"자연어 질문을 SQL로 변환하고 실행\"\"\"\n",
    "        if not self.sql_generator:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"SQL 생성기가 초기화되지 않았습니다. setup_database()를 먼저 실행하세요.\",\n",
    "                \"question\": question\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            print(f\"🤔 질문 분석 중: '{question}'\")\n",
    "            \n",
    "            # 1. SQL 생성\n",
    "            sql_result = self.sql_generator.generate_sql(question)\n",
    "            \n",
    "            if not sql_result[\"success\"]:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": sql_result[\"error\"],\n",
    "                    \"question\": question\n",
    "                }\n",
    "            \n",
    "            sql_query = sql_result[\"sql_query\"]\n",
    "            relevant_schemas = sql_result[\"relevant_schemas\"]\n",
    "            \n",
    "            print(f\"🔍 관련 테이블 {len(relevant_schemas)}개 발견\")\n",
    "            print(f\"⚡ SQL 생성 완료\")\n",
    "            \n",
    "            result = {\n",
    "                \"success\": True,\n",
    "                \"question\": question,\n",
    "                \"sql_query\": sql_query,\n",
    "                \"relevant_schemas\": [\n",
    "                    {\n",
    "                        \"table\": schema[\"full_name\"],\n",
    "                        \"score\": schema.get(\"similarity_score\", 0)\n",
    "                    }\n",
    "                    for schema in relevant_schemas\n",
    "                ],\n",
    "                \"execution_result\": None\n",
    "            }\n",
    "            \n",
    "            # 2. SQL 실행 (옵션)\n",
    "            if execute_sql and self.sql_executor:\n",
    "                print(\"🚀 SQL 실행 중...\")\n",
    "                execution_result = self.sql_executor.execute_sql(sql_query, dry_run=dry_run)\n",
    "                result[\"execution_result\"] = execution_result\n",
    "                \n",
    "                if execution_result[\"success\"]:\n",
    "                    print(\"✅ SQL 실행 성공\")\n",
    "                else:\n",
    "                    print(f\"❌ SQL 실행 실패: {execution_result['error']}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"질문 처리 실패: {str(e)}\",\n",
    "                \"question\": question\n",
    "            }\n",
    "    \n",
    "    def get_database_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"현재 설정된 데이터베이스 요약 정보\"\"\"\n",
    "        if not self.vectorized_schemas:\n",
    "            return {\"message\": \"설정된 데이터베이스가 없습니다.\"}\n",
    "        \n",
    "        summary = {\n",
    "            \"total_tables\": len(self.vectorized_schemas),\n",
    "            \"tables\": []\n",
    "        }\n",
    "        \n",
    "        for schema in self.vectorized_schemas:\n",
    "            table_info = {\n",
    "                \"name\": schema[\"full_name\"],\n",
    "                \"row_count\": schema.get(\"row_count\", 0),\n",
    "                \"column_count\": len(schema[\"columns\"]),\n",
    "                \"has_embedding\": \"embedding\" in schema\n",
    "            }\n",
    "            summary[\"tables\"].append(table_info)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def interactive_session(self):\n",
    "        \"\"\"대화형 세션 시작\"\"\"\n",
    "        print(\"🎯 Text-to-SQL RAG 대화형 세션 시작\")\n",
    "        print(\"   종료하려면 'quit' 또는 'exit'를 입력하세요.\")\n",
    "        print(\"   도움말은 'help'를 입력하세요.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                question = input(\"\\n💬 질문을 입력하세요: \").strip()\n",
    "                \n",
    "                if question.lower() in ['quit', 'exit', '종료']:\n",
    "                    print(\"👋 세션을 종료합니다.\")\n",
    "                    break\n",
    "                \n",
    "                if question.lower() in ['help', '도움말']:\n",
    "                    self._show_help()\n",
    "                    continue\n",
    "                \n",
    "                if question.lower() in ['summary', '요약']:\n",
    "                    summary = self.get_database_summary()\n",
    "                    print(f\"\\n📊 데이터베이스 요약: {summary['total_tables']}개 테이블\")\n",
    "                    for table in summary['tables'][:5]:\n",
    "                        print(f\"   📋 {table['name']}: {table['row_count']:,}행, {table['column_count']}컬럼\")\n",
    "                    continue\n",
    "                \n",
    "                if not question:\n",
    "                    continue\n",
    "                \n",
    "                # 질문 처리\n",
    "                result = self.ask_question(question)\n",
    "                self._display_result(result)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n👋 세션을 종료합니다.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 오류 발생: {str(e)}\")\n",
    "    \n",
    "    def _show_help(self):\n",
    "        \"\"\"도움말 표시\"\"\"\n",
    "        help_text = \"\"\"\n",
    "🔧 사용 가능한 명령어:\n",
    "- help, 도움말: 이 도움말 표시\n",
    "- summary, 요약: 데이터베이스 요약 정보\n",
    "- quit, exit, 종료: 세션 종료\n",
    "\n",
    "💡 질문 예시:\n",
    "- \"고객 수를 알려주세요\"\n",
    "- \"월별 매출을 보여주세요\"\n",
    "- \"가장 많이 팔린 상품 10개는?\"\n",
    "- \"최근 일주일 주문 현황은?\"\n",
    "\"\"\"\n",
    "        print(help_text)\n",
    "    \n",
    "    def _display_result(self, result: Dict[str, Any]):\n",
    "        \"\"\"결과 표시\"\"\"\n",
    "        if not result[\"success\"]:\n",
    "            print(f\"\\n❌ 오류: {result['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n🔍 SQL 쿼리:\")\n",
    "        print(f\"```sql\\n{result['sql_query']}\\n```\")\n",
    "        \n",
    "        if result[\"relevant_schemas\"]:\n",
    "            print(f\"\\n📋 사용된 테이블:\")\n",
    "            for schema in result[\"relevant_schemas\"]:\n",
    "                score = schema[\"score\"]\n",
    "                print(f\"   • {schema['table']} (매칭점수: {score:.3f})\")\n",
    "        \n",
    "        if result[\"execution_result\"]:\n",
    "            execution = result[\"execution_result\"]\n",
    "            if execution[\"success\"]:\n",
    "                print(f\"\\n✅ 실행 결과 ({execution['row_count']}개 행):\")\n",
    "                if execution[\"results\"]:\n",
    "                    df = pd.DataFrame(execution[\"results\"][:10])  # 처음 10개만\n",
    "                    print(df.to_string(index=False))\n",
    "                    if execution[\"row_count\"] > 10:\n",
    "                        print(f\"... 그 외 {execution['row_count'] - 10}개 행\")\n",
    "            else:\n",
    "                print(f\"\\n❌ 실행 실패: {execution['error']}\")\n",
    "\n",
    "print(\"✅ Text-to-SQL RAG 애플리케이션 클래스 정의 완료\")\n",
    "\n",
    "# 📋 01단계 완료 요약 및 다음 단계 준비\n",
    "\n",
    "print(\"🎉 Northwind 데이터베이스 구축 완료!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# 현재 상태 확인\n",
    "if spark and 'northwind_schema' in globals():\n",
    "    \n",
    "    print(\"✅ 구축 완료 상태:\")\n",
    "    schema = northwind_schema\n",
    "    \n",
    "    print(f\"   📊 데이터베이스: {schema['database_name']}\")\n",
    "    print(f\"   📋 테이블 수: {schema['total_tables']}개\")\n",
    "    \n",
    "    # 각 테이블 상태\n",
    "    print(f\"\\n📋 테이블별 데이터 현황:\")\n",
    "    for table_name, info in schema['schema_details'].items():\n",
    "        print(f\"   📄 {table_name}: {info['row_count']:,}개 레코드, {len(info['columns'])}개 컬럼\")\n",
    "    \n",
    "    # 총 데이터 규모\n",
    "    import builtins\n",
    "    total_records = builtins.sum(info['row_count'] for info in schema['schema_details'].values())\n",
    "    print(f\"\\n📈 총 데이터: {total_records:,}개 레코드\")\n",
    "    \n",
    "    # 관계 확인\n",
    "    print(f\"🔗 테이블 관계: {len(schema['relationships'])}개\")\n",
    "    for rel_name, rel_desc in list(schema['relationships'].items())[:3]:\n",
    "        print(f\"   • {rel_desc}\")\n",
    "    if len(schema['relationships']) > 3:\n",
    "        print(f\"   ... 그 외 {len(schema['relationships']) - 3}개\")\n",
    "    \n",
    "    print(f\"\\n🎯 Text-to-SQL 테스트 준비 완료!\")\n",
    "    \n",
    "    # 간단한 테스트 질문들\n",
    "    sample_questions = [\n",
    "        \"전체 고객 수는 몇 명인가요?\",\n",
    "        \"가장 많이 팔린 상품 5개를 보여주세요\",\n",
    "        \"월별 주문 현황을 알려주세요\", \n",
    "        \"고객별 총 구매 금액을 계산해주세요\",\n",
    "        \"카테고리별 상품 개수는?\",\n",
    "        \"직원별 담당 주문 건수는?\",\n",
    "        \"배송비가 가장 비싼 주문들을 찾아주세요\",\n",
    "        \"할인이 적용된 주문 상세를 보여주세요\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n💬 테스트할 수 있는 자연어 질문 예시:\")\n",
    "    for i, question in enumerate(sample_questions, 1):\n",
    "        print(f\"   {i}. {question}\")\n",
    "    \n",
    "    print(f\"\\n🔧 다음 노트북 실행 방법:\")\n",
    "    print(f\"   1. 새 노트북 생성: 02_langchain_agent_text_to_sql.ipynb\")\n",
    "    print(f\"   2. 이 노트북의 변수들 활용:\")\n",
    "    print(f\"      - spark: Spark 세션\")\n",
    "    print(f\"      - northwind_schema: 스키마 메타데이터\")\n",
    "    print(f\"   3. LangChain Agent 구현 시작\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 설정이 완료되지 않았습니다.\")\n",
    "    print(\"   위의 모든 셀을 순서대로 실행해주세요.\")\n",
    "\n",
    "# 현재 노트북에서 바로 사용할 수 있는 도구들\n",
    "print(f\"\\n🛠️ 현재 노트북에서 사용 가능한 함수들:\")\n",
    "available_functions = [\n",
    "    \"test_northwind_queries() - 기본 쿼리 테스트 재실행\",\n",
    "    \"verify_northwind_database() - 데이터베이스 상태 재확인\", \n",
    "    \"analyze_schema_for_langchain() - 스키마 정보 재분석\",\n",
    "    \"spark.sql('YOUR_SQL_QUERY') - 직접 SQL 실행\"\n",
    "]\n",
    "\n",
    "for func in available_functions:\n",
    "    print(f\"   • {func}\")\n",
    "\n",
    "print(f\"\\n🎯 목표 달성!\")\n",
    "print(f\"   ✅ Databricks에 실제 사용 가능한 샘플 데이터베이스 구축\")\n",
    "print(f\"   ✅ LangChain Agent 구현을 위한 기반 환경 완료\")\n",
    "print(f\"   ✅ Text-to-SQL 시스템의 첫 번째 단계 성공\")\n",
    "\n",
    "print(f\"\\n🚀 다음: LangChain Agent 기반 Text-to-SQL 구현!\")\n",
    "print(f\"   노트북: 02_langchain_agent_text_to_sql.ipynb\")\n",
    "\n",
    "# 스키마 정보 추출 및 LangChain Agent 연동 준비\n",
    "\n",
    "print(\"🔗 LangChain Agent 연동을 위한 스키마 분석\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. 상세 스키마 정보 추출\n",
    "def extract_table_schema(table_name):\n",
    "    \"\"\"테이블의 상세 스키마 정보 추출\"\"\"\n",
    "    try:\n",
    "        # 컬럼 정보 조회\n",
    "        describe_result = spark.sql(f\"DESCRIBE TABLE northwind.{table_name}\").collect()\n",
    "        \n",
    "        columns = []\n",
    "        for row in describe_result:\n",
    "            if row.col_name and not row.col_name.startswith('#'):\n",
    "                columns.append({\n",
    "                    \"name\": row.col_name,\n",
    "                    \"type\": row.data_type,\n",
    "                    \"nullable\": True  # Delta Lake는 기본적으로 nullable\n",
    "                })\n",
    "        \n",
    "        # 샘플 데이터 조회\n",
    "        sample_df = spark.table(f\"northwind.{table_name}\").limit(3)\n",
    "        sample_data = [row.asDict() for row in sample_df.collect()]\n",
    "        \n",
    "        # 테이블 통계\n",
    "        total_count = spark.table(f\"northwind.{table_name}\").count()\n",
    "        \n",
    "        return {\n",
    "            \"table_name\": table_name,\n",
    "            \"columns\": columns,\n",
    "            \"sample_data\": sample_data,\n",
    "            \"total_records\": total_count,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"table_name\": table_name,\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"failed\"\n",
    "        }\n",
    "\n",
    "# 2. 모든 테이블의 스키마 추출\n",
    "print(\"\\n📊 테이블 스키마 추출 중...\")\n",
    "northwind_schema_info = {}\n",
    "\n",
    "tables = spark.sql(\"SHOW TABLES IN northwind\").collect()\n",
    "table_names = [row.tableName for row in tables]\n",
    "\n",
    "for table_name in sorted(table_names):\n",
    "    print(f\"   🔍 {table_name} 분석 중...\")\n",
    "    schema_info = extract_table_schema(table_name)\n",
    "    northwind_schema_info[table_name] = schema_info\n",
    "    \n",
    "    if schema_info[\"status\"] == \"success\":\n",
    "        col_count = len(schema_info[\"columns\"])\n",
    "        record_count = schema_info[\"total_records\"]\n",
    "        print(f\"      ✅ {col_count}개 컬럼, {record_count:,}개 레코드\")\n",
    "    else:\n",
    "        print(f\"      ❌ 실패: {schema_info['error']}\")\n",
    "\n",
    "# 3. 테이블 간 관계 정의 (Northwind 표준 관계)\n",
    "table_relationships = {\n",
    "    \"customers\": {\n",
    "        \"primary_key\": \"customerid\",\n",
    "        \"related_tables\": [\"orders\"],\n",
    "        \"description\": \"고객 정보 (주문과 1:N 관계)\"\n",
    "    },\n",
    "    \"suppliers\": {\n",
    "        \"primary_key\": \"supplierid\", \n",
    "        \"related_tables\": [\"products\"],\n",
    "        \"description\": \"공급업체 정보 (상품과 1:N 관계)\"\n",
    "    },\n",
    "    \"categories\": {\n",
    "        \"primary_key\": \"categoryid\",\n",
    "        \"related_tables\": [\"products\"],\n",
    "        \"description\": \"상품 카테고리 (상품과 1:N 관계)\"\n",
    "    },\n",
    "    \"products\": {\n",
    "        \"primary_key\": \"productid\",\n",
    "        \"foreign_keys\": [\"supplierid\", \"categoryid\"],\n",
    "        \"related_tables\": [\"order_details\", \"suppliers\", \"categories\"],\n",
    "        \"description\": \"상품 정보 (주문상세와 1:N, 공급업체/카테고리와 N:1 관계)\"\n",
    "    },\n",
    "    \"employees\": {\n",
    "        \"primary_key\": \"employeeid\",\n",
    "        \"related_tables\": [\"orders\"],\n",
    "        \"description\": \"직원 정보 (주문과 1:N 관계)\"\n",
    "    },\n",
    "    \"shippers\": {\n",
    "        \"primary_key\": \"shipperid\",\n",
    "        \"related_tables\": [\"orders\"],\n",
    "        \"description\": \"운송업체 정보 (주문과 1:N 관계)\"\n",
    "    },\n",
    "    \"orders\": {\n",
    "        \"primary_key\": \"orderid\",\n",
    "        \"foreign_keys\": [\"customerid\", \"employeeid\", \"shipvia\"],\n",
    "        \"related_tables\": [\"order_details\", \"customers\", \"employees\", \"shippers\"],\n",
    "        \"description\": \"주문 정보 (주문상세와 1:N, 고객/직원/운송업체와 N:1 관계)\"\n",
    "    },\n",
    "    \"order_details\": {\n",
    "        \"primary_key\": [\"orderid\", \"productid\"],\n",
    "        \"foreign_keys\": [\"orderid\", \"productid\"],\n",
    "        \"related_tables\": [\"orders\", \"products\"],\n",
    "        \"description\": \"주문 상세 정보 (주문/상품과 N:1 관계)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 4. Agent용 스키마 요약 생성\n",
    "agent_schema_summary = {\n",
    "    \"database_name\": \"northwind\",\n",
    "    \"description\": \"Northwind 샘플 무역 회사 데이터베이스 - 고객, 주문, 상품, 직원 등의 비즈니스 데이터\",\n",
    "    \"total_tables\": len(table_names),\n",
    "    \"tables\": {},\n",
    "    \"relationships\": table_relationships,\n",
    "    \"common_queries\": [\n",
    "        \"고객별 주문 통계\",\n",
    "        \"상품별 매출 분석\", \n",
    "        \"직원별 성과 분석\",\n",
    "        \"카테고리별 상품 현황\",\n",
    "        \"국가별 고객 분포\",\n",
    "        \"월별 매출 추이\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 각 테이블 정보를 Agent용으로 요약\n",
    "for table_name, schema_info in northwind_schema_info.items():\n",
    "    if schema_info[\"status\"] == \"success\":\n",
    "        agent_schema_summary[\"tables\"][table_name] = {\n",
    "            \"description\": table_relationships.get(table_name, {}).get(\"description\", f\"{table_name} 테이블\"),\n",
    "            \"columns\": [f\"{col['name']} ({col['type']})\" for col in schema_info[\"columns\"]],\n",
    "            \"primary_key\": table_relationships.get(table_name, {}).get(\"primary_key\"),\n",
    "            \"foreign_keys\": table_relationships.get(table_name, {}).get(\"foreign_keys\", []),\n",
    "            \"record_count\": schema_info[\"total_records\"],\n",
    "            \"sample_data\": schema_info[\"sample_data\"][:1]  # 첫 번째 레코드만\n",
    "        }\n",
    "\n",
    "print(f\"\\n📋 LangChain Agent용 스키마 요약:\")\n",
    "print(f\"   📂 데이터베이스: {agent_schema_summary['database_name']}\")\n",
    "print(f\"   📊 테이블 수: {agent_schema_summary['total_tables']}개\")\n",
    "print(f\"   🔗 관계 정의: {len(table_relationships)}개 테이블\")\n",
    "\n",
    "total_columns = builtins.sum(len(info.get('columns', [])) for info in agent_schema_summary['tables'].values())\n",
    "total_records = builtins.sum(info.get('record_count', 0) for info in agent_schema_summary['tables'].values())\n",
    "\n",
    "print(f\"   📈 전체 컬럼 수: {total_columns}개\")\n",
    "print(f\"   📊 전체 레코드 수: {total_records:,}개\")\n",
    "\n",
    "# 5. 연동 준비 완료 상태 확인\n",
    "print(f\"\\n✅ LangChain Agent 연동 준비 완료!\")\n",
    "print(f\"📋 다음 노트북에서 사용할 수 있는 데이터:\")\n",
    "print(f\"   🗄️ northwind_schema_info: 상세 스키마 정보\")\n",
    "print(f\"   📊 agent_schema_summary: Agent용 요약 정보\")\n",
    "print(f\"   🔗 table_relationships: 테이블 관계 정보\")\n",
    "\n",
    "print(f\"\\n🎯 다음 단계:\")\n",
    "print(f\"   1. 02_langchain_agent_text_to_sql.ipynb 노트북 열기\")\n",
    "print(f\"   2. 위 변수들을 활용하여 LangChain Agent 구현\")\n",
    "print(f\"   3. Text-to-SQL 기능 테스트 및 데모\")\n",
    "\n",
    "print(f\"\\n🎉 Northwind 데이터베이스 구축 및 Agent 연동 준비 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
